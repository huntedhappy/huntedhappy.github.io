<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>ipam - Tag - Dokyung&#39;s DevOoOps</title>
    <link>https://huntedhappy.github.io/tags/ipam/</link>
    <description>ipam - Tag | Dokyung&#39;s DevOoOps</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 24 May 2023 01:42:56 &#43;0900</lastBuildDate><atom:link href="https://huntedhappy.github.io/tags/ipam/" rel="self" type="application/rss+xml" /><item>
  <title>The Documentation IPAM on TANZU</title>
  <link>https://huntedhappy.github.io/tanzu-ipam/</link>
  <pubDate>Wed, 24 May 2023 01:42:56 &#43;0900</pubDate>
  <author>Author</author>
  <guid>https://huntedhappy.github.io/tanzu-ipam/</guid>
  <description><![CDATA[<div class="featured-image"><img loading="eager" src="https://huntedhappy.github.io/tanzu-ipam/ipam.png" alt="The Documentation IPAM on TANZU" title="IPAM on TANZU" referrerpolicy="no-referrer"/></div><h1 id="1--ipam-on-tanzu" class="heading-element">
  <a href="#1--ipam-on-tanzu" class="heading-mark"></a>1.  IPAM ON TANZU</h1><p>탄주는 기본적으로 DHCP 서버가 필요 하다. K8S 환경을 유지 하기 위해 VM의 라이프 사이클을 제공 하고 있기 때문이다. 그렇기 때문에 VM에 HANG이 걸리거나, 삭제가 되면 자동적으로 복구를 하게 된다. K8S에서 POD를 관리 하는 것처럼 탄주는 Master, Worker Node를 하나의 POD로 인식을 하게 설계를 한게 아닌가 싶다. 하지만 이런 자동화 시스템으로 인하여 STATIC하게 IP를 넣을 수도 없으며(DHCP서버를 통해 배포 후 IP를 변경 할 수 있겠지만 권장 하지 않는다.) 무조건적으로 DHCP 서버가 필요 하기 때문에 설치시 많은 어려움이 있을 수 있다. 또한 DHCP 서버를 운영 함으로 인해서 불필요한 자원을 사용하게 될 수도 있다.</p>
<p>지금 작성 하는 부분은 DHCP서버를 완전히 배제 할 수는 없다. 다만 메니저 클러스터를 배포 한 후 CLUSTER의 IP는 관리 클러스터에서 관리를 할 수 있게 설정을 할 수 있다. 두가지 방법이 있을 수 있는대 처음은 YTT를 구성함으로 인해서 처음부터 클러스터가 배포 될 경우 IP를 할당 받을 수 있게 할 수 도 있으며, 두번째로는 클러스터를 DHCP서버를 배포 후 IPAM을 사용하게 구성 할 수 있다.</p>
<p>당연히 YTT로 구성을 하여서 처음부터 배포를 하는 것이 권장을 할 것이다. 하지만 한편으로는 걱정도 된다. 이렇게 구성 함으로 인해서 업그레이드가 잘 될지.. (추후에 업그레이드도 진행을 해봐야 할 것이다&hellip;)</p>
<p>현재 작성하는 방법은 2번째 방법으로 진행 할 예정이다.
방법은 간단하다. 우선 metal3-io가 무엇을 하는지는 알지 못하지만 해당 github에서 제공을 받아 설치를 해볼 수 있을 것이다.</p>
<p>아래와 같이 GIT을 다운로드 받은 후 간단하게 실행을 해준다.</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/arbhoj/vsphere-ipam.git
</span></span><span class="line"><span class="cl">kubectl apply -f vsphere-ipam/metal3ipam/provider-components/infrastructure-components.yaml
</span></span><span class="line"><span class="cl">kubectl apply -f vsphere-ipam/spectro-ipam-adapter/install.yaml</span></span></code></pre></td></tr></table>
</div>
</div><p>그리고 아래와 같이 환경 변수를 적용해도 되고, 그냥 입력을 해도 된다. 편한대로 설정을 해보자</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>run2
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">NETWORK_NAME</span><span class="o">=</span>LS-TKGM01-WORKLOAD-10.253.127.x <span class="c1">#This is the name of the network to be used in vSphere</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">START_IP</span><span class="o">=</span>10.253.127.120
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">END_IP</span><span class="o">=</span>10.253.127.130
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CIDR</span><span class="o">=</span><span class="m">24</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">GATEWAY</span><span class="o">=</span>10.253.127.1
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">DNS_SERVER</span><span class="o">=</span>10.253.107.2</span></span></code></pre></td></tr></table>
</div>
</div><p>위와 같이 환경변수를 적용 후 ippol을 실행 해주자.</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">cat <span class="s">&lt;&lt; EOF | kubectl apply -f -
</span></span></span><span class="line"><span class="cl"><span class="s">apiVersion: ipam.metal3.io/v1alpha1
</span></span></span><span class="line"><span class="cl"><span class="s">kind: IPPool
</span></span></span><span class="line"><span class="cl"><span class="s">metadata:
</span></span></span><span class="line"><span class="cl"><span class="s">  name: ${CLUSTER_NAME}-pool
</span></span></span><span class="line"><span class="cl"><span class="s">  labels:
</span></span></span><span class="line"><span class="cl"><span class="s">    cluster.x-k8s.io/network-name: ${NETWORK_NAME}
</span></span></span><span class="line"><span class="cl"><span class="s">spec:
</span></span></span><span class="line"><span class="cl"><span class="s">  clusterName: ${CLUSTER_NAME}
</span></span></span><span class="line"><span class="cl"><span class="s">  namePrefix: ${CLUSTER_NAME}-prov
</span></span></span><span class="line"><span class="cl"><span class="s">  pools:
</span></span></span><span class="line"><span class="cl"><span class="s">    - start: ${START_IP}
</span></span></span><span class="line"><span class="cl"><span class="s">      end: ${END_IP}
</span></span></span><span class="line"><span class="cl"><span class="s">      prefix: ${CIDR}
</span></span></span><span class="line"><span class="cl"><span class="s">      gateway: ${GATEWAY}
</span></span></span><span class="line"><span class="cl"><span class="s">  prefix: 24
</span></span></span><span class="line"><span class="cl"><span class="s">  gateway: ${GATEWAY}
</span></span></span><span class="line"><span class="cl"><span class="s">  dnsServers: [${DNS_SERVER}]
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span></span></span></code></pre></td></tr></table>
</div>
</div><p>그리고 machine에다가 lables을 붙여주어야 한다. 지금 해도 되고 새로 생성 할 때 해도 된다.</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl label vspheremachinetemplates run2-control-plane cluster.x-k8s.io/ip-pool-name<span class="o">=</span>run2-pool
</span></span><span class="line"><span class="cl">kubectl label vspheremachinetemplates run2-worker cluster.x-k8s.io/ip-pool-name<span class="o">=</span>run2-pool</span></span></code></pre></td></tr></table>
</div>
</div><p>이렇게 구성을 하였다고 하더라도 실제적으로 vspheremachinetemplates 여기에 수정을 할 수가 없기 때문에 별도로 새로 생성을 해야 한다. 아마 이 부분은 vmsizing을 변경 할 때 많이 봤을 수도 있다. vmsizing은 이렇게 변경도 가능하지만 왠만하면 nodepool을 사용하여서 변경하는게 좋지 않을가 하는 생각을 잠시 한다.</p>
<p>아래와 같이 주석 처리 한부분을 추가 및 변경을 해준다.</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get VSphereMachineTemplate run2-control-plane -o yaml <span class="p">|</span> kubectl neat &gt; run2-control-plane.yaml
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">vi run2-control-plan.yaml
</span></span><span class="line"><span class="cl">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
</span></span><span class="line"><span class="cl">kind: VSphereMachineTemplate
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  annotations:
</span></span><span class="line"><span class="cl">    vmTemplateMoid: vm-134165
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    cluster.x-k8s.io/ip-pool-name: run2-pool   <span class="c1">## ippool이랑 name 일치 하는지 확인</span>
</span></span><span class="line"><span class="cl">  name: run2-control-plane01
</span></span><span class="line"><span class="cl">  namespace: default
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  template:
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">      cloneMode: fullClone
</span></span><span class="line"><span class="cl">      datacenter: /OBDC
</span></span><span class="line"><span class="cl">      datastore: /OBDC/datastore/vsanDatastore
</span></span><span class="line"><span class="cl">      diskGiB: <span class="m">300</span>
</span></span><span class="line"><span class="cl">      folder: /OBDC/vm/tanzu
</span></span><span class="line"><span class="cl">      memoryMiB: <span class="m">8192</span>
</span></span><span class="line"><span class="cl">      network:
</span></span><span class="line"><span class="cl">        devices:
</span></span><span class="line"><span class="cl">        - dhcp4: <span class="nb">false</span>   <span class="c1">#### 변경</span>
</span></span><span class="line"><span class="cl">          networkName: /OBDC/network/LS-TKGM01-WORKLOAD-10.253.127.x
</span></span><span class="line"><span class="cl">      numCPUs: <span class="m">4</span>
</span></span><span class="line"><span class="cl">      resourcePool: /OBDC/host/OBCLUSTER/Resources/dk-tanzu
</span></span><span class="line"><span class="cl">      server: vcsa01.vcf.local
</span></span><span class="line"><span class="cl">      storagePolicyName: k8s
</span></span><span class="line"><span class="cl">      template: /OBDC/vm/temp/photon-3-kube-v1.24.10+vmware.1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl apply -f run2-control-plan.yaml</span></span></code></pre></td></tr></table>
</div>
</div><p>마찬가지로 worker노드도 템플릿을 생성 해준다.</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl get VSphereMachineTemplate run2-worker -o yaml <span class="p">|</span> kubectl neat &gt; run2-worker.yaml
</span></span><span class="line"><span class="cl">vi run2-worker.yaml
</span></span><span class="line"><span class="cl">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
</span></span><span class="line"><span class="cl">kind: VSphereMachineTemplate
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  annotations:
</span></span><span class="line"><span class="cl">    vmTemplateMoid: vm-134165
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    cluster.x-k8s.io/ip-pool-name: run2-pool    <span class="c1">## ippool이랑 name 일치 하는지 확인</span>
</span></span><span class="line"><span class="cl">  name: run2-worker01
</span></span><span class="line"><span class="cl">  namespace: default
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  template:
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">      cloneMode: fullClone
</span></span><span class="line"><span class="cl">      datacenter: /OBDC
</span></span><span class="line"><span class="cl">      datastore: /OBDC/datastore/vsanDatastore
</span></span><span class="line"><span class="cl">      diskGiB: <span class="m">300</span>
</span></span><span class="line"><span class="cl">      folder: /OBDC/vm/tanzu
</span></span><span class="line"><span class="cl">      memoryMiB: <span class="m">16384</span>
</span></span><span class="line"><span class="cl">      network:
</span></span><span class="line"><span class="cl">        devices:
</span></span><span class="line"><span class="cl">        - dhcp4: <span class="nb">false</span>    <span class="c1">#### 변경</span>
</span></span><span class="line"><span class="cl">          networkName: /OBDC/network/LS-TKGM01-WORKLOAD-10.253.127.x
</span></span><span class="line"><span class="cl">      numCPUs: <span class="m">4</span>
</span></span><span class="line"><span class="cl">      resourcePool: /OBDC/host/OBCLUSTER/Resources/dk-tanzu
</span></span><span class="line"><span class="cl">      server: vcsa01.vcf.local
</span></span><span class="line"><span class="cl">      storagePolicyName: k8s
</span></span><span class="line"><span class="cl">      template: /OBDC/vm/temp/photon-3-kube-v1.24.10+vmware.1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl apply -f run2-worker.yaml</span></span></code></pre></td></tr></table>
</div>
</div><p>이렇게 구성을 하게 되면 준비는 다 된것이다. 이제 master, worker node의 template만 변경 하게 되면 된다.</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl patch md run2-md-0 --type &#39;json&#39; -p &#39;[{&#34;op&#34;:&#34;replace&#34;,&#34;path&#34;:&#34;/spec/template/spec/infrastructureRef/name&#34;,&#34;value&#34;:&#34;run2-worker01&#34;}]&#39; --dry-run=client -o yaml | kubectl apply -f -
</span></span><span class="line"><span class="cl">kubectl patch kcp run2-control-plane --type &#39;json&#39; -p &#39;[{&#34;op&#34;:&#34;replace&#34;,&#34;path&#34;:&#34;/spec/machineTemplate/infrastructureRef/name&#34;,&#34;value&#34;:&#34;run2-control-plane01&#34;}]&#39; --dry-run=client -o yaml | kubectl apply - f-</span></span></code></pre></td></tr></table>
</div>
</div><p>이렇게 변경을 하게 되면 자동적으로 VM이 삭제 하고 재 생성 되는 것을 확인 할 수 있다.</p>
<p>그러면 IPPOOL에서 제대로 받아 오는지 확인을 해볼 수 있을 것이다.</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl describe ippools.ipam.metal3.io run2-pool
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Status:
</span></span><span class="line"><span class="cl">  Indexes:
</span></span><span class="line"><span class="cl">    run2:                        10.253.127.121
</span></span><span class="line"><span class="cl">    run2-control-plane-wnng6-0:  10.253.127.124
</span></span><span class="line"><span class="cl">    run2-worker-hcmw8-0:         10.253.127.120
</span></span><span class="line"><span class="cl">    run2-worker-sgbmb-0:         10.253.127.122
</span></span><span class="line"><span class="cl">    run2-worker-sjf5m-0:         10.253.127.123</span></span></code></pre></td></tr></table>
</div>
</div><p>DHCP서버를 완전히 안사용하게 할 수는 없겠지만 이 방법으로 어느정도 DHCP서버를 대체 할 수 있지 않을까라는 조심스러운 생각을 해본다.</p>
]]></description>
</item>
</channel>
</rss>
