[{"categories":["Documentation"],"content":"IPAM on TANZU","date":"2023-05-24","objectID":"/tanzu-ipam/","tags":["tanzu","k8s","ipam","devops","dk","dokyung"],"title":"The Documentation IPAM on TANZU","uri":"/tanzu-ipam/"},{"categories":["Documentation"],"content":"1. IPAM ON TANZU 탄주는 기본적으로 DHCP 서버가 필요 하다. K8S 환경을 유지 하기 위해 VM의 라이프 사이클을 제공 하고 있기 때문이다. 그렇기 때문에 VM에 HANG이 걸리거나, 삭제가 되면 자동적으로 복구를 하게 된다. K8S에서 POD를 관리 하는 것처럼 탄주는 Master, Worker Node를 하나의 POD로 인식을 하게 설계를 한게 아닌가 싶다. 하지만 이런 자동화 시스템으로 인하여 STATIC하게 IP를 넣을 수도 없으며(DHCP서버를 통해 배포 후 IP를 변경 할 수 있겠지만 권장 하지 않는다.) 무조건적으로 DHCP 서버가 필요 하기 때문에 설치시 많은 어려움이 있을 수 있다. 또한 DHCP 서버를 운영 함으로 인해서 불필요한 자원을 사용하게 될 수도 있다. 지금 작성 하는 부분은 DHCP서버를 완전히 배제 할 수는 없다. 다만 메니저 클러스터를 배포 한 후 CLUSTER의 IP는 관리 클러스터에서 관리를 할 수 있게 설정을 할 수 있다. 두가지 방법이 있을 수 있는대 처음은 YTT를 구성함으로 인해서 처음부터 클러스터가 배포 될 경우 IP를 할당 받을 수 있게 할 수 도 있으며, 두번째로는 클러스터를 DHCP서버를 배포 후 IPAM을 사용하게 구성 할 수 있다. 당연히 YTT로 구성을 하여서 처음부터 배포를 하는 것이 권장을 할 것이다. 하지만 한편으로는 걱정도 된다. 이렇게 구성 함으로 인해서 업그레이드가 잘 될지.. (추후에 업그레이드도 진행을 해봐야 할 것이다…) 현재 작성하는 방법은 2번째 방법으로 진행 할 예정이다. 방법은 간단하다. 우선 metal3-io가 무엇을 하는지는 알지 못하지만 해당 github에서 제공을 받아 설치를 해볼 수 있을 것이다. 아래와 같이 GIT을 다운로드 받은 후 간단하게 실행을 해준다. git clone https://github.com/arbhoj/vsphere-ipam.git kubectl apply -f vsphere-ipam/metal3ipam/provider-components/infrastructure-components.yaml kubectl apply -f vsphere-ipam/spectro-ipam-adapter/install.yaml 그리고 아래와 같이 환경 변수를 적용해도 되고, 그냥 입력을 해도 된다. 편한대로 설정을 해보자 export CLUSTER_NAME=run2 export NETWORK_NAME=LS-TKGM01-WORKLOAD-10.253.127.x #This is the name of the network to be used in vSphere export START_IP=10.253.127.120 export END_IP=10.253.127.130 export CIDR=24 export GATEWAY=10.253.127.1 export DNS_SERVER=10.253.107.2 위와 같이 환경변수를 적용 후 ippol을 실행 해주자. cat \u003c\u003c EOF | kubectl apply -f - apiVersion: ipam.metal3.io/v1alpha1 kind: IPPool metadata: name: ${CLUSTER_NAME}-pool labels: cluster.x-k8s.io/network-name: ${NETWORK_NAME} spec: clusterName: ${CLUSTER_NAME} namePrefix: ${CLUSTER_NAME}-prov pools: - start: ${START_IP} end: ${END_IP} prefix: ${CIDR} gateway: ${GATEWAY} prefix: 24 gateway: ${GATEWAY} dnsServers: [${DNS_SERVER}] EOF 그리고 machine에다가 lables을 붙여주어야 한다. 지금 해도 되고 새로 생성 할 때 해도 된다. kubectl label vspheremachinetemplates run2-control-plane cluster.x-k8s.io/ip-pool-name=run2-pool kubectl label vspheremachinetemplates run2-worker cluster.x-k8s.io/ip-pool-name=run2-pool 이렇게 구성을 하였다고 하더라도 실제적으로 vspheremachinetemplates 여기에 수정을 할 수가 없기 때문에 별도로 새로 생성을 해야 한다. 아마 이 부분은 vmsizing을 변경 할 때 많이 봤을 수도 있다. vmsizing은 이렇게 변경도 가능하지만 왠만하면 nodepool을 사용하여서 변경하는게 좋지 않을가 하는 생각을 잠시 한다. 아래와 같이 주석 처리 한부분을 추가 및 변경을 해준다. kubectl get VSphereMachineTemplate run2-control-plane -o yaml | kubectl neat \u003e run2-control-plane.yaml vi run2-control-plan.yaml apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereMachineTemplate metadata: annotations: vmTemplateMoid: vm-134165 labels: cluster.x-k8s.io/ip-pool-name: run2-pool ## ippool이랑 name 일치 하는지 확인 name: run2-control-plane01 namespace: default spec: template: spec: cloneMode: fullClone datacenter: /OBDC datastore: /OBDC/datastore/vsanDatastore diskGiB: 300 folder: /OBDC/vm/tanzu memoryMiB: 8192 network: devices: - dhcp4: false #### 변경 networkName: /OBDC/network/LS-TKGM01-WORKLOAD-10.253.127.x numCPUs: 4 resourcePool: /OBDC/host/OBCLUSTER/Resources/dk-tanzu server: vcsa01.vcf.local storagePolicyName: k8s template: /OBDC/vm/temp/photon-3-kube-v1.24.10+vmware.1 kubectl apply -f run2-control-plan.yaml 마찬가지로 worker노드도 템플릿을 생성 해준다. kubectl get VSphereMachineTemplate run2-worker -o yaml | kubectl neat \u003e run2-worker.yaml vi run2-worker.yaml apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereMachineTemplate metadata: annotations: vmTemplateMoid: vm-134165 labels: cluster.x-k8s.io/ip-pool-name: run2-pool ## ippool이랑 name 일치 하는지 확인 name: run2-worker01 namespace: default spec: template: spec: cloneMode: fullClone datacenter: /OBDC datastore: /OBDC/datastore/vsanDatastore diskGiB: 300 folder: /OBDC/vm/tanzu memoryMiB: 16384 network: devices: - dhcp4: false #### 변경 networkName: /OBDC/network/LS-TKGM01-WORKLOAD-10.253.127.x numCPUs: 4 resourcePool: /OBDC/host/OBCLUSTER/Resources/dk-tanzu server: vcsa01.vcf.local storagePolicyName: k8s template: /OBDC/vm/temp/photon-3-kube-v1.24.10+vmware.1 kubectl apply -f run2-worker.yaml 이렇게 구성을 ","date":"2023-05-24","objectID":"/tanzu-ipam/:0:0","tags":["tanzu","k8s","ipam","devops","dk","dokyung"],"title":"The Documentation IPAM on TANZU","uri":"/tanzu-ipam/"},{"categories":["Documentation"],"content":"Tanzu Community Edition with TAP","date":"2023-03-24","objectID":"/tmc/","tags":["tanzu","k8s","tmc","tsm","to","devops","dk","dokyung"],"title":"The Dcoumentation TMC","uri":"/tmc/"},{"categories":["Documentation"],"content":"1. TANZU Portfolio RUN \u0026 BUILD \u0026 MANAGE \u0026 LABS로 구성되어 있습니다. RUN의 경우는 멀티 클라우드에서 컨테이너 환경을 제공을 하며 BUILD의 경우 CI/CD 그리고 TAC (골든 이미지) 그리고 saas 형태로 제공하는 manage로 TMC (멀티 클러스터 관리), TSM (서비스 메쉬), TO (모니터링) 그리고 마지막으로 컨테이너를 잘 사용 할 수 있게 App Modernization을 도와주는 LABS 로 구성이 되어 있습니다. 아래 영상은 MANAGE 역할을 하는 TMC, TSM, TO의 대한 설명및 데모를 보여주고 있으므로 해당 솔루션의 대한 정보를 확인 할 수 있습니다. \r위의 영상이 도움이 되었으면 좋겠습니다. ","date":"2023-03-24","objectID":"/tmc/:1:0","tags":["tanzu","k8s","tmc","tsm","to","devops","dk","dokyung"],"title":"The Dcoumentation TMC","uri":"/tmc/"},{"categories":["Documentation"],"content":"CICD Design And Built","date":"2023-03-24","objectID":"/cicd/","tags":["supplychain","tap supplychain","argo","jenkins","visual studio","cicd","harbor","tanzu","k8s","devops","dk","dokyung","tap supplychain add"],"title":"The Documentation SupplyChain on TAP","uri":"/cicd/"},{"categories":["Documentation"],"content":"TANZU SUPPLYCHAIN 아래 내용은 기본적으로 VMWARE의 TAP솔루션을 알고 있다는 가정하에 작성을 하였습니다. CI/CD는 (Continuous Integration/Continuous Delivery) 지속적인 통합 및 지속적인 배포입니다. 예를 들어: 사용자는 IDE 환경 (IntelliJ or visual studio or eclipse)에서 소스코드를 수정 및 디버깅을 합니다. 이 후에 수정된 소스코드를 GIT에 PUSH를 합니다. JENKINS에서는 GIT에 수정된 소스를 Webhook 또는 Polling을 통해 변경을 감지 합니다. JENKINS에서 수정된 코드를 통해 Docker Image를 생성을 합니다. 생성된 Docker Image를 이미지 레포지토리에 업로드 합니다. Docker Image가 변경이 되었기 때문에 CD를 위해 별도의 GIT Project에 변경된 이미지를 수정 합니다. CD 솔루션을 통해 (ARGO 등) GIT이 변경이 되었으므로 컨테이너이미지를 업데이트 합니다. MANIFEST의 경우 HELM or KUSTOMIZATION or 각각 생성 할 수도 있습니다. JENKINS의 PIPELINE figure 1-1 와 같이 구성 할 수 있습니다. SLACK을 연동 후 원하는 문자로 알림을 받을 수 있습니다. figure 1-2 1-1 JENKINS PIPELINE 1-2 SLACK ALERT 위와 같이 JENKINS와 ARGO 를 통해 CI/CD를 구성 할 수 있습니다. 하지만 각각 구성하기 위해서는 JENKINS 그리고 ARGO의 대해서도 이해가 필요 합니다. 또한 K8S에 배포를 하기 위해서는 MANIFEST의 대해서도 이해가 필요 합니다. 가령 Deployment, statfulset, ingress 등등의 대해서도, 이해가 필요 하며 HELM 또는 KUSTOMIZAION을 사용한다면 해당 오픈소스의 대해서도 이해가 필요 합니다. TANZU APPLICATION PLATFORM은 여러 이해가 필요 한부분을 workload.yaml을 구성하면 SUPPLYCHAIN을 통해 CI/CD 구성을 사용자가 각각 구성 할 필요 없이 제공을 하고 있습니다. TANZU APPLICATION PLATFORM? VMWARE TANZU TAP 1-3 SUPPLYCHAIN 기본적으로 하나의 클러스터에 3가지중 하나의 SUPPLYCHAIN을 제공 하고 있습니다. DEFULAT의 경우 위의 설명 드린 대로 사용자는 workload.yaml을 적절하게 작성을 하게 되면 CI/CD가 동작 합니다. 아래 그림과 같이 workload.yaml 실행을 통해 supplychain이 순차적으로 동작을 하게 됩니다. 1-4 SUPPLYCHAIN GUI에서 확인을 해보면 아래와 같습니다. 1-5 SUPPLYCHAIN 그리고 두번째의 경우는 추가적으로 TEKTON을 통해 소스코드를 테스팅 할 수 있습니다. 그리고 마지막의 경우는 GRYPE을 통해 소스 및 이미지에 보안 취약점이 있는지 확인을 합니다. GUI에서 확인을 해보면 아래와 같이 취약점에 대해서 확인 할 수 있습니다. 1-6 SUPPLYCHAIN 하지만 위에서 설명 드린 대로 하나의 클러스터에는 3가지중 하나만 사용을 할 수 있습니다. 추가적인 supplychain을 구성 하고 싶을수도 있을 것입니다. 동일한 서비스의 대해서 빠르게 개발을 위해 BASIC으로 구성을 하고 싶을 것이며, QA/STAGING은 소스/이미지에 보안취약점이 있는지 잘 생성을 했는지 확인을 하고 싶을 것입니다. 물론 클러스터를 분리해서 구성 할 수도 있을 것입니다. 하지만 리소스 부족으로 분리를 할 수 없을 경우 하나의 클러스터에 모두 구성이 필요할수도 있을수 있습니다. 만약 그런 상황이 발생하면 3개중에 하나의 supplychain만 사용이 가능하기 때문에 한가지를 선택 해야 합니다. 그렇기 때문에 supplychain을 추가 하는 방법이 필요 할 수 있습니다. 아래와 같이 기본적으로 제공 하고 있습니다. 1-7 SUPPLYCHAIN SUPPLYCHAIN ADD cat \u003c\u003c 'EOF' \u003e basic_supply_chain.yaml #@ load(\"@ytt:data\", \"data\") #@ load(\"@ytt:assert\", \"assert\") #@ data.values.registry.server or assert.fail(\"missing registry.server\") #@ data.values.registry.repository or assert.fail(\"missing registry.repository\") --- apiVersion: carto.run/v1alpha1 kind: ClusterSupplyChain metadata: name: source-to-url-with-custom-support spec: resources: - name: source-provider params: - name: serviceAccount value: #@ data.values.service_account - name: gitImplementation value: #@ data.values.git_implementation templateRef: kind: ClusterSourceTemplate name: source-template - name: image-provider params: - name: serviceAccount value: #@ data.values.service_account - name: registry value: ca_cert_data: \"\" repository: #@ data.values.registry.repository server: #@ data.values.registry.server - default: default name: clusterBuilder - default: ./Dockerfile name: dockerfile - default: ./ name: docker_build_context sources: - name: source resource: source-provider templateRef: kind: ClusterImageTemplate options: - name: kpack-template selector: matchFields: - key: spec.params[?(@.name==\"dockerfile\")] operator: DoesNotExist - name: kaniko-template selector: matchFields: - key: spec.params[?(@.name==\"dockerfile\")] operator: Exists - images: - name: image resource: image-provider name: config-provider params: - name: serviceAccount value: #@ data.values.service_account templateRef: kind: ClusterConfigTemplate name: convention-template - configs: - name: config resource: config-provider name: app-config templateRef: kind: ClusterConfigTemplate options: - name: config-template selector: matchLabels: apps.tanzu.vmware.com/workload-type: web - name: server-template selector: matchLabels: apps.tanzu.vmware.com/workload-type: server - name: worker-template selector: matchLabels: apps.tanzu.vmware.com/workload-type: worker - configs: - name: app_def resource: app-config name: service-bindings templateRef: kind: ClusterConfigTemplate name: service-bindings - configs","date":"2023-03-24","objectID":"/cicd/:1:0","tags":["supplychain","tap supplychain","argo","jenkins","visual studio","cicd","harbor","tanzu","k8s","devops","dk","dokyung","tap supplychain add"],"title":"The Documentation SupplyChain on TAP","uri":"/cicd/"},{"categories":["Documentation"],"content":"TANZU \u0026 OKTA","date":"2023-03-04","objectID":"/tap-techdocs/","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation TechDocs in Tap","uri":"/tap-techdocs/"},{"categories":["Documentation"],"content":"1. 간단한 설명 ","date":"2023-03-04","objectID":"/tap-techdocs/:1:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation TechDocs in Tap","uri":"/tap-techdocs/"},{"categories":["Documentation"],"content":"1.1. Backstage? Backstage는 개발자 포털 구축을 위한 오픈 플랫폼입니다. 2년 전 Spotify의 소규모 팀이 내부에서 Backstage를 오픈소스로 출시 후 CNCF SANDBOX에서 현재는 INCUBATING 단계가 되었습니다. CNCF는 프로젝트 성숙도에 따라서 SANDBOX, INCUBATING, GRADUATED 단계로 구성 되어 있습니다. 프로젝트의 성숙도는 CNCF 위원회 멤버들에 의해서 결정되며, GRADUATED 단계의 프로젝트가 되기 위해서는 GRADUATED 요건이 필요 하며 위원회 멤버 과반수 이상의 찬성표를 받아야 합니다. https://www.cncf.io/projects/ Backstage WebPage를 가게 되면 엄청 많은 기능들을 확인 할 수 있습니다. Backstage LINK ","date":"2023-03-04","objectID":"/tap-techdocs/:1:1","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation TechDocs in Tap","uri":"/tap-techdocs/"},{"categories":["Documentation"],"content":"1.2. TechDocs? Backstage에 내장되어 있으며 MarkDown 파일을 작성 함으로 Backstage에 문서형 사이트를 얻을 수 있습니다. Backstage TechDocs 참고링크 TANZU의 TAP의 경우 Backstage의 기능으로 구성이 되어 있습니다. ","date":"2023-03-04","objectID":"/tap-techdocs/:1:2","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation TechDocs in Tap","uri":"/tap-techdocs/"},{"categories":["Documentation"],"content":"2. TechDocs 설정 사전 구성 Object Storage : ‘awsS3’ or ‘googleGcs’ or ‘azureBlobStorage’ or ‘openStackSwift’ TANZU TAP npm, node, npx (9.5.0, 18.14.2, 9.5.0)의 버전으로 테스트 진행 앞서 설명에서 MarkDown를 통해 Tanzu의 TAP에 문서형태를 구성 할 수 있습니다. 사전에 Tanzu Net에서 sample을 다운로드 받을 수 있습니다. Sample Download ## 압축 해제 tar zxvf tap-gui-yelb-catalog.tgz ## markdown cd tap-gui-yelb-catalog 위의 Sample 파일을 다운로드 받은 후 TAP에서 Catalog를 등록 한 후 DOCS를 보면 아래와 같이 오류가 나는 것을 확인 할 수 있습니다. Error TANZU에서 TechDocs 관련 LINK는 아래에서 참고 할 수 있지만, 문서를 보면 어떻게 해야 되는지 잘 알지 못할 수도 있습니다. TechDocs In TAP 참고링크 그리고 SAMPLE에서 mkdocs.yml 의 내용을 보면 docs에 index.md, add-docs.md를 읽을 수 있게 구성이 되어져 있는 것을 확인 해 볼 수 있습니다. site_name: 'Yelb demo catalog' nav: - Home: index.md - Adding Documentation: add-docs.md plugins: - techdocs-core spotify에서 제공하는 techdocs docker를 다운로드 합니다. docker pull spotify/techdocs:v1.1.0 테스트 환경에서는 AWS의 S3를 구성 하지 않고, MINIO를 사용하여 구성하였습니다. S3의 계정 정보를 환경변수로 저장합니다. MINIO에 저장 할 수 있는 버킷을 생성 합니다. export AWS_ACCESS_KEY_ID='' export AWS_SECRET_ACCESS_KEY='' export AWS_REGION=ap-northeast-2 페이지를 작성 하기 위해 MarkDown을 npx로 웹 관련 파일을 생성 합니다. npx @techdocs/cli generate --source-dir /var/tmp/tkgm/workload/catalog/yelb-catalog --output-dir ./site ## 아래와 같이 ./site 폴더에 파일들이 생성 된 것을 확인 할 수 있습니다. ls site/ 404.html README.md add-docs assets images index.html search sitemap.xml sitemap.xml.gz techdocs_metadata.json 사전에 Catalog를 등록 하면 DOCS에서 아래처럼 확인이 가능 합니다. yelb-catalog-info 확인 및 site 파일이 생성이 되면 오브젝트 스토리지에 파일들을 업로드 해줍니다. npx @techdocs/cli publish --publisher-type awsS3 --storage-name techdocs --entity default/location/yelb-catalog-info --directory ./site --awsEndpoint https://minio-volumes.huntedhappy.kro.kr --awsS3ForcePathStyle true MINIO BUCKET 여기까지 완료가 되면 tap values yaml 파일을 업데이트 합니다. techdocs: builder: 'external' # Alternatives - 'external' publisher: type: 'awsS3' # Alternatives - 'googleGcs' or 'awsS3' or 'azureBlobStorage' or 'openStackSwift'. Read documentation for using alternatives. awsS3: bucketName: 'techdocs' credentials: accessKeyId: '' secretAccessKey: '' region: ap-northeast-2 s3ForcePathStyle: true endpoint: https://minio-volumes.huntedhappy.kro.kr ## 수정 후 tanzu package installed update tap -n tap-install --values-file {file.yaml} 이 후 완료가 되면 아래 처럼 DOCS를 확인 할 수 있습니다. MARKDOWN을 작성 함으로 인해서 TAP에 개발을 한 Application의 대한 설명을 작성 할 수 있습니다. DOCS1 DOCS2 DOCS3 DOCS4 ","date":"2023-03-04","objectID":"/tap-techdocs/:2:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation TechDocs in Tap","uri":"/tap-techdocs/"},{"categories":["Documentation"],"content":"How to Tanzu Custom Image Create","date":"2023-03-01","objectID":"/tanzu-custom-image/","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"1. Tanzu Custom Image VMware에서 기본적으로 제공하는 이미지외에 별도로 Custom Image를 구성 할 수 있다. 호환되는 OS버전은 아래와 같다. 지원 하는 OS Version 1.6 버전 Custom Image 생성 1.6 Custom-Image 2.1 버전 Custom Image 생성 2.1 Custom-Image TANZU 버전별 Sample Download Sample Downloads 사전에 도커가 설치 되어 있어야 함 RHEL의 경우 Subscription이 되어 있어야 함 ","date":"2023-03-01","objectID":"/tanzu-custom-image/:1:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"2. 공통 작업 Docker Server 실행 docker pull projects.registry.vmware.com/tkg/linux-resource-bundle:v1.23.10_vmware.1-tkg.1 docker run -d -p 3000:3000 projects.registry.vmware.com/tkg/linux-resource-bundle:v1.23.10_vmware.1-tkg.1 Sample 다운로드 ## 1.6 Sample Download curl -L https://apigw.vmware.com/sampleExchange/v1/downloads/8031 -o TKG-Image-Builder-for-Kubernetes-v1.23.10-master.zip unzip TKG-Image-Builder-for-Kubernetes-v1.23.10-master.zip cd TKG-Image-Builder-for-Kubernetes-v1.23.10-on-TKG-v1.6.1-master/TKG-Image-Builder-for-Kubernetes-v1_23_10---vmware_1-tkg_v1_6_1/ ## 2.1 Sample Download curl -L https://apigw.vmware.com/sampleExchange/v1/downloads/8061 -o TKG-Image-Builder-for-Kubernetes-v1.24.9-master.zip unzip TKG-Image-Builder-for-Kubernetes-v1.24.9-master.zip cd /TKG-Image-Builder-for-Kubernetes-v1.24.9-on-TKG-v2.1.0-master/TKG-Image-Builder-for-Kubernetes-v1_24_9---vmware_1-tkg_v2_1_0/ tkg.json 설정 변경 ## 아래 \u003cIP\u003e:\u003cPORT\u003e 설정 cat \u003c\u003c 'EOF' \u003e tkg.json { \"build_version\": \"{{user `build_name`}}-kube-v1.23.10_vmware.1\", \"pause_image\": \"projects.registry.vmware.com/tkg/pause:3.6\", \"containerd_sha256\": \"48f4327570dd7543464a28893160ab3bc9719ed2553f0a529a884b40f6dafd29\", \"containerd_url\": \"http://\u003cIP\u003e:\u003cPORT\u003e/files/containerd/cri-containerd-v1.6.6+vmware.2.linux-amd64.tar\", \"containerd_version\": \"v1.6.6+vmware.2\", \"crictl_sha256\": \"\", \"crictl_url\": \"\", \"custom_role\": \"true\", \"custom_role_names\": \"/home/imagebuilder/tkg\", \"extra_debs\": \"nfs-common unzip apparmor apparmor-utils sysstat\", \"extra_rpms\": \"sysstat\", \"goss_vars_file\": \"\", \"goss_tests_dir\": \"/home/imagebuilder/goss\", \"goss_entry_file\": \"goss/goss.yaml\", \"goss_download_path\": \"/tkg-tmp/goss-linux-amd64\", \"goss_remote_folder\": \"/tkg-tmp\", \"goss_remote_path\": \"/tkg-tmp/goss\", \"kubernetes_series\": \"v1.23\", \"kubernetes_semver\": \"v1.23.10+vmware.1\", \"kubernetes_source_type\": \"http\", \"kubernetes_http_source\": \"http://\u003cIP\u003e:\u003cPORT\u003e/files/kubernetes\", \"kubernetes_container_registry\": \"projects.registry.vmware.com/tkg\", \"kubernetes_cni_semver\": \"v1.1.1+vmware.7\", \"kubernetes_cni_source_type\": \"http\", \"kubernetes_cni_http_source\": \"http://\u003cIP\u003e:\u003cPORT\u003e/files/cni_plugins\", \"kubernetes_cni_http_checksum\": \"\", \"kubernetes_load_additional_imgs\": \"true\" } EOF vCenter 및 Tanzu Kubernetes 버전 설정 ## vCenter의 정보를 넣어 준다. cat \u003c\u003c EOF \u003e vsphere.json { \"vcenter_server\": \"\", \"username\": \"\", \"password\": \"\", \"insecure_connection\": \"true\", \"datacenter\": \"Datacenter\", \"cluster\": \"Cluster\", \"resource_pool\": \"\", \"folder\": \"tanzu\", \"datastore\": \"vsanDatastore\", \"network\": \"VM Network\", \"convert_to_template\": \"false\", \"create_snapshot\": \"false\", \"linked_clone\": \"false\", \"template\": \"false\" } EOF ## 설치할 버전 설정 ## 1.6 cat \u003c\u003c EOF \u003e metadata.json { \"VERSION\": \"v1.23.10+vmware.1-dokyung.0\" } EOF ## 2.1 { \"VERSION\": \"v1.24.9+vmware.1-dokyung.0\" } ","date":"2023-03-01","objectID":"/tanzu-custom-image/:2:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"3. Ubuntu Custom Image 생성 이미지 생성 실행 mkdir output ## 1.6 설정 docker run --net=host -it --rm \\ -v $(pwd)/vsphere.json:/home/imagebuilder/vsphere.json \\ -v $(pwd)/tkg.json:/home/imagebuilder/tkg.json \\ -v $(pwd)/tkg:/home/imagebuilder/tkg \\ -v $(pwd)/stig_ubuntu_2004:/home/imagebuilder/stig_ubuntu_2004 \\ -v $(pwd)/goss/vsphere-ubuntu-1.23.10+vmware.2-goss-spec.yaml:/home/imagebuilder/goss/goss.yaml \\ -v $(pwd)/metadata.json:/home/imagebuilder/metadata.json \\ -v $(pwd)/output:/home/imagebuilder/output \\ --env PACKER_VAR_FILES=\"tkg.json vsphere.json\" \\ --env OVF_CUSTOM_PROPERTIES=/home/imagebuilder/metadata.json \\ --env IB_OVFTOOL=1 \\ projects.registry.vmware.com/tkg/image-builder:v0.1.13_vmware.2 \\ build-node-ova-vsphere-ubuntu-2004 ## 2.1 설정 docker run -it --rm \\ -v $(pwd)/vsphere.json:/home/imagebuilder/vsphere.json \\ -v $(pwd)/tkg.json:/home/imagebuilder/tkg.json \\ -v $(pwd)/tkg:/home/imagebuilder/tkg \\ -v $(pwd)/goss/vsphere-ubuntu-1.24.9+vmware.1-goss-spec.yaml:/home/imagebuilder/goss/goss.yaml \\ -v $(pwd)/metadata.json:/home/imagebuilder/metadata.json \\ -v $(pwd)/output:/home/imagebuilder/output \\ --env PACKER_VAR_FILES=\"tkg.json vsphere.json\" \\ --env OVF_CUSTOM_PROPERTIES=/home/imagebuilder/metadata.json \\ --env IB_OVFTOOL=1 \\ --network host \\ projects.registry.vmware.com/tkg/image-builder:v0.1.13_vmware.2 \\ build-node-ova-vsphere-ubuntu-2004 LOG ## Ubuntu Custom Image 생성시 아래와 같은 Log를 볼 수 있다, vCenter에서는 Template으로 생성이 되는 것을 확인 할 수 있다. hack/ensure-ansible.sh Starting galaxy collection install process Nothing to do. All requested collections are already installed. If you want to reinstall them, consider using `--force`. hack/ensure-ansible-windows.sh hack/ensure-packer.sh hack/ensure-goss.sh Right version of binary present hack/ensure-ovftool.sh packer build -var-file=\"/home/imagebuilder/packer/config/kubernetes.json\" -var-file=\"/home/imagebuilder/packer/config/cni.json\" -var-file=\"/home/imagebuilder/packer/config/containerd.json\" -var-file=\"/home/imagebuilder/packer/config/ansible-args.json\" -var-file=\"/home/imagebuilder/packer/config/goss-args.json\" -var-file=\"/home/imagebuilder/packer/config/common.json\" -var-file=\"/home/imagebuilder/packer/config/additional_components.json\" -color=true -var-file=\"packer/ova/packer-common.json\" -var-file=\"/home/imagebuilder/packer/ova/ubuntu-2004.json\" -var-file=\"packer/ova/vsphere.json\" -except=local -only=vsphere-iso -var-file=\"/home/imagebuilder/tkg.json\" -var-file=\"/home/imagebuilder/vsphere.json\" -only=vsphere packer/ova/packer-node.json vsphere: output will be in this color. ==\u003e vsphere: File /home/imagebuilder/.cache/packer/48e4ec4daa32571605576c5566f486133ecc271f.iso already uploaded; continuing ==\u003e vsphere: File [vsanDatastore] packer_cache//48e4ec4daa32571605576c5566f486133ecc271f.iso already exists; skipping upload. ==\u003e vsphere: Creating VM... ==\u003e vsphere: Customizing hardware... ==\u003e vsphere: Mounting ISO images... ==\u003e vsphere: Adding configuration parameters... ==\u003e vsphere: Creating floppy disk... vsphere: Copying files flatly from floppy_files vsphere: Done copying files from floppy_files vsphere: Collecting paths from floppy_dirs vsphere: Resulting paths from floppy_dirs : [./packer/ova/linux/ubuntu/http/] vsphere: Recursively copying : ./packer/ova/linux/ubuntu/http/ vsphere: Done copying paths from floppy_dirs vsphere: Copying files from floppy_content vsphere: Done copying files from floppy_content ==\u003e vsphere: Uploading created floppy image ==\u003e vsphere: Adding generated Floppy... ==\u003e vsphere: Starting HTTP server on port 8033 ==\u003e vsphere: Set boot order temporary... ==\u003e vsphere: Power on VM... ==\u003e vsphere: Waiting 10s for boot... ==\u003e vsphere: HTTP server is working at http://10.253.126.163:8033/ ==\u003e vsphere: Typing boot command... ==\u003e vsphere: Waiting for IP... ==\u003e vsphere: IP address: 10.253.126.102 ==\u003e vsphere: Using SSH communicator to connect: 10.253.126.102 ==\u003e vsphere: Waiting for SSH to become available... ==\u003e vsphere: Connected to SSH! ==\u003e vsphere: Provisioning wit","date":"2023-03-01","objectID":"/tanzu-custom-image/:3:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"4. RHEL Custom Image 생성 rhel-8.json 생성 cat \u003c\u003c 'EOF' \u003e rhel-8.json { \"boot_command_prefix\": \"\u003cup\u003e\u003ctab\u003e text inst.ks=\", \"boot_command_suffix\": \"/8/ks.cfg\u003center\u003e\u003cwait\u003e\", \"boot_media_path\": \"http://{{ .HTTPIP }}:{{ .HTTPPort }}\", \"build_name\": \"rhel-8\", \"distro_arch\": \"amd64\", \"distro_name\": \"rhel\", \"distro_version\": \"8\", \"epel_rpm_gpg_key\": \"https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-8\", \"guest_os_type\": \"rhel8-64\", \"http_directory\": \"./packer/ova/linux/{{user `distro_name`}}/http/\", \"iso_checksum\": \"a6a7418a75d721cc696d3cbdd648b5248808e7fef0f8742f518e43b46fa08139\", \"iso_checksum_type\": \"sha256\", \"iso_url\": \"./rhel-8.7-x86_64-dvd.iso\", \"os_display_name\": \"RHEL 8\", \"redhat_epel_rpm\": \"https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm\", \"shutdown_command\": \"shutdown -P now\", \"vsphere_guest_os_type\": \"rhel8_64Guest\" } EOF rhel-8 생성 ## RHSM_USER= --env RHSM_PASS= \\ -- subscription user/pw 설정 ## 1.6 설정 docker run -it --rm \\ -v $(pwd)/vsphere.json:/home/imagebuilder/vsphere.json \\ -v $(pwd)/tkg.json:/home/imagebuilder/tkg.json \\ -v $(pwd)/tkg:/home/imagebuilder/tkg \\ -v $(pwd)/goss/vsphere-ubuntu-1.23.10+vmware.2-goss-spec.yaml:/home/imagebuilder/goss/goss.yaml \\ -v $(pwd)/metadata.json:/home/imagebuilder/metadata.json \\ -v $(pwd)/output:/home/imagebuilder/output \\ -v $(pwd)/rhel-8.json:/home/imagebuilder/packer/ova/rhel-8.json \\ -v $(pwd)/ks.cfg:/home/imagebuilder/packer/ova/linux/photon/http/3/ks.cfg \\ -v $(pwd)/rhel-8.7-x86_64-dvd.iso:/home/imagebuilder/rhel-8.7-x86_64-dvd.iso \\ --network host \\ --env RHSM_USER={ID} --env RHSM_PASS={PW} \\ --env PACKER_VAR_FILES=\"tkg.json vsphere.json\" \\ --env OVF_CUSTOM_PROPERTIES=/home/imagebuilder/metadata.json \\ --env IB_OVFTOOL=1 \\ --env IB_OVFTOOL_ARGS=\"--allowExtraConfig\" \\ projects.registry.vmware.com/tkg/image-builder:v0.1.13_vmware.2 \\ build-node-ova-vsphere-rhel-8 ## 2.1 설정 docker run -it --rm \\ -v $(pwd)/vsphere.json:/home/imagebuilder/vsphere.json \\ -v $(pwd)/tkg.json:/home/imagebuilder/tkg.json \\ -v $(pwd)/tkg:/home/imagebuilder/tkg \\ -v $(pwd)/goss/vsphere-rhel-8-v1.24.9+vmware.1-tkg_v2_1_0-goss-spec.yaml:/home/imagebuilder/goss/goss.yaml \\ -v $(pwd)/metadata.json:/home/imagebuilder/metadata.json \\ -v $(pwd)/output:/home/imagebuilder/output \\ -v $(pwd)/rhel-8.json:/home/imagebuilder/packer/ova/rhel-8.json \\ -v $(pwd)/ks.cfg:/home/imagebuilder/packer/ova/linux/photon/http/3/ks.cfg \\ -v $(pwd)/rhel-8.7-x86_64-dvd.iso:/home/imagebuilder/rhel-8.7-x86_64-dvd.iso \\ --network host \\ --env RHSM_USER={ID} --env RHSM_PASS={PW} \\ --env PACKER_VAR_FILES=\"tkg.json vsphere.json\" \\ --env OVF_CUSTOM_PROPERTIES=/home/imagebuilder/metadata.json \\ --env IB_OVFTOOL=1 \\ --env IB_OVFTOOL_ARGS=\"--allowExtraConfig\" \\ projects.registry.vmware.com/tkg/image-builder:v0.1.13_vmware.2 \\ build-node-ova-vsphere-rhel-8 LOG ","date":"2023-03-01","objectID":"/tanzu-custom-image/:4:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"rhel 8 이미지 로그 hack/ensure-ansible.sh Starting galaxy collection install process Nothing to do. All requested collections are already installed. If you want to reinstall them, consider using `--force`. hack/ensure-ansible-windows.sh hack/ensure-packer.sh hack/ensure-goss.sh Right version of binary present hack/ensure-ovftool.sh packer build -var-file=\"/home/imagebuilder/packer/config/kubernetes.json\" -var-file=\"/home/imagebuilder/packer/config/cni.json\" -var-file=\"/home/imagebuilder/packer/config/containerd.json\" -var-file=\"/home/imagebuilder/packer/config/ansible-args.json\" -var-file=\"/home/imagebuilder/packer/config/goss-args.json\" -var-file=\"/home/imagebuilder/packer/config/common.json\" -var-file=\"/home/imagebuilder/packer/config/additional_components.json\" -color=true -var-file=\"packer/ova/packer-common.json\" -var-file=\"/home/imagebuilder/packer/ova/rhel-8.json\" -var-file=\"packer/ova/vsphere.json\" -except=local -only=vsphere-iso -var-file=\"/home/imagebuilder/tkg.json\" -var-file=\"/home/imagebuilder/vsphere.json\" -only=vsphere packer/ova/packer-node.json vsphere: output will be in this color. ==\u003e vsphere: Retrieving ISO ==\u003e vsphere: Trying ./rhel-8.7-x86_64-dvd.iso ==\u003e vsphere: Trying ./rhel-8.7-x86_64-dvd.iso?checksum=sha256%3Aa6a7418a75d721cc696d3cbdd648b5248808e7fef0f8742f518e43b46fa08139 ==\u003e vsphere: ./rhel-8.7-x86_64-dvd.iso?checksum=sha256%3Aa6a7418a75d721cc696d3cbdd648b5248808e7fef0f8742f518e43b46fa08139 =\u003e /home/imagebuilder/rhel-8.7-x86_64-dvd.iso ==\u003e vsphere: Uploading rhel-8.7-x86_64-dvd.iso to packer_cache/rhel-8.7-x86_64-dvd.iso ==\u003e vsphere: Creating VM... ==\u003e vsphere: Customizing hardware... ==\u003e vsphere: Mounting ISO images... ==\u003e vsphere: Adding configuration parameters... ==\u003e vsphere: Starting HTTP server on port 8709 ==\u003e vsphere: Set boot order temporary... ==\u003e vsphere: Power on VM... ==\u003e vsphere: Waiting 10s for boot... ==\u003e vsphere: HTTP server is working at http://10.253.126.163:8709/ ==\u003e vsphere: Typing boot command... ==\u003e vsphere: Waiting for IP... ==\u003e vsphere: IP address: 10.253.126.195 ==\u003e vsphere: Using SSH communicator to connect: 10.253.126.195 ==\u003e vsphere: Waiting for SSH to become available... ==\u003e vsphere: Connected to SSH! ==\u003e vsphere: Provisioning with shell script: ./packer/files/flatcar/scripts/bootstrap-flatcar.sh ==\u003e vsphere: Provisioning with Ansible... vsphere: Setting up proxy adapter for Ansible.... ==\u003e vsphere: Executing Ansible: ansible-playbook -e packer_build_name=\"vsphere\" -e packer_*****_type=vsphere-iso -e packer_http_addr=10.253.126.163:8709 --ssh-extra-args '-o IdentitiesOnly=yes' --extra-vars containerd_url=http://10.253.126.163:3000/files/containerd/cri-containerd-v1.6.6+vmware.2.linux-amd64.tar containerd_sha256=48f4327570dd7543464a28893160ab3bc9719ed2553f0a529a884b40f6dafd29 pause_image=projects.registry.vmware.com/tkg/pause:3.6 containerd_additional_settings= containerd_cri_socket=/var/run/containerd/containerd.sock containerd_version=v1.6.6+vmware.2 crictl_url= crictl_sha256= crictl_source_type=pkg custom_role_names=\"/home/image*****/tkg\" firstboot_custom_roles_pre=\"\" firstboot_custom_roles_post=\"\" node_custom_roles_pre=\"\" node_custom_roles_post=\"\" disable_public_repos=false extra_debs=\"nfs-common unzip apparmor apparmor-utils sysstat\" extra_repos=\"\" extra_rpms=\"sysstat\" http_proxy= https_proxy= kubeadm_template=etc/kubeadm.yml kubernetes_cni_http_source=http://10.253.126.163:3000/files/cni_plugins kubernetes_cni_http_checksum= kubernetes_http_source=http://10.253.126.163:3000/files/kubernetes kubernetes_container_registry=projects.registry.vmware.com/tkg kubernetes_rpm_repo=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 kubernetes_rpm_gpg_key=\"https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\" kubernetes_rpm_gpg_check=True kubernetes_deb_repo=\"https://apt.kubernetes.io/ kubernetes-xenial\" kubernetes_deb_gpg_key=https://packages.cloud.google.com/apt/doc/apt-key.gpg kubernetes_cni_deb_versio","date":"2023-03-01","objectID":"/tanzu-custom-image/:5:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"5. Photon Custom Image 생성 LOG ## Photon Custom Image 생성시 아래와 같은 Log를 볼 수 있다, vCenter에서는 Template으로 생성이 되는 것을 확인 할 수 있다. hack/ensure-ansible.sh Starting galaxy collection install process Nothing to do. All requested collections are already installed. If you want to reinstall them, consider using `--force`. hack/ensure-ansible-windows.sh hack/ensure-packer.sh hack/ensure-goss.sh Right version of binary present hack/ensure-ovftool.sh packer build -var-file=\"/home/imagebuilder/packer/config/kubernetes.json\" -var-file=\"/home/imagebuilder/packer/config/cni.json\" -var-file=\"/home/imagebuilder/packer/config/containerd.json\" -var-file=\"/home/imagebuilder/packer/config/ansible-args.json\" -var-file=\"/home/imagebuilder/packer/config/goss-args.json\" -var-file=\"/home/imagebuilder/packer/config/common.json\" -var-file=\"/home/imagebuilder/packer/config/additional_components.json\" -color=true -var-file=\"packer/ova/packer-common.json\" -var-file=\"/home/imagebuilder/packer/ova/photon-3.json\" -var-file=\"packer/ova/vsphere.json\" -except=local -only=vsphere-iso -var-file=\"/home/imagebuilder/tkg.json\" -var-file=\"/home/imagebuilder/vsphere.json\" -only=vsphere packer/ova/packer-node.json vsphere: output will be in this color. ==\u003e vsphere: File /home/imagebuilder/.cache/packer/2d88648c04e690990b2940ca3710b0baadf15256.iso already uploaded; continuing ==\u003e vsphere: File [vsanDatastore] packer_cache//2d88648c04e690990b2940ca3710b0baadf15256.iso already exists; skipping upload. ==\u003e vsphere: Creating VM... ==\u003e vsphere: Customizing hardware... ==\u003e vsphere: Mounting ISO images... ==\u003e vsphere: Adding configuration parameters... ==\u003e vsphere: Starting HTTP server on port 8897 ==\u003e vsphere: Set boot order temporary... ==\u003e vsphere: Power on VM... ==\u003e vsphere: Waiting 10s for boot... ==\u003e vsphere: HTTP server is working at http://10.253.126.163:8897/ ==\u003e vsphere: Typing boot command... ==\u003e vsphere: Waiting for IP... ==\u003e vsphere: IP address: 10.253.126.198 ==\u003e vsphere: Using SSH communicator to connect: 10.253.126.198 ==\u003e vsphere: Waiting for SSH to become available... ==\u003e vsphere: Connected to SSH! ==\u003e vsphere: Provisioning with shell script: ./packer/files/flatcar/scripts/bootstrap-flatcar.sh ==\u003e vsphere: Provisioning with Ansible... vsphere: Setting up proxy adapter for Ansible.... ==\u003e vsphere: Executing Ansible: ansible-playbook -e packer_build_name=\"vsphere\" -e packer_*****_type=vsphere-iso -e packer_http_addr=10.253.126.163:8897 --ssh-extra-args '-o IdentitiesOnly=yes' --extra-vars containerd_url=http://10.253.126.163:3000/files/containerd/cri-containerd-v1.6.6+vmware.2.linux-amd64.tar containerd_sha256=48f4327570dd7543464a28893160ab3bc9719ed2553f0a529a884b40f6dafd29 pause_image=projects.registry.vmware.com/tkg/pause:3.6 containerd_additional_settings= containerd_cri_socket=/var/run/containerd/containerd.sock containerd_version=v1.6.6+vmware.2 crictl_url= crictl_sha256= crictl_source_type=pkg custom_role_names=\"/home/image*****/tkg\" firstboot_custom_roles_pre=\"\" firstboot_custom_roles_post=\"\" node_custom_roles_pre=\"\" node_custom_roles_post=\"\" disable_public_repos=false extra_debs=\"nfs-common unzip apparmor apparmor-utils sysstat\" extra_repos=\"\" extra_rpms=\"sysstat\" http_proxy= https_proxy= kubeadm_template=etc/kubeadm.yml kubernetes_cni_http_source=http://10.253.126.163:3000/files/cni_plugins kubernetes_cni_http_checksum= kubernetes_http_source=http://10.253.126.163:3000/files/kubernetes kubernetes_container_registry=projects.registry.vmware.com/tkg kubernetes_rpm_repo=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 kubernetes_rpm_gpg_key=\"https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\" kubernetes_rpm_gpg_check=True kubernetes_deb_repo=\"https://apt.kubernetes.io/ kubernetes-xenial\" kubernetes_deb_gpg_key=https://packages.cloud.google.com/apt/doc/apt-key.gpg kubernetes_cni_deb_version=1.1.1-00 kubernetes_cni_rpm_version=1.1.1-0 kubernetes_cni_semver=v1.1.1+vmware.7 kubernetes_cni_","date":"2023-03-01","objectID":"/tanzu-custom-image/:6:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"6. TKR 등록 TKR 등록 ## 1.6 vi ~/.config/tanzu/tkg/bom/tkr-bom-v1.23.10+vmware.1-tkg.1.yaml ## 아래 ova를 찾아서 추가 해준다. ova: - name: ova-photon-3-rt osinfo: name: photon version: \"3\" arch: amd64 version: v1.23.10+vmware.1-dokyung.0 - name: ova-ubuntu-2004-rt osinfo: name: ubuntu version: \"20.04\" arch: amd64 version: v1.23.10+vmware.1-dokyung.0 - name: ova-rhel-8-rt osinfo: name: rhel version: \"8\" arch: amd64 version: v1.23.10+vmware.1-dokyung.0 ## 2.1 vi ~/.config/tanzu/tkg/bom/tkr-bom-v1.24.9+vmware.1-tkg.1.yaml ## 아래 ova를 찾아서 추가 해준다. ova: - name: ova-photon-3-rt osinfo: name: photon version: \"3\" arch: amd64 version: v1.24.9+vmware.1-dokyung.0 - name: ova-ubuntu-2004-rt osinfo: name: ubuntu version: \"20.04\" arch: amd64 version: v1.24.9+vmware.1-dokyung.0 - name: ova-rhel-8-rt osinfo: name: rhel version: \"8\" arch: amd64 version: v1.24.9+vmware.1-dokyung.0 ","date":"2023-03-01","objectID":"/tanzu-custom-image/:7:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"7. OVA 업로드 업로드 ## 1.6 ### photon ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/photon-3-kube-v1.23.10_vmware.1/photon-3-kube-v1.23.10+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ### ubuntu ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/ubuntu-2004-kube-v1.23.10_vmware.1/ubuntu-2004-kube-v1.23.10+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ### rhel ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/rhel-8-kube-v1.23.10_vmware.1/rhel-8-kube-v1.23.10+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ## 2.1 ### photon ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/photon-3-kube-v1.24.9_vmware.1/photon-3-kube-v1.24.9+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ### ubuntu ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/ubuntu-2004-kube-v1.24.9_vmware.1/ubuntu-2004-kube-v1.24.9+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ### rhel ovftool --acceptAllEulas --net:nic0={NIC}--datastore={STORE} --vmFolder={FOLDER} --importAsTemplate output/rhel-8-kube-v1.24.9_vmware.1/rhel-8-kube-v1.24.9+vmware.1.ova 'vi://{ID}:{PW}}@{vCenter FQDN or IP}/{DATACENTER}/host/{CLUSTER}/' ","date":"2023-03-01","objectID":"/tanzu-custom-image/:8:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"6. 배포 클러스터 배포 tanzu cluster create -f {FILE} -v 9 -y ","date":"2023-03-01","objectID":"/tanzu-custom-image/:9:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"7. 확인 1.6 rhel node photon node 2.1 All node ","date":"2023-03-01","objectID":"/tanzu-custom-image/:10:0","tags":["tanzu","k8s","TAP","tanzu Custom Image","devops","dk","dokyung"],"title":"The Documentation TANZU CUSTOM IMAGE","uri":"/tanzu-custom-image/"},{"categories":["Documentation"],"content":"TANZU Application Platform \u0026 Openshift","date":"2023-01-29","objectID":"/tapandopenshift/","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"1. TANZU APPLICATION PLATFORM ON OPENSHIFT Tanzu Application Platform은 VMware에서 제공하는 CI/CD 솔루션입니다. 기본 VMware제품들은 대부분 VMware 솔루션에 Dependency가 있었습니다. 그러나 TAP의 경우는 K8S 환경이면 어디든 설치가 된다는 컨셉으로 나온 것이 아닌가 싶습니다. 1.3.x 부터 OPENSHIFT에서도 지원을 한다고 해서 한번 설치를 진행 해 보았으며 어떻게 설치를 할 수 있는지 한번 알아 보도록 하겠습니다. 기본적으로 OPENSHIFT는 설치가 되어 있다는 것을 가정으로 TAP를 설치를 진행을 하겠습니다. 참고로 OPENSHIFT의 경우 TAP에서 지원하는 것은 4.10 , 4.11 두가지를 지원 합니다. ocp 지원 버전 ocp 설치 버전 ocp 설치 버전 설치 버전 Openshift Version 4.11.9 Tanzu Application Platform 1.4.0 TBS 1.9.0 AVI 22.1.2 처음에 Openshift(Openshift를 잘 모르다 보니)를 통해 TAP를 배포하려고 하다 보니 여러가지 이슈가 발생했는대, 해결이 안되는 부분이 AVI 그러니까 외부 로드밸런서를 사용하지 않고 Openshift가 가지고 있는 Ingress를 사용하려고 했는대 실패를 하였습니다. 또한 문제는 Openshift를 잘 몰라서 그럴 수 있겠지만, 내가 원하는 Domain을 설정 하는 것도 어려운 부분이 있었습니다. 그래서 우선 별도로 AVI를 구성하여 설치를 진행 후 TAP 설치를 하였습니다. 그리고 설명이 부족한 부분이 많을 수 있는대, 설명 할 것이 너무 많기 때문에 좀더 Install에 대해서 집중을 해서 글을 작성 하였습니다. 아래에 route에 포함되어 있지 않은 도메인을 차단 하는것인지.. 여기 route에 tap를 구성 후 tap-gui 또는 어플리케이션의 대해서 어떻게 설정 해야 되는지는 아직 의문이 남아 있습니다. oc route 확인 AVI 와 Openshift를 연동시 생성 되는 VIP가 많아서 이 부분도 당황 스러운 부분이 있었지만, 이 부분도 깊게 파고 들지는 않았습니다. 우선 목표는 TAP를 구성 하기 위함이 크기 때문입니다. AVI 상태 ","date":"2023-01-29","objectID":"/tapandopenshift/:1:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"2. KAPP 설치 기본적으로 KAPP이 설치가 되어 있지 않기 때문에 KAPP 설치가 필요 합니다. 이유는 TAP이라는 솔루션이 Tanzu라는 명령어를 통해 설치가 되기 때문에 입니다. ## package를 설치 할 수 있게 kapp-contoller 설치 kubectl apply -f https://github.com/vmware-tanzu/carvel-kapp-controller/releases/latest/download/release.yml ## tanzu에서 secret을 생성 할 수 있게 secretget-controller 설치 kubectl apply -f https://github.com/carvel-dev/secretgen-controller/releases/latest/download/release.yml kapp을 설치를 하게 되면 이제 tanzu package repository 및 secret repository를 등록 할 수 있습니다. package 및 reository 그러면 위와 같이 list를 확인 할 수 있습니다. 위에 controller를 설치를 하지 않으면 에러가 발생 하는 것을 알 수 있습니다. 에러 내용은 아래와 같습니다. kapp contoller를 설치를 안할 경우 발생하는 에러 ## Error: failed to check for the availability of 'packaging.carvel.dev' API: failed to discover unmatched GroupVersionResources: the server is currently unable to handle the request ## Error: exit status 1 secret contoller를 설치를 안할 경우 발생하는 에러 ## Error: secret plugin can not be used as 'secretgen.carvel.dev/v1alpha1' API is not available in the cluster ## Error: exit status 1 ","date":"2023-01-29","objectID":"/tapandopenshift/:2:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"3. TANZU FRAMEWORK 및 Pcakage Image Repository 업로드 TANZU FRAMEWORK다운로드는 Tanzu Net에서 다운로드를 받을 수 있다. TANZU APPLICATION PLATFORM 설치는 1.4.0으로 진행 tar xvf tanzu-framework-linux-amd64-v0.25.4.1.tar tanzu plugin install -l ./cli/ all ## 이미지를 받기 위해 tanzu net에 로그인 한다. docker login registry.tanzu.vmware.com -u {tanzuusername} ## 이후에 내부 또는 gcr 등등 repository에 로그인 한다. 테스트는 내부 Harbor를 사용한다. docker login infra-harbor.huntedhappy.kro.kr -u admin ## TAP Package 내부 Harbor에 업로드 imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:1.4.0 --to-repo infra-harbor.huntedhappy.kro.kr/tap-packages/1.4.0 ","date":"2023-01-29","objectID":"/tapandopenshift/:3:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"4. TAP Package 등록 ## namespace 생성 kubectl create ns tap-install ## Harbor Secret 사용할 경우 kubectl create secret docker-registry registry-credentials --docker-server=infra-harbor.huntedhappy.kro.kr --docker-username=admin --docker-password=${INSTALL_REGISTRY_PASSWORD} -n tap-install ## package repository 추가 tanzu package repository add tanzu-tap-repository \\ --url harbor-infra.huntedhappy.kro.kr/tap-packages/1.4.0 \\ --namespace tap-install ## TBS 설치 imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/full-tbs-deps-package-repo:1.9.0 --to-tar=tbs-full-deps-1.9.0.tar ## TBS Harobr업로드 imgpkg copy --tar tbs-full-deps-1.9.0.tar --to-repo harbor-infra.huntedhappy.kro.kr/tap/tbs-full-deps/1.9.0 --include-non-distributable-layers ## TBS Repository 등록 tanzu package repository add tbs-full-deps-repository \\ --url infra-harbor.huntedhappy.kro.kr/tap/tbs-full-deps/1.9.0 \\ --namespace tap-install ","date":"2023-01-29","objectID":"/tapandopenshift/:4:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"5. 설치 TAP같은 경우 멀티 클러스터를 지원 하지만, 테스트는 full 기반으로 테스트를 진행 하였습니다. 설치는 ootb_supply_chain_testing_scanning 로 테스트를 진행 했지만. 좀 복잡 할 수 있기 때문에 여기에는 ootb_supply_chain_basic으로 기술 합니다. vi tap-full-vaules.yaml profile: full ceip_policy_disclosed: true buildservice: kp_default_repository: \"infra-harbor.huntedhappy.kro.kr/tap/tap-packages\" kp_default_repository_secret: name: \"registry-credentials\" namespace: \"tap-install\" exclude_dependencies: true springboot_conventions: autoConfigureActuators: true policy: tuf_enable: true accelerator: domain: huntedhappy.kro.kr ingress: include: true enable_tls: true tls: secret_name: share-secret namespace: tap-install server: service_type: \"ClusterIP\" watched_namespace: \"accelerator-system\" samples: include: false shared: kubernetes_distribution: \"openshift\" # To be passed only for OpenShift. Defaults to \"\". ingress_issuer: default-ca-issuer image_registry: secret: name: \"registry-credentials\" namespace: \"tap-install\" ingress_domain: \"huntedhappy.kro.kr\" # ca_cert_data: | appliveview: ingressEnabled: true sslDisabled: false tls: namespace: tap-install secretName: share-secret appliveview_connector: backend: sslDisabled: false host: appliveview.huntedhappy.kro.kr cnrs: domain_name: huntedhappy.kro.kr domain_template: \"{{.Name}}.{{.Domain}}\" default_tls_secret: tap-install/share-secret scanning: metadataStore: url: \"\" # Disable embedded integration since it's deprecated metadata_store: ns_for_export_app_cert: \"*\" ingress_domain: huntedhappy.kro.kr app_service_type: ClusterIP ingress_enabled: \"true\" targetImagePullSecret: \"registry-credentials\" tls: secretName: share-secret namespace: tap-install supply_chain: basic ootb_supply_chain_basic: registry: server: \"infra-harbor.huntedhappy.kro.kr\" repository: \"app/supply_chain\" grype: namespace: \"tap-install\" # (optional) Defaults to default namespace. targetImagePullSecret: \"registry-credentials\" tap_gui: service_type: ClusterIP ingressEnabled: \"true\" ingressDomain: \"huntedhappy.kro.kr\" tls: namespace: tap-install secretName: share-secret app_config: organization: name: 'huntedhappy' app: title: 'huntedhappy TAP' baseUrl: https://tap-gui.huntedhappy.kro.kr support: url: https://tanzu.vmware.com/support items: - title: Contact Support icon: email links: - url: https://tanzu.vmware.com/support title: Tanzu Support Page - title: Documentation icon: doc links: - url: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/index.html title: Tanzu Application Platform Documentation backend: baseUrl: https://tap-gui.huntedhappy.kro.kr cors: origin: https://tap-gui.huntedhappy.kro.kr contour: certificates.duration: 87600h certificates.renewBefore: 360h envoy: service: type: LoadBalancer excluded_packages: - policy.apps.tanzu.vmware.com - learningcenter.tanzu.vmware.com - workshops.learningcenter.tanzu.vmware.com ## rback 설정 cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: tap-gui --- apiVersion: v1 kind: ServiceAccount metadata: namespace: tap-gui name: tap-gui-viewer automountServiceAccountToken: false --- apiVersion: v1 kind: Secret metadata: name: tap-gui-viewer namespace: tap-gui annotations: kubernetes.io/service-account.name: tap-gui-viewer type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tap-gui-read-k8s subjects: - kind: ServiceAccount namespace: tap-gui name: tap-gui-viewer roleRef: kind: ClusterRole name: k8s-reader apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: k8s-reader rules: - apiGroups: [''] resources: ['pods', 'pods/log', 'services', 'configmaps'] verbs: ['get', 'watch', 'list'] - apiGroups: ['apps'] resources: ['deployments', 'replicasets'] verbs: ['get', 'watch', 'list'] - apiGroups: ['autoscaling'] resources: ['horizontalpodautoscalers'] verbs: ['get', 'watch', 'list'] - apiGroups: ['networking.k8s.io'] resources: ['ingresses'] verbs: ['get', 'watch', 'list'] - apiGroups: ","date":"2023-01-29","objectID":"/tapandopenshift/:5:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"6. 설치 완료 설치 완료#1 Sample App Test 설치 완료#2 Sample App Test를 하게 될 경우 아래와 같은 문구가 나오는대 왜 나오는지는 아직 파악 하지 못하였습니다. 하지만 SAMPLE APP 배포는 잘 동작 하는 것을 확인 할 수 있습니다. I0129 14:54:36.521207 2768383 request.go:682] Waited for 1.007213447s due to client-side throttling, not priority and fairness, request: GET:https://api.openshift.huntedhappy.kro.kr:6443/apis/console.openshift.io/v1alpha1?timeout=32s 배포가 완료 되면 httpproxy,pod,deliverable을 확인 하여 httpproxy에 설정된 도메인으로 접속 할 수 있다. FQDN 확인 설치 완료#2 TAP GUI 화면#1 TAP GUI 화면#2 TAP GUI 화면#2 설치가 완료 되면 TAP의 대해서는 Visual Sutudio Code, IntelliJ 환경에서도 잘 동작하는 것을 확인 할 수 있습니다. 나중에 추가적인 설명을 할 수 있으면 적을 수 있도록 하겠습니다. ","date":"2023-01-29","objectID":"/tapandopenshift/:6:0","tags":["tanzu","k8s","Openshift","tap on openshift","devops","dk","dokyung"],"title":"Tap install on Openshift","uri":"/tapandopenshift/"},{"categories":["Documentation"],"content":"Harbor Disk Size Expand","date":"2023-01-07","objectID":"/harbor/","tags":["harbor","tanzu","k8s","devops","dk","dokyung","slack","notification"],"title":"The Documentation Harbor","uri":"/harbor/"},{"categories":["Documentation"],"content":"Harbor에 VM Disk가 부족 하게 되면 사이즈를 늘려보자 ","date":"2023-01-07","objectID":"/harbor/:1:0","tags":["harbor","tanzu","k8s","devops","dk","dokyung","slack","notification"],"title":"The Documentation Harbor","uri":"/harbor/"},{"categories":["Documentation"],"content":"1. Harbor VM DISK 증설 df -h fdisk -l ## 아래와 같이 에러가 나온다면 \u003e\u003e GPT PMBR size mismatch ## parted로 fix를 해보자 parted /dev/sda p \u003e f Disk 상태 확인 Disk 증설 Disk 증설 후 ## 그리고 온라인 상태에서 디스크가 보이지 않은 경우는 아래와 같이 하자. ls /sys/class/scsi_host for line in $(ls /sys/class/scsi_host) do echo \"- - -\" \u003e /sys/class/scsi_host/$line/scan done ls -lat /dev/sd* Disk 증설 후 SCAN 디스크 증설 확인#1 디스크 증설 확인#2 pvs lvs pvcreate /dev/sdb vgextend ubuntu-vg /dev/sdb lvextend /dev/mapper/ubuntu--vg-ubuntu--lv /dev/sdb resize2fs -p /dev/mapper/ubuntu--vg-ubuntu--lv Disk 확인 및 증설#1 Disk 확인 및 증설#2 ","date":"2023-01-07","objectID":"/harbor/:2:0","tags":["harbor","tanzu","k8s","devops","dk","dokyung","slack","notification"],"title":"The Documentation Harbor","uri":"/harbor/"},{"categories":["Documentation"],"content":"2. TANZU에 Pacakge로 설치 한 Harbor 사이즈 증설 kubectl -n tanzu-system-registry get pvc --selector=component=registry --show-labels ## 아래 storage 용량을 설정한다. cat \u003c\u003c EOF \u003e harbor-registry-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: finalizers: - kubernetes.io/pvc-protection labels: app: harbor component: registry kapp.k14s.io/app: \"1610567506920108209\" kapp.k14s.io/association: v1.034269eb21810ed9131cc41a27c729d4 name: harbor-registry-200gb namespace: tanzu-system-registry spec: accessModes: - ReadWriteOnce resources: requests: storage: 200Gi storageClassName: default volumeMode: Filesystem EOF ## 그리고 pvc 생성 후 pod를 삭제 하기 위해 0으로 변경 kubectl -n tanzu-system-registry apply -f harbor-registry-pvc.yaml kubectl -n tanzu-system-registry scale deployment harbor-registry --replicas=0 watch -n 5 kubectl get pod -n tanzu-system-registry ## volumemount 및 volumes을 설정 해준다. kubectl -n tanzu-system-registry edit deployment harbor-registry ## 아래 항목을 찾는다. volumeMounts: - mountPath: /storage name: registry-data - mountPath: /etc/registry/passwd name: registry-htpasswd subPath: passwd - mountPath: /etc/registry/config.yml name: registry-config subPath: config.yml - mountPath: /etc/harbor/ssl/registry name: registry-internal-certs ## 마지막줄에 아래 내용을 추가 한다. - mountPath: /storage2 name: registry-data2 ## 아래 항목을 찾는다. volumes: - name: registry-htpasswd secret: defaultMode: 420 items: - key: REGISTRY_HTPASSWD path: passwd secretName: harbor-registry-htpasswd - configMap: defaultMode: 420 name: harbor-registry-ver-6 name: registry-config - name: registry-internal-certs secret: defaultMode: 420 secretName: harbor-registry-internal-tls ## 마지막줄에 아래 내용을 추가 한다. - name: registry-data2 persistentVolumeClaim: claimName: harbor-registry-200gb ## pod를 생성 한다. kubectl -n tanzu-system-registry scale deployments.apps harbor-registry --replicas=1 watch -n 5 kubectl get pod -n tanzu-system-registry ## pod에 접속 하여 storage repository를 확인 한 후 신규로 연결된 storage2에 Copy. kubectl -n tanzu-system-registry get po --selector=component=registry registry=$(kubectl -n tanzu-system-registry get po --selector=component=registry | awk '{print $1}' | grep harbor) kubectl -n tanzu-system-registry exec -ti $registry -- /bin/bash df -h /storage ls storage/docker/registry/v2/repositories/library/ cp -rfp /storage/* /storage2/ ## pod 삭제 kubectl -n tanzu-system-registry scale deployment harbor-registry --replicas=0 watch -n 5 kubectl get pod -n tanzu-system-registry ## 위에 설정한 volumemount 및 volumes 삭제 kubectl -n tanzu-system-registry edit deployment harbor-registry ## 새로 생성 한 pvc 확인 newpvc=$(kubectl get pv |grep 200gb | awk '{print $1}') kubectl patch pv $newpvc -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' kubectl get pv | grep harbor-registry kubectl -n tanzu-system-registry delete pvc --selector=component=registry ## pv에서 아래 내용 삭제 newpv=$(kubectl get pv | grep 200gb | awk '{print $1}') kubectl edit pv $newpv ## 아래 항목을 찾은 후 claimRef 내용을 삭제 해준다. claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: harbor-registry-100gb namespace: tanzu-system-registry resourceVersion: \"5894668\" uid: 326b24df-dd6f-4679-af84-6530013aed22 ","date":"2023-01-07","objectID":"/harbor/:3:0","tags":["harbor","tanzu","k8s","devops","dk","dokyung","slack","notification"],"title":"The Documentation Harbor","uri":"/harbor/"},{"categories":["Documentation"],"content":"pvc 생성 cat \u003c\u003c EOF \u003e harbor-registry-200gb.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: finalizers: - kubernetes.io/pvc-protection labels: app: harbor component: registry kapp.k14s.io/app: \"1610567506920108209\" kapp.k14s.io/association: v1.034269eb21810ed9131cc41a27c729d4 name: harbor-registry namespace: tanzu-system-registry spec: accessModes: - ReadWriteOnce resources: requests: storage: 200Gi storageClassName: default volumeName: $newpvc volumeMode: Filesystem EOF ## pvc 생성 kubectl apply -f harbor-registry-200gb.yaml -n tanzu-system-registry ## pod 생성 kubectl -n tanzu-system-registry scale deployment harbor-registry --replicas=1 watch -n 5 kubectl get pod -n tanzu-system-registry ## Delete로 변경 kubectl patch pv $newpvc -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Delete\"}}' ## 확인 registry=$(kubectl -n tanzu-system-registry get po --selector=component=registry | awk '{print $1}' | grep harbor) kubectl -n tanzu-system-registry exec -ti $registry -- /bin/bash df -h /storage 증설 전 증설 후 확인 Docker Image 확인 ","date":"2023-01-07","objectID":"/harbor/:3:1","tags":["harbor","tanzu","k8s","devops","dk","dokyung","slack","notification"],"title":"The Documentation Harbor","uri":"/harbor/"},{"categories":["Documentation"],"content":"Tanzu Community Edition with TAP","date":"2022-12-06","objectID":"/tce/","tags":["tanzu","k8s","TCE","TAP","iterage","devops","dk","dokyung"],"title":"The Documentation Tanzu Community Edition with TAP","uri":"/tce/"},{"categories":["Documentation"],"content":"1. TANZU Community Edition? Tanzu Community Edition은 무료로 사용 가능한 VMware에서 제공하는 Kubernetes 플랫폼으로 손쉽게 클러스터를 구성 할 수 있는 솔루션이다. 유료 서비스인 TKG 플랫폼의 모든 기능을 사용 할 수 있지만, 몇가지 제약적인 부분이 있을 수 있다. 가령 하나의 클러스터만 배포를 할 수 있는 단점이 있을 수 있으며 별도로 솔루션의 대한 이슈 및 설치의 대해서 지원을 받지 못한다. 하지만 TKG 솔루션의 대해서 사전에 테스트 환경을 구축 함으로 Kubernetes 플랫폼의 손쉬운 배포 와 VMware에서 제공하는 오픈소스 에코 시스템을 통해 확장의 대해서도 손쉽게 구현을 할 수 있을 것이다. 아래에 제공하는 오픈소스를 효율적으로 구성을 할 수 있다. Eco System ","date":"2022-12-06","objectID":"/tce/:1:0","tags":["tanzu","k8s","TCE","TAP","iterage","devops","dk","dokyung"],"title":"The Documentation Tanzu Community Edition with TAP","uri":"/tce/"},{"categories":["Documentation"],"content":"2. TANZU Community Edition 구성 Docker 설치 링크 KIND 설치 링크 KIND Kubernetes 클러스터는 싱글 노드에서 구축을 할 수 있으며, 비슷한 솔루션으로는 MINIKUBE, K3S등이 있다. 구성환경은 윈도우 10, i7-4770 CPU 16GB 이며 아래는 gitops를 사용하지 않았으며, 마찬가지로 gitops로 구성하여 git에 소스를 머지 할 수도 있지만 여기서는 해당 기능의 대해서는 넣지 않았다. KIND Install # MAC OS # for Intel Macs [ $(uname -m) = x86_64 ]\u0026\u0026 curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-darwin-amd64 # for M1 / ARM Macs [ $(uname -m) = arm64 ] \u0026\u0026 curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-darwin-arm64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind # WINDOWS curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64 Move-Item .\\kind-windows-amd64.exe c:\\some-dir-in-your-PATH\\kind.exe # Linux curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind KIND Cluster 생성 kind-expose-port.yaml 파일 생성 후 아래 내용 추가 kind:ClusterapiVersion:kind.x-k8s.io/v1alpha4nodes:- role:control-planeextraPortMappings:- containerPort:31443# expose port 31443 of the node to port 80 on the host for use later by Contour ingress (envoy)hostPort:443- containerPort:31080# expose port 31080 of the node to port 80 on the host for use later by Contour ingress (envoy)hostPort:80 # 실행 kind create cluster --config kind-expose-port.yaml --image kindest/node:v1.23.12 Kind Cluster 확인 Pivnet 다운로드 curl.exe -Lo pivnet-windows-amd64-3.0.1.exe https://github.com/pivotal-cf/pivnet-cli/releases/download/v3.0.1/pivnet-windows-amd64-3.0.1 Move-Item .\\pivnet-windows-amd64-3.0.1.exe c:\\tmc\\pivnet.exe pivnet login --api-token=\u003cAPI Token\u003e ## TANZU NET에서 EULA Accept 필요 Tanzu CLI Install # tanzu application framework download pivnet dlpf -p tanzu-application-platform -r 1.3.2 -g *framework* # Linux tar xvf tanzu-framework-*-amd64-*.tar install cli/core/v0.25.0/tanzu-core-*_amd64 /usr/local/bin/tanzu export TANZU_CLI_NO_INIT=true # Windows Expand-Archive -Force tanzu-framework-*-amd64-*.zip c:\\tmc Copy-Item C:\\tmc\\cli\\core\\v0.25.0/tanzu-core-*_amd64* c:\\tmc\\tanzu.exe # Version 확인 tanzu version ----------------- version: v0.25.0 buildDate: 2022-08-25 sha: 6288c751-dirty # Plugin 설치 tanzu plugin install --local c:\\tmc\\cli all Cluster Essentials Install # tanzu-cluster-essentials download pivnet dlpf -p tanzu-cluster-essentials -r 1.3.0 -g *essentials* tar xzvf tanzu-cluster-essentials-windows-amd64-1.3.0.tgz -C c:\\tmc $Env:TANZUNET_USERNAME='' $Env:TANZUNET_PASSWORD='' $Env:INSTALL_BUNDLE='registry.tanzu.vmware.com/tanzu-cluster-essentials/cluster-essentials-bundle:1.3.0' $Env:INSTALL_REGISTRY_HOSTNAME='registry.tanzu.vmware.com' $Env:INSTALL_REGISTRY_USERNAME=$Env:TANZUNET_USERNAME $Env:INSTALL_REGISTRY_PASSWORD=$Env:TANZUNET_PASSWORD c:\\tmc\\install.bat -y 만약 원하는 OS만 받고 싶으면 해당 하는 파일이름을 -g {설치하고자 하는 OS 선택} TKGM Downloads essentials Downloads 구성된 KIND cluster에 TAP iterate를 설치하여 source를 테스트 하고 빠르게 build를 함으로 개발의 민첩성을 제공 할 수 있다. 별도의 클러스터를 구성해서 사용 할 수 있지만, 이렇게 TCE를 구성함으로 인해서 노트북에서도 생성 후 테스트를 할 수 있다. iterate에 포함된 opensource Tanzu Application Install kubectl create ns tap-install kubectl create secret docker-registry tap-registry --docker-server=registry.tanzu.vmware.com --docker-username=$Env:TANZUNET_USERNAME --docker-password=$Env:TANZUNET_PASSWORD -n tap-install tanzu package repository add tanzu-tap-repository --url registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:1.3.2 -n tap-install --wait=false # plugin 확인 tanzu package available list --namespace tap-install # GIT 환경변수 등록 $Env:GITHUB_USERNAME=github-name $Env:GITHUB_TOKEN=api-token # tap-values.yaml shared: ingress_domain: huntedhappy.kro.kr image_registry: project_path: ghcr.io/$Env:GITHUB_USERNAME username: $Env:GITHUB_USERNAME password: $Env:GITHUB_TOKEN ceip_policy_disclosed: true profile: iterate supply_chain: basic contour: contour: replicas: 1 envoy: service: type: NodePort nodePorts: http: 31080 https: 31443 hostPorts: enable: true cnrs: domain_template: \"{{.Name}}.{{.Domain}}\" provider: local excluded_packages: - po","date":"2022-12-06","objectID":"/tce/:2:0","tags":["tanzu","k8s","TCE","TAP","iterage","devops","dk","dokyung"],"title":"The Documentation Tanzu Community Edition with TAP","uri":"/tce/"},{"categories":["Documentation"],"content":"TANZU \u0026 KEYCLOAK","date":"2022-07-15","objectID":"/tanzu-keycloak/","tags":["tanzu","k8s","KEYCLOAK","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 Keycloak","uri":"/tanzu-keycloak/"},{"categories":["Documentation"],"content":"1. TANZU와 KEYCLOAK 연동 TANZU는 기본적으로 LDAPS 또는 OIDC와 연동이 가능합니다. 그 중에 무료 서비스인 KEYCLOAK을 활용하여 TANZU와 KEYCLOAK 연동 ","date":"2022-07-15","objectID":"/tanzu-keycloak/:1:0","tags":["tanzu","k8s","KEYCLOAK","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 Keycloak","uri":"/tanzu-keycloak/"},{"categories":["Documentation"],"content":"2. KEYCLOAK 구성 KEYCLOAK DOWNLOAD LINK keycloak download KEYCLOAK 설치 인증서는 사설 인증서로 생성 ## 압축 해제 tar zxvf keycloak-18.0.2.tar.gz cd keycloak-18.0.2 export KEYCLOAK_ADMIN=admin export KEYCLOAK_ADMIN_PASSWORD='' bin/kc.sh start-dev --https-certificate-file {인증서} --https-certificate-key-file {인증서 KEY} --https-port 8443 --hostname {hostname} \u0026 reaml 생성 realm 생성 필요한 Client Scopes Client Scopes Client Scopes Client Scopes Clients 생성 Clients 생성 Clients 생성 Clients 생성 Clients 생성 Role 생성한 Client Scopes를 Default Client Scopes에 이동 Scopes 선택 Groups 설정 Groups 설정 Roles Roles 설정 Groups Groups 설정 Groups 설정 Users Users 생성 Users 생성 User Password 설정 User Password 설정 User Password 설정 Group에 할당 ","date":"2022-07-15","objectID":"/tanzu-keycloak/:2:0","tags":["tanzu","k8s","KEYCLOAK","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 Keycloak","uri":"/tanzu-keycloak/"},{"categories":["Documentation"],"content":"3. TANZU 구성 사설 인증서로 구성을 했기 때문에 당연히 worker Node에도 신뢰된 인증서로 들어가 있어야 하며, 처음 구성시 CA를 넣는 부분이 없기 때문에 배포가 완료 후 CA를 넣어야 함 OIDC 추출 ## management cluster 변경 cluster=tkgm02 echo $IDENTITY_MANAGEMENT_TYPE export _TKG_CLUSTER_FORCE_ROLE=\"management\" export FILTER_BY_ADDON_TYPE=\"authentication/pinniped\" tanzu cluster create $cluster --dry-run -f tkgm01.yaml \u003e $cluster-example-secret.yaml 위와 같이 하면 아래와 같은 파일을 확인 할 수 있다. apiVersion: v1 kind: Secret metadata: annotations: tkg.tanzu.vmware.com/addon-type: authentication/pinniped labels: clusterctl.cluster.x-k8s.io/move: \"\" tkg.tanzu.vmware.com/addon-name: pinniped tkg.tanzu.vmware.com/cluster-name: tkgm02 name: cjenm-tkgm02-pinniped-addon namespace: tkg-system stringData: values.yaml: | #@data/values #@overlay/match-child-defaults missing_ok=True --- infrastructure_provider: vsphere tkg_cluster_role: management custom_cluster_issuer: \"\" custom_tls_secret: \"\" http_proxy: \"\" https_proxy: \"\" no_proxy: \"\" identity_management_type: oidc pinniped: cert_duration: 2160h cert_renew_before: 360h supervisor_svc_endpoint: https://0.0.0.0:31234 supervisor_ca_bundle_data: ca_bundle_data_of_supervisor_svc supervisor_svc_external_ip: 0.0.0.0 supervisor_svc_external_dns: null upstream_oidc_client_id: {CLIENT ID} upstream_oidc_client_secret: {CLIENT SECRET} upstream_oidc_issuer_url: https://{KEYCLAOK FQDN}:8443/realms/access upstream_oidc_tls_ca_data: {base64로 CA인증서} upstream_oidc_additional_scopes: - openid - profile - email - groups - offline_access upstream_oidc_claims: username: email groups: groups supervisor: service: name: pinniped-supervisor type: LoadBalancer type: tkg.tanzu.vmware.com/addon 실행 kubectl apply -f $cluster-example-secret.yaml -n tkg-system 완료 후 테스트 tanzu mc kubeconfig get --export-file=tanzu-cli-tkgm02 kubectl get pod -A --kubeconfig tanzu-cli-tkgm02 요청 생성한 계정으로 로그인 LOGIN TOKEN을 얻을 수 있다. TOKEN 얻기 TOKEN을 붙여 넣으면 아래와 같이 요청이 되는 것을 확인 할 수 있다. kubectl 요청 만약 권한이 없다면 아래 처럼 권한이 없다고 나온다. kubectl 요청 ","date":"2022-07-15","objectID":"/tanzu-keycloak/:3:0","tags":["tanzu","k8s","KEYCLOAK","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 Keycloak","uri":"/tanzu-keycloak/"},{"categories":["Documentation"],"content":"TANZU \u0026 OKTA","date":"2022-07-15","objectID":"/tanzu-okta/","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 OTKA","uri":"/tanzu-okta/"},{"categories":["Documentation"],"content":"1. TANZU와 OKTA 연동 TANZU는 기본적으로 LDAPS 또는 OIDC와 연동이 가능합니다. 그 중에 OKTA 서비스를 활용하여 TANZU와 OKTA를 연동 하겠습니다. OKTA는 기본적으로 30일간 무료로 사용이 가능합니다. ","date":"2022-07-15","objectID":"/tanzu-okta/:1:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 OTKA","uri":"/tanzu-okta/"},{"categories":["Documentation"],"content":"2. OKTA 구성 OKTA접속 후 관리자로 변경 관리자로 변경 Application 추가 Apps 추가 Apps 추가 redirect URIs는 kubectl get svc를 통해 확인 Apps 추가 Redirect URIs 확인 Sign On 수정 Group을 생성은 Optional Groups 생성 및 Assignment APP Assign APP Assign APP Assign ","date":"2022-07-15","objectID":"/tanzu-okta/:2:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 OTKA","uri":"/tanzu-okta/"},{"categories":["Documentation"],"content":"3. TANZU 구성 OIDC_IDENTITY_PROVIDER_CLIENT_SECRET를 base64로 변경 필요 echo -n '{CLIENT SECRETS}' | base64 TANZU MGMT 에서 OIDC 부분을 찾은 후 파일 수정 IDENTITY_MANAGEMENT_TYPE: \"oidc\" #! Settings for IDENTITY_MANAGEMENT_TYPE: \"oidc\" CERT_DURATION: 2160h CERT_RENEW_BEFORE: 360h IDENTITY_MANAGEMENT_TYPE: oidc OIDC_IDENTITY_PROVIDER_CLIENT_ID: 0oa2i[...]NKst4x7 OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \u003cencoded:LVVnMFNsZFIy[...]TMTV3WUdPZDJ2Xw==\u003e OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM: groups OIDC_IDENTITY_PROVIDER_ISSUER_URL: https://dev-[...].okta.com OIDC_IDENTITY_PROVIDER_SCOPES: openid,profile,email,groups,offline_access OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: email ","date":"2022-07-15","objectID":"/tanzu-okta/:3:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 OTKA","uri":"/tanzu-okta/"},{"categories":["Documentation"],"content":"4. SA 생성 후 TEST SA 생성 kubectl create clusterrolebinding id-mgmt-test-user --clusterrole cluster-admin --user {mail address} TEST tanzu mc kubeconfig get --export-file=tanzu-cli-tkgm02 kubectl get pod -A --kubeconfig tanzu-cli-tkgm02 TEST TEST TEST TEST TEST ","date":"2022-07-15","objectID":"/tanzu-okta/:4:0","tags":["tanzu","k8s","OKTA","OIDC","devops","dk","dokyung"],"title":"The Documentation Tanzu \u0026 OTKA","uri":"/tanzu-okta/"},{"categories":["Documentation"],"content":"Wordpress \u0026 Nginx Install","date":"2022-06-01","objectID":"/wordpress/","tags":["nginx","wordpress","dk","dokyung"],"title":"The Documentation Wordpress \u0026 Nginx Install","uri":"/wordpress/"},{"categories":["Documentation"],"content":"NGINX를 사용하여 WORDPRESS를 설치 ","date":"2022-06-01","objectID":"/wordpress/:0:0","tags":["nginx","wordpress","dk","dokyung"],"title":"The Documentation Wordpress \u0026 Nginx Install","uri":"/wordpress/"},{"categories":["Documentation"],"content":"1. NGINX 설치 설치 NGINX 설치 sudo add-apt-repository ppa:ondrej/nginx -y sudo apt-get update sudo apt-get dist-upgrade -y sudo apt-get install nginx -y nginx -v PHP 설치 sudo add-apt-repository ppa:ondrej/php -y sudo apt-get update sudo apt-get install php8.0-fpm php8.0-common php8.0-mysql \\ php8.0-xml php8.0-xmlrpc php8.0-curl php8.0-gd \\ php8.0-imagick php8.0-cli php8.0-dev php8.0-imap \\ php8.0-mbstring php8.0-opcache php8.0-redis \\ php8.0-soap php8.0-zip -y php-fpm8.0 -v NGINX 구성 cat \u003c\u003c EOF | tee /etc/nginx/sites-available/wordpress.tkg.io server { listen 80; server_name wordpress.tkg.io; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; root /var/www/html; index index.php; location ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php8.0-fpm.sock; } } EOF rm -rf /etc/nginx/sites-available/default rm -rf /etc/nginx/sites-enabled/default ## Symbolic Link sudo ln -s /etc/nginx/sites-available/wordpress.tkg.io /etc/nginx/sites-enabled/wordpress.tkg.io sed -i -e \"s/font\\/woff2 woff/font\\/woff2 woff2/g\" /etc/nginx/mime.types ## Validataion Check nginx -t ## Nginx Reload nginx -s reload PHP 파일 수정 sed -i -e \"s/upload_max_filesize = 2M/upload_max_filesize = 64M/g\" /etc/php/8.0/fpm/php.ini sed -i -e \"s/post_max_size = 8M/post_max_size = 64M/g\" /etc/php/8.0/fpm/php.ini sudo php-fpm8.0 -t sudo systemctl restart php8.0-fpm MARIADB 설치 sudo apt-get install software-properties-common sudo apt-key adv --fetch-keys 'https://mariadb.org/mariadb_release_signing_key.asc' sudo add-apt-repository 'deb [arch=amd64,arm64,ppc64el] http://mirrors.up.pt/pub/mariadb/repo/10.4/ubuntu focal main' sudo apt-get install mariadb-server -y ","date":"2022-06-01","objectID":"/wordpress/:1:0","tags":["nginx","wordpress","dk","dokyung"],"title":"The Documentation Wordpress \u0026 Nginx Install","uri":"/wordpress/"},{"categories":["Documentation"],"content":"MARIADB PASSWORD 변경 mysql -u root -e 'create database wordpress character set utf8; grant all privileges on wordpress.* to 'root'@'localhost' identified by \"WordPress!234\"; flush privileges;' WordPress 설치 mkdir /wordpress cd /wordpress \u0026\u0026 wget http://wordpress.org/latest.tar.gz \u0026\u0026 tar -xzf /wordpress/latest.tar.gz -C /wordpress --strip-components 1 WordPress 설정 cp /wordpress/wp-config-sample.php /wordpress/wp-config.php sed -i -e \"s/database_name_here/wordpress/g\" /wordpress/wp-config.php sed -i -e \"s/username_here/root/g\" /wordpress/wp-config.php sed -i -e \"s/password_here/WordPress\\!234/g\" /wordpress/wp-config.php cp -R * /var/www/html/ ","date":"2022-06-01","objectID":"/wordpress/:1:1","tags":["nginx","wordpress","dk","dokyung"],"title":"The Documentation Wordpress \u0026 Nginx Install","uri":"/wordpress/"},{"categories":["Documentation"],"content":"완료 화면 wordpress-install ","date":"2022-06-01","objectID":"/wordpress/:1:2","tags":["nginx","wordpress","dk","dokyung"],"title":"The Documentation Wordpress \u0026 Nginx Install","uri":"/wordpress/"},{"categories":["Documentation"],"content":"NAP Built","date":"2022-04-10","objectID":"/networks/","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0. OSI 7 Layer OSI 7Layer ","date":"2022-04-10","objectID":"/networks/:1:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0.1. Ethernet Frame EthernetFrame ","date":"2022-04-10","objectID":"/networks/:1:1","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0.2. Ethernet Header ( 2 Layer) Destination Address 6 bytes = 48 bits 목적지 맥 주소 6 bytes(48bits), 주소의 첫 번째 비트가 1이면 멀티캐스트 이며, 모든 비트가 1이면 브로드캐스트이다. FF:FF:FF:FF:FF = Broadcast Source Address 6 bytes = 48 bits 출발지 맥 주소 6 bytes(48bits) 802.1q tag 4 bytes = 32 bits L2에서 VLAN 설정 Ethernet Type 2 bytes = 16 bits Ethernet 및 802.3 와의 호환성을 위한 구분 방법 (Len/Type : 길이 또는 타입) 0x 600 이하이면 =\u003e Length (IEEE 802.3) 로 해석 Length : 수납되는 LLC 프레임 길이(3~1500 바이트)를 나타냄 ☞ MTU 0x 600 이상이면 =\u003e Type (DIX 2.0) 로 해석 Type : Data에 담겨있는 상위 프로토콜 종류 Type Field Description 0x0600h Xerox XNS IDP 0x0800h IPv4 0x0805h X.25 0x0806h ARP 0x0835h RARP 0x6003h DEC DECnet Phase IV 0x8100h VLAN ID 0x8137h Novell Netware IPX 0x8191h NetBIOS 0x86DDh IPv6 0x8847h MPLS } 0x8863h PPPoE Discovery Stage 0x8864h PPPoE PPP Session Stage 0x888Eh IEEE 802.1X 0x88CCh LLDP (Link Layer Discovery Protocol) ","date":"2022-04-10","objectID":"/networks/:1:2","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0.3. IP Header (3 Layer) IP Header windows에서 MTU 사이즈 확인 하는 방법 ping Option 설정 -l size , -f 조각화 하지 않음, IP 20 bytes, ICMP 8 bytes 1500 - (IP + ICMP) = 1472 ping -l -f 1472 ICMP로 확인 명령어로 MTU 확인 netsh interface ipv4 show interfaces netsh로 확인 변경은 아래 명령어로 가능하다. (변경시 상단 스위치에서도 변경 필요, 변경 하지 않으면 단편화가 된다.) netsh interface ipv4 set subinterface “색인 번호” 또는 “이름” 으로 변경이 가능하다. netsh interface ipv4 set subinterface “10” mtu=9000 store=persistent netsh interface ipv4 set subinterface “이더넷 2” mtu=9000 store=persistent ","date":"2022-04-10","objectID":"/networks/:1:3","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0.4. TCP Header (4 Layer) TCP Header Source port (16 bits) 출발지 port Destination port (16 bits) 목적지 port Sequence number (32 bits) 패킷의 순서 Acknowledgment number (32 bits) Data offset (4 bits) TCP 헤더의 크기를 32 bits로 지정 Reserved (3 bits) 향후 사용을 위해 0으로 설정 (현재는 사용하지 않음) Flags (9 bits) 다음과 같은 9개의 1비트 플래그(제어 비트)를 포함합니다. NS(1비트): ECN-nonce - 은폐 보호 [a] CWR(1비트): CWR(Congestion Window Reduced) 플래그는 송신 호스트가 ECE 플래그가 설정된 TCP 세그먼트를 수신하고 혼잡 제어 메커니즘에서 응답했음을 나타내기 위해 설정됩니다. [비] ECE(1비트): ECN-Echo는 SYN 플래그 값에 따라 이중 역할을 합니다. 다음을 나타냅니다. SYN 플래그가 설정되면(1), TCP 피어는 ECN 을 사용할 수 있습니다. SYN 플래그가 클리어(0)이면 IP 헤더에 Congestion Experienced 플래그(ECN=11)가 설정된 패킷이 정상 전송 중에 수신되었음을 나타냅니다. [b] 이것은 TCP 발신자에게 네트워크 정체(또는 임박한 정체)를 나타내는 역할을 합니다. URG(1비트): 긴급 포인터 필드가 중요함을 나타냅니다. ACK(1비트): Acknowledgement 필드가 중요함을 나타냅니다. 클라이언트가 보낸 초기 SYN 패킷 이후의 모든 패킷에는 이 플래그가 설정되어 있어야 합니다. PSH(1비트): 푸시 기능. 버퍼링된 데이터를 수신 애플리케이션에 푸시하도록 요청합니다. RST(1비트): 연결 재설정 SYN(1비트): 시퀀스 번호를 동기화합니다. 각 끝에서 보낸 첫 번째 패킷에만 이 플래그가 설정되어야 합니다. 일부 다른 플래그 및 필드는 이 플래그를 기반으로 의미를 변경하며 일부는 설정된 경우에만 유효하고 다른 일부는 해제된 경우에만 유효합니다. FIN(1비트): 보낸 사람의 마지막 패킷 Windows size (16 bits) 한번에 받을 수 있는 데이터의 양이며 최대 64Kbytes 까지 가능하다.PC에서 Windows Size만큼 한번에 전송 후에 ACK를 기다린다. Windows Szie는 가변적이므로 통신에 문제가 없으면 Size를 늘리고, 문제가 발생 한다면 Size를 줄인다. 이렇게 늘렸다 줄였다 하는 것을 Sliding Window라고 하며 이렇게 전송속도를 제어 하는 것을 Flow-Control이라고 부른다. Checksum (16 bits) 16비트 체크섬 필드는 TCP 헤더, 페이로드 및 IP 의사 헤더의 오류 검사에 사용됩니다. 의사 헤더는 소스 IP 주소 , 대상 IP 주소 , TCP 프로토콜의 프로토콜 번호 (6), TCP 헤더 및 페이로드의 길이(바이트)로 구성됩니다. Sliding Window 참고 블로그 Ethernet frame wiki IP Header wiki TCP Header wiki Ethernet Type 참고 0.4.1. (FIN, FIN-WAIT)관련 FIN FIN_WAIT1 상태 A가 B에게 Connection을 Close하면서, 종료 신호인 FIN segment를 A에서 B에게 보내고, A는 FIN_WAIT_1 상태가 된다. (이때 A System의 User는 더이상의 SEND는 사용 할 수 없고, RECIEVE는 계속 가능) CLOSE_WAIT 상태 B가 신호를 받으면 B는 CLOSE_WAIT에 들어가면서 그에 대한 ACK을 보낸다. FIN_WAIT2 상태 A는 FIN_WAIT2상태가 되고, B는 Connection Close하면서 FIN을 보낸다. TIME_WAIT 상태 A는 TIME_WAIT상태가 되어 최종적으로 B와의 Connection이 닫히는지 확인 한다. 즉 서버가 클라이언트로 부터 FIN에 대한 ACK을 받고나서 클라이언트의 FIN을 기다리는 상태가 FIN_WAIT2 상태이다. ","date":"2022-04-10","objectID":"/networks/:1:4","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"0.5. Overlay 0.5.1. VXLAN Outer IP Header 20 Bytes + UDP Header 8 bytes + VXLAN 8 bytes + Inner Ethernet 18 bytes VLAN 포함 20 + 8 + 8 + 18 = 54 만약 VLAN이 포함 되어 있지 않다면 20 + 8 + 8 + 14 = 50 그래서 1500 + 54 = 1554 가 필요 하지만 계산하기 편하게 1600 이상으로 설정 하는 것을 권고 한다. 또한 요새는 SDN 제품들이 jumbo frame(9000) 으로 기본 셋팅 되어서 나오기 때문에 9000으로 설정을 해도 된다. VXLAN 0.5.2. GENEVE GENEVE#1 GENEVE#2 PARAMETER VXLAN GENEVE Abbreviation for VXLAN (Virtual Extensible LAN) GENEVE (Generic Network Virtualization Encapsulation) Developed by VMware, Arista Networks and Cisco VMware, Microsoft, Red Hat and Intel Protocol UDP UDP Port No 4789 6081 Header Length 8 bytes 16 bytes Transport security, service chaining, in-band telemetry Not Supported Supported RFC VXLAN is officially documented by the IETF in RFC 7348 RFC 8926 Protocol Identifier No Yes Non-Client Payload Indication No Yes Extensibility No. Infact all fields in VXLAN header have predefined value Yes Hardware friendly vendor extensibility mechanism Limited Yes Term used for Tunnel Endpoints VTEP TEP VXLAN - GENEVE 참고 ","date":"2022-04-10","objectID":"/networks/:1:5","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"1. L2 ","date":"2022-04-10","objectID":"/networks/:2:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"1.1. MAC주소 MAC은 일반적으로 OSI 7 Layer중에서 2 Layer에 속한다. 확인하는 방법은 윈두우에서 CMD창을 연 후 ipconfig /all명령어를 사용하면 확인할 수 있다. 나오는 부분중에 물리적 주소라고 나오는 것을 확인 할 수 있다. 리눅스의 경우는 ifconfig라고 치면 바로 확인을 할 수 있을 것이다. 물론 H/W 주소이지만 변경은 가능하다. 그러므르 보안적으로 이슈가 될 수 있다. MAC 주소 확인 확인을 하게 되면 E0-3F-49-AB-C8-AD 위와 같이 표시가 되며 절반으로 나누어 앞의 세부분은 생산자를 나타내고, 뒤의 세 부분은 장치의 일련번호(Host Identifier)을 나타낸다. MAC 제조회사 찾는 곳 제조회사 찾는 방법 ","date":"2022-04-10","objectID":"/networks/:2:1","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"1.2. ARP \u0026 RARP \u0026 GARP ARP(Address Resolution Protofcol)는 IP주소를 MAC주소와 대응(Bind)시키기 위해 사용되는 프로토콜이다. IP주소는 알지만 MAC주소를 모르는 경우 사용 할 수 있다. 윈도우에서 arp -a {IP} 로 확인 할 수 있다. ARP 확인 RARP(Reverse Address Resolution Protocol)는 그 반대로 MAC주소로 IP를 대응(Bind)시키기 위해 사용되는 프로토콜이다. MAC주소는 알지만 IP주소를 모르는 경우 사용 할 수 있다. GARP(Gratuitous ARP)는 PC를 스위치에 연결을 하게 되면 나의 IP와 MAC은 이거라고 알리는대 사용한다. 3번정도 GARP를 보낸다. 그래서 IP 주소 충돌을 감지 할 수 있으며, GARP를 수신한 모든 호스트/라우터는 ARP Table을 갱신할 수 있다. 또 다른 목적은 VRRP/HSRP프로토콜에서 사용된다(VRRP/HSRP의 설명은 패스한다). ARP probe ARP probe는 sender의 IP주소를 0으로 해서 ARP요청을 하며 IPv4 주소의 충돌을 감지 할 수 있다. Probe 확인 ARP announcements 다른 호스트의 ARP 테이블을 갱신 할 수 있다. Announcement 확인 ","date":"2022-04-10","objectID":"/networks/:2:2","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"1.3. L2 통신 기능 설명 기술 learning 출발지 주소가 MAC 테이블에 주소가 없으면 MAC 주소를 저장 MAC table flooding 목적지 주소가 MAC 테이블에 없으면 전체 포트로 전달 Broadcast filtering 출발지/목적지가 동일 네트워크에 있으면 다른 네트워크로 전파 차단 Collision Domain aging MAC 테이블 캐쉬 Aging Time PC를 연결 하면 PC에서 PROBE를 3번 정도 보내고 Announcement 후 스위치 및 PC에서는 ARP를 Learning한다. 만약 스위치 및 PC에서 arp table이 없으면 aging Time이 끝났거나 또는 GARP를 받지 못했을 경우 또는 ARP테이블이 갱신이 안될 경우, PC 1에서 PC 2로 통신을 할 때 Broadcast(flooding)를 보낸다. 이 때 스위치에서는 동일한 VLAN(Filtering)의 모든 포트로 (목적지 IP:ff:ff:ff:ff:ff) 브로드캐스트를 보낸다. PC2는 해당 브로드캐스트를 받고 반대로 PC1에게 전달을 하게 된다. 스위치 및 PC는 해당 MAC Table에 저장한다. 그리고 스위치에서 연결된 PC에서 트래픽을 전달 한다.(Forwarding) L2 통신 ","date":"2022-04-10","objectID":"/networks/:2:3","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"2. L3 라우터에 Dynamic(RIP, EIGRP, OSPF, BGP) 또는 STATIC의 대한 설명은 하지 않는다. Gateway를 적용 하면 아래와 같이 Gateway의 ARP를 얻기 위핸 Broadcast를 요청 하는 것을 확인 할 수 있다. GATEWAY 동일한 대역의 경우 ARP가 각각의 Host에 등록이 되어 있거나 L2 스위치에 등록이 되어 있기 때문에 G/W가 필요 없다. G/W의 역할은 동일한 대역이 아닌 다른 대역과 통신 하기 위해 동일한 대역이 아닌 대역을 G/W에 물어보기 위해 존재 한다. PC에서 라우팅 테이블을 확인 해 볼 수 있다. route print 명령어를 통해 확인 해보자. 만약 피시에 Network Interface가 두개이고 목적지가 각각 다른 NIC으로 향해야 한다면. 모든 목적지는 G/W로 지정을 하고 STATIC하게 라우팅을 2번 Interface로 테이블을 지정 할 수 있을 것이다. 그리고 만약 동일한 목적지로 라우팅이 잡혀 있다면 METRIC이 더 빠른쪽으로 라우팅을 한다는 것을 확인 할 수 있을 것이다. route print 그리고 MAC주소는 라우터를 거치면서 바뀌게 된다. 아래에서 보듯이 실제 10.253.107.2의 MAC주소는 00:50:56:b0:f8:47 이지만 목적지 Host 10.253.126.25에서 확인하였을 때는 G/W인 10.253.126.1의 MAC주소로 바뀌어서 들어오는 것을 확인 할 수 있다. route print ","date":"2022-04-10","objectID":"/networks/:3:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"3. L4 L3라우터의 경우 IP N/W을 통해 라우팅 경로(라우팅 경로라고 하면 G/W를 통해 봤듯이 자기가 모르는 대역을 다른 라우터에 물어보기 위한 것이라고 보면 된다. 이에 활용되는 것이 STATIC, RIP, EIGRP, OSPF. BGP가 있으며 하나 하나의 대한 기술적 내용은 기술 하지 않는다.)를 확보하는 반면. L4의 경우 Port까지 확인 한다. 이때부터 세션 베이스라고 보면 된다. 한마디로 라우터의 경우 2개의 물리적인 라우터가 있을 경우 A라는 라우터를 통해 들어온 트래픽이 B라는 트래픽을 통해 나가더라도 통신에 이슈가 발생 하지 않는다. L3의 경우 Asymmetirc구조라도 상관 없음 하지만 L4 Layer이상의 경우 세션 베이스이기 때문에 자기가 가지고 있지 않은 정보가 들어오면 Drop을 한다. 그래서 동일한 경로로 통신을 해야 한다. L4의 경우 Asymmetirc구조일 경우 Drop 또한 기술적으로 LoadBalancer의 경우 One-Arm구성 일 경우 어떻게 구성해야되는지의 대한 여러가지 방안들이 있지만 단순하게 어떻게 통신을 한다는지의 대한 내용만 적었다.. 마찬가지로 LoadBalancer의 경우 다양한 기술이 포함되어 있기 때문에 이렇게 되면 안됩니다. 라는 정도만 작성했다. ","date":"2022-04-10","objectID":"/networks/:4:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"4. HTTPS 이야기 HTTPS는 공개키와 비공개키를 사용한다. 이유는 공개키는 비공개키보다 암/복호화 하는대 더 많은 트래픽이 필요 하기 때문에 공개키로 비공개키를 암호화하여 비공개키를 안전하게 전달 후 비공개키를 통해 데이터를 전달한다. HTTPS 이야기 HTTPS 이야기 HTTPS 이야기 클라이언 헬로우 : \u003c sslversion, random number, cipersuites, session id 포함\u003e ciphersuites 복수인 이유는 클라이언트에서 어떤 암호화 프로토콜이 있는지 서버에게 알려주게 된다. 서버 헬로우 : \u003c sslversion, random number, ciphersuit, session id 포함 \u003e 사이트의 ciphersuite 단수인데요. 이유는 클라이언트가 보내준 암호화중에 제일 암호화가 높은것으로 선택 사용자의 웹브라우저에는 인증기관의 공개키가 이미 내장되어 있기 때문에 내장된 공개키로 인증서가 제대로된 인증서인지 판단 가끔 브라우저 버전이 낮으면 최신 인증기관의 정보가 없을 수 있다. 만약에 사설 인증서일 경우 인증된 인증서가 아닐 뿐 사아트에서는 공개키를 전달 했기 때문에 전달 받은 공개키로 대칭키를 암호화(Pre-Master-Secret) 한다. 사용자는 사이트에서 얻은 공개키를 이용하여, PMS(Pre-Master-Secret)를 암호화 (Pre-Master-Secret)는 사용자가 Hello 보낼때 들어가있는 randomKey, 서버가 hello 할때 random key를 조합하여 만듦 사이트는 자기의 개인키로 사용자가 공개키로 암호화한것을 복호화 하여 안에 들어있는 Pre-Master-Secret를 이용하여 사용자와 메시지를 주고 받음 Pre-Master-Secret는 master Secret을 만들고 master-Secret 은 세션키를 생성 이후 세션키 값을 이용하여 아까 했던 일련의 상황들을 모두 reuse를 하여 복잡한 방식을 모두 하지 않고 대칭키 방식으로 메시지를 주고 받습니다. 공인된 인증기관에서 발급 받은 인증서의 경우 기본적으로 인증서에는 공개키와 개인키가 포함되어 있기 때문에 이미 브라우저에 내장되어 있는 인증기관의 공개키로 복호화 하여 사이트가 인증기관에서 인증을 받은 사이트라는것을 검증 한다. 만약 사설 인증서인 경우 웹브라우저에서 검증되지 않는 사이트라고 나오며 * 이는 인증기관에서 인증되지 않는 사이트라는 것만 검사 할뿐 이를 무시를 하게 되면 서버에서는 이미 공개키를 전달 했기 때문에 해당 공개키를 가지고 대칭키를 암호화 하여 사이트에 전달을 하게 된다 그리고 사이트에는 개인키로 복호화 하여 대칭키를 획득 한 후 대칭키로 암/복호화를 하여 데이터를 주고 받게 된다. ","date":"2022-04-10","objectID":"/networks/:5:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"5. 유저에서 웹서핑 까지 웹서핑 사용자가 웹서핑을 하기 위해 브라우저에서 해당하는 DOMAIN을 입력한다. 예를 들어 www.naver.com을 입력하게 되면 최초로 LDNS에게 요청하게 된다. LDNS는 자기가 잡은 DNS를 LDSN라고 한다. LDSN에서는 해당 도메인의 ROOT DNS에 쿼리를 날린다. ROOT DNS는 하위 DNS인 .COM의 IP를 응답해준다. .COM의 DNS는 하위 DNS인 naver.com의 DNS의 IP를 응답해준다. naver.com은 실제적으로 www.naver.com의 A 레코드를 가지고 있으면 LDNS에게 응답을 해주고 LDNS는 최종적으로 사용자에게 www.naver.com의 IP를 알려주게 된다. ","date":"2022-04-10","objectID":"/networks/:6:0","tags":["network","l2","l3","l4","osi 7layer","tcp","vxlan","vxlan header","geneve","geneve header","ip header","ethernet header","Ethernet Frame","tpc/ip header","vmware","nsxt","https","tls","dk","dokyung"],"title":"The Documentation Networks","uri":"/networks/"},{"categories":["Documentation"],"content":"VMware Tanzu with AVI","date":"2022-04-05","objectID":"/avi/","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"AVI에서 제공하는 AKO인 INGRESS Controller를 사용 하는 방법 제공 ","date":"2022-04-05","objectID":"/avi/:0:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"1. TANZU에서 Cluster ServiceType 변경 serviceType ManageMent Cluster로 Context 변경 ### NodePort 변경 kubectl apply -f - \u003c\u003c EOF apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: name: nodeport01 spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: Default-Cloud clusterSelector: matchLabels: ako-l7-nodeport-01: \"true\" controller: avi.tkg.io dataNetwork: cidr: 10.253.127.0/24 name: LS_TKGM_10.253.127.x extraConfigs: cniPlugin: antrea disableStaticRouteSync: false l4Config: autoFQDN: disabled ingress: defaultIngressController: true disableIngressClass: false nodeNetworkList: - cidrs: - 10.253.127.0/24 networkName: LS_TKGM_10.253.127.x serviceType: NodePort shardVSSize: SMALL serviceEngineGroup: Default-Group EOF ### ClusterIP 변경 kubectl apply -f - \u003c\u003c EOF apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: name: clusterip01 spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: Default-Cloud clusterSelector: matchLabels: ako-l7-clusterip-01: \"true\" controller: avi.tkg.io dataNetwork: cidr: 10.253.127.0/24 name: LS_TKGM_10.253.127.x extraConfigs: cniPlugin: antrea disableStaticRouteSync: false # required l4Config: autoFQDN: disabled ingress: defaultIngressController: true disableIngressClass: false # required nodeNetworkList: # required - cidrs: - 10.253.127.0/24 networkName: LS_TKGM_10.253.127.x serviceType: ClusterIP # required shardVSSize: SMALL # required serviceEngineGroup: Default-Group EOF ### NodePortLocal 변경 kubectl apply -f - \u003c\u003c EOF apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: name: npl-enabled spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: Default-Cloud clusterSelector: matchLabels: npl-enabled: \"true\" controlPlaneNetwork: cidr: 10.253.127.0/24 name: LS_TKGM_10.253.127.x controller: avi.tkg.io dataNetwork: cidr: 10.253.127.0/24 name: LS_TKGM_10.253.127.x extraConfigs: cniPlugin: antrea disableStaticRouteSync: false # required l4Config: autoFQDN: disabled ingress: defaultIngressController: true disableIngressClass: false nodeNetworkList: - cidrs: - 10.253.127.0/24 networkName: LS_TKGM_10.253.127.x serviceType: NodePortLocal # required shardVSSize: SMALL serviceEngineGroup: Default-Group EOF NodePort 테스트 kubectl config use-context {Management Cluster} kubectl get cluster --show-labels kubectl label cluster tkgm01-tkc-dev01 ako-l7-nodeport-01=true kubectl config use-context {TKC} kubectl create deploy hello --image=paulbouwer/hello-kubernetes:1.7 --replicas=3 --port=8080 kubectl expose deployment hello --type=NodePort --port=80 --target-port=8080 kubectl create ingress hello --class=avi-lb --rule=\"hello.avi.tkg.io/=hello:8080\" 접속 화면 NodePort로 구성이 되어 있기 때문에 아래와 같이 NodeIP와 30000번대의 Port로 서버풀이 지정되는 것을 확인 할 수 있다. AVI VS 상태 확인 ClusterIP 테스트 kubectl config use-context {Management Cluster} kubectl get cluster --show-labels kubectl label cluster tkgm01-tkc-dev01 ako-l7-clusterip-01=true kubectl config use-context {TKC} kubectl get cm avi-k8s-config -n avi-system -o jsonpath={.data.serviceType} kubectl create deploy hello --image=paulbouwer/hello-kubernetes:1.7 --replicas=3 --port=8080 kubectl expose deployment hello --type=ClusterIP --port=80 --target-port=8080 kubectl create ingress hello --class=avi-lb --rule=\"hello.avi.tkg.io/=hello:8080\" ServiceType 변경 확인 AVI Routing 확인 ClusterIP로 구성이 되어 있기 때문에 아래와 같이 실제 Pod와 실제 Port로 서버풀이 지정되는 것을 확인 할 수 있다. AVI VS 상태 확인 ","date":"2022-04-05","objectID":"/avi/:1:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"2. AVI Infra Settings 만약에 하나의 클러스터에서 VS별 SE-GROUP을 분리 하거나, 또는 VS의 IP 대역을 분리 하고 싶을 경우 AVIINFRASETTING을 사용하여 구성을 할 수 있다. 구성 kubectl apply -f - \u003c\u003c EOF apiVersion: ako.vmware.com/v1alpha1 kind: AviInfraSetting metadata: name: other-infra spec: seGroup: name: Default-Group network: vipNetworks: - networkName: pg-dk-10.253.107.x cidr: 10.253.107.0/24 enableRhi: false l7Settings: shardSize: MEDIUM EOF kubectl apply -f - \u003c\u003c EOF apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: other-infra spec: controller: ako.vmware.com/avi-lb parameters: apiGroup: ako.vmware.com kind: AviInfraSetting name: other-infra EOF 테스트 kubectl create deploy hello --image=paulbouwer/hello-kubernetes:1.7 --replicas=3 --port=8080 kubectl expose deployment hello --type=ClusterIP --port=80 --target-port=8080 kubectl create ingress hello --class=other-infra --rule=\"hello.avi.tkg.io/=hello:8080\" VS IP 변경 전 아래와 같이 VS IP가 변경 된 것을 확인 할 수 있다. VS IP 변경 후 기존의 하나의 SE그룹에 모두 VS가 구성이 되어 있다. SE Group 생성 ## SEGROUP을 변경 kubectl patch aviinfrasettings other-infra --type 'json' -p '[{\"op\":\"replace\",\"path\":\"/spec/seGroup/name\",\"value\":\"tkgm01\"}]' kubectl patch ing hello --patch '{\"spec\": {\"ingressClassName\": \"other-infra\"}}' 현재는 SE가 기존의 기존 SE 배포 전 다른 SE 그룹으로 변경 후 다른 SE 그룹으로 변경 후 위와 같은 방법으로 동일한 클러스터 내에서 FQDN별로 SE-GROUP 또는 IP 대역을 분리 할 수 있다. ","date":"2022-04-05","objectID":"/avi/:2:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"3. AVI AutoScalling AVI Autoscalling 기본 동작 autoscalling 재 조정값 ## AVI Controller SSH접속 switchto tenant admin switchto cloud Default-Cloud auto_rebalance save configure serviceenginegroup Default-Group auto_rebalance_interval interval-value auto_rebalance_criteria option auto_rebalance_capacity_per_se integer-value 예시: auto_rebalance_interval 300 auto_rebalance_criteria 의 옵션 값은 아래와 같음 se_auto_rebalance_cpu se_auto_rebalance_mbps se_auto_rebalance_open_conns se_auto_rebalance_pps 예시: auto_rebalance_capacity_per_se 200000 max_cpu_usage value min_cpu_usage value ## 종합 switchto tenant Avi switchto cloud azure configure serviceenginegroup Default-Group auto_rebalance_interval 300 auto_rebalance_criteria se_auto_rebalance_pps auto_rebalance_capacity_per_se 200000 max_cpu_usage 70 min_cpu_usage 30 save auto_rebalance 변경 전 auto_rebalance 변경 후 auto scallingout ","date":"2022-04-05","objectID":"/avi/:3:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"4. BGP 연동 후 Rhi(Route Health Injection) BGP ECMP를 구성 하여 SE를 탄력적으로 확장을 할 수 있습니다. 물리 스위치에는 ECMP를 구성이 필요. 구성 AVI에서 BGP 설정 BGP 설정 BGP 설정 SE에 접속 해서 BGP 상태를 확인 한다. ## AVI Controller SSH 접속 shell admin / {password} ## 서비스 엔진 접속 attach serviceengine tkcdevAvi-se-jpjbe ip netns ## 서비스 엔진 bash 접속 sudo ip netns exec avi_ns1 bash ## BGP 확인 netcat localhost bgpd enable show run show bgp summary BGP 상태 확인 BGP 상태 확인 RHI Enabled kubectl patch aviinfrasettings other-infra --type 'json' -p '[{\"op\":\"replace\",\"path\":\"/spec/network/enableRhi\",\"value\":true}]' Rhi Enable 스위치에서 라우팅 확인 BGP 라우팅 ","date":"2022-04-05","objectID":"/avi/:4:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"5. GatewayClass GATEWAY를 사용하는 이유는 여러개의 LoadBalancer의 IP를 하나의 IP로 설정하고 Port를 사용하기 위해서다. SVC를 생성하면 생성하는 만큼 IP가 생성이 되기때문에 IP를 공통으로 사용을 할 수 있다. GATEWAYCLASS 연계 구성을 하기 위해서는 클러스터에서 servicesAPI: true 를 True로 변경 필요 및 AutoFQDN와 DefaultDomain이 필요하다. 위에서 언급한 AVIINFRASETTING을 한 후 GATEWAYCLASS를 생성한다. cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.x-k8s.io/v1alpha1 kind: GatewayClass metadata: name: critical-gwc spec: controller: ako.vmware.com/avi-lb parametersRef: group: ako.vmware.com kind: AviInfraSetting name: other-infra EOF GATEWAY 생성 cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.x-k8s.io/v1alpha1 kind: Gateway metadata: name: avi-alb-gw namespace: default spec: gatewayClassName: critical-gwc listeners: - protocol: TCP port: 8080 routes: selector: matchLabels: ako.vmware.com/gateway-namespace: default ako.vmware.com/gateway-name: avi-alb-gw group: v1 kind: Service - protocol: TCP port: 80 routes: selector: matchLabels: ako.vmware.com/gateway-namespace: default ako.vmware.com/gateway-name: avi-alb-gw group: v1 kind: Service EOF 만약에 LB IP를 지정 하고 싶다면. 아래와 같이 IP를 지정하면 된다. cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.x-k8s.io/v1alpha1 kind: Gateway metadata: name: avi-alb-gw namespace: default spec: gatewayClassName: critical-gwc addresses: - type: IPAddress value: 10.253.107.203 listeners: - protocol: TCP port: 8080 routes: selector: matchLabels: ako.vmware.com/gateway-namespace: default ako.vmware.com/gateway-name: avi-alb-gw group: v1 kind: Service - protocol: TCP port: 80 routes: selector: matchLabels: ako.vmware.com/gateway-namespace: default ako.vmware.com/gateway-name: avi-alb-gw group: v1 kind: Service EOF 테스트 kubectl create deploy hello --image=paulbouwer/hello-kubernetes:1.7 --replicas=3 --port=8080 kubectl expose deployment hello --type=LoadBalancer --port=80 --target-port=8080 -l 'ako.vmware.com/gateway-namespace=default','ako.vmware.com/gateway-name=avi-alb-gw' 아래와 같이 동일한 IP로 두개의 SVC를 동일한 IP로 Port(80 , 8080)만 다른게 구성 할 수 있다. VS상태#1 VS상태#2 VS상태#2 ","date":"2022-04-05","objectID":"/avi/:5:0","tags":["tanzu","avi","tanzu integration avi","AVIinfrasetting","ClusterIP","AVI BGP","AVI AutoScalling","NodePort","NodePortLocal","dk","dokyung","avi rhi"],"title":"The Documentation vSphere Tanzu with AVI Load Balancer","uri":"/avi/"},{"categories":["Documentation"],"content":"VxRail","date":"2022-03-28","objectID":"/vxrail/","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"VxRail은 전체 스택 무결성 및 포괄적인 수명주기 관리로 비즈니스 변화에 신속하게 대응하는 인프라스트럭처를 제공하여 운영 효율성을 높이고, 위험을 줄이며, 팀이 비즈니스에 집중할 수 있도록 지원합니다. ","date":"2022-03-28","objectID":"/vxrail/:0:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"PreConfig ","date":"2022-03-28","objectID":"/vxrail/:1:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"DNS 구성 Component Name BaseDomain A Record vcsa01 vxrail.local 192.168.215.10 vxmgr vxrail.local 192.168.215.9 esxi01 vxrail.local 192.168.215.11 esxi02 vxrail.local 192.168.215.12 esxi03 vxrail.local 192.168.215.13 esxi04 vxrail.local 192.168.215.14 ","date":"2022-03-28","objectID":"/vxrail/:1:1","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"Default Passwqord Component Name Networking Configuration Username Default Password BIOS N/A 192.168.215.10 iDRAC DHCP 192.168.215.9 root ESXi DHCP 192.168.215.11 root vCenter Server Applicance root vmware VxRail Manager 192.168.10.200 root Passw0rd! VxRail Manager 192.168.10.200 mystic VxRailManager@201602! ","date":"2022-03-28","objectID":"/vxrail/:1:2","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"1. RASR을 통해 초기화 기본적으로 VxRail장비가 입고가 되면 기본 이미지로 부팅이 되며 라이센스가 있을 경우 업그레이드를 할 수 있다. 초기화를 하는 이유는 테스트 위해 60일 EVALUATION을 재사용하려면 RASR을 통해 공장 초기화를 해야 한다. 기존의 VxRail을 구성한 상태에서 증설을 위해 추가로 구성을 하게 될경우 기본 들어오는 버전이 낮거나 높을 수 있다. 이럴 경우 RASR을 통해 펌웨어, 이미지를 업그레이드 또는 다운그레이드를 통해 증설을 할 수 있다. 기본 설치되어 있는 VxRail이 테스트 하고자 하는 버전 보다 낮거나 또는 높거나 할 경우 변경을 할 수 있다. 초기화 방법 USB 장비 모든 부분의 버전을 업그레이드 및 다운그레이가 필요 할 경우 USB 방법사용, 또한 설치가 제일 빠름 Virtual Media iDRAC의 Virtual Media를 통해 설치 하는 방법 부분적 설치만 가능 (Firmware 등등 업그레이드 또는 다운그레이드 못함) 또한 설치가 느림 USB를 연결 한 후 iDRAC에서 Local SD Card로 부팅 USB 설치#1 USB 설치#2 Advancde 항목으로 접근 Advanced Install DUP(s) 펌웨어 등 업그레이드 및 다운그레이드할 내용이 있는지 확인이 필요 Install DUP(s)#1 업그레이드 / 다운그레이드를 선택 후 업그레이드가 있는지 다운그레이드가 있는지 체크 후 있으면 설치 Install DUP(s)#2 진행 시 재부팅이 여러번 발생 대략 30분 ~ 1시간 소요 Install DUP(s)#3 DUP를 업그레이드 또는 다운그레이드 이후 [F]actory Reset 실행 (1시간 소요) ","date":"2022-03-28","objectID":"/vxrail/:2:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"2. ESXI 초기 설정 ESXI#1 ESXi Shell Enable을 통해 콘솔 접속 ESXI#2 esxcli vm process list 명령어를 VxRail Manager가 보이는 호스트를 찾는다. 아무곳에서나 해도 되지만. 시간이 오래 걸린다. ESXI#3 기본 디폴트 IP는 192.168.10.200이므로 통신이 된다면 VxRail Manager VM을 켜고 접속 하면 된다. 만약 VLAN 또는 ESXI에 접근을 하고 싶으면 명령어로 VLAN 또는 IP를 설정 할 수 있다 또는 VM을 킬 수도 있다. 여기에서는 여러가지 방법을 설명하지는 않겠다. 기본 디폴트 IP와 변경하고자 하는 IP 둘다 통신이 되면 된다. 여러가지 방법론이 있겠지만, 신경을 쓰고 싶지 않다면 DEFAULT IP와 변경하고자 하는 IP 둘다 통신이 되는 것으로 테스트를 하면 좋을 것이다. VLAN 3939가 실제적으로 Discovery를 하는 부분으로 스위치에서 3939설정을 해주면 나중에 호스트 증설을 할때 별도로 신경을 쓰지 않아도 된다. 명령어 esxcli network vswitch standard portgroup list esxcli network vswitch standard portgroup set -p \"Private Management Network\" -v 0 esxcli network ip interface list esxcli network ip interface ipv4 set -i vmk2 -I 192.168.215.13 -N 255.255.255.0 -g 192.168.215.1 -t static VxRail Manager IP 변경 방법 vxrail-primary --setup --vxrail-address 192.168.215.9 --vxrail-netmask 255.255.255.0 --vxrail-gateway 192.168.215.1 --no-roll-back --verbose ","date":"2022-03-28","objectID":"/vxrail/:3:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"3. INIT 원하는 언어를 선택 할 수 있다. INIT#1 INIT#2 Standrad Cluster (3 or more hosts)는 3개이상의 VSAN 구성 VSAN 2-Node Cluster (2 hosts only) witness host 필요 Dynamic Node Cluster (2 or more hosts) VSAN 구성이 아닌 외부 스토리지로 구성 할 경우 INIT#3 설치가 잘 되었다면 아래와 같이 자동으로 Discovery가 됨 INIT#4 INIT#5 JSON파일로 구성이 다 되어 있다면 JSON을 삽입 해도 되지만 없으면 Step-by-step으로 진행 INIT#6 INIT#7 INIT#8 INIT#9 INIT#10 INIT#11 INIT#12 나중을 위해서 JSON파일을 다운로드 받을 수 있다. INIT#13 INIT#14 INIT#15 INIT#16 ","date":"2022-03-28","objectID":"/vxrail/:4:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"4. HOST ADD 만약 기존 구성된 클러스터와 다른 버전이 설치되어 있는 호스트라면 RSAR 초기화 필요 호스트를 추가 할 때 Host DISCOVERY가 되지 않으면 loudmouth restart를 해준다. HOST ADD#1 HOST ADD#2 HOST ADD#3 HOST ADD#4 HOST ADD#5 HOST ADD#6 HOST ADD#7 HOST ADD#8 HOST ADD#9 HOST ADD#10 HOST ADD 시 HOST가 DISCOVERY되지 않는다면 loudmouth를 리스타트 해준다. HOST ADD#11 ","date":"2022-03-28","objectID":"/vxrail/:5:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"5. UPGRADE UPGRADE#1 UPGRADE#2 UPGRADE#3 UPGRADE#4 UPGRADE#5 UPGRADE#6 UPGRADE#7 UPGRADE#8 UPGRADE#9 UPGRADE#10 ","date":"2022-03-28","objectID":"/vxrail/:6:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"6. VxRail BackUP 백업 스케줄 설정 가능 VxRail 접속 ## 백업 리스트 확인 cd /mystic/vxm_backup_restore python vxm_backup_restore.py -l ## 메뉴얼 백업 python vxm_backup_restore.py -b 스케쥴 백업 메뉴얼 백업 백업 리스트 확인 ","date":"2022-03-28","objectID":"/vxrail/:7:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"6. VxRail Recovery VxRail Manager가 망가졌을 경우 사용 가능 백업이 없으면 의미 없음 VxRail OVA 이미지 필요 ## 신규 설치한 vxrail manager 접속 후 IP 변경 /opt/vmware/share/vami/vami_set_network eth0 STATICV4 192.168.215.9 255.255.255.0 192.168.215.1 ## Backup 확인 cd /mystic/vxm_backup_restore python vxm_backup_restore.py -l ## Recovery 실행 1 python vxm_backup_restore.py -r –-vcenter 192.168.215.10 ","date":"2022-03-28","objectID":"/vxrail/:8:0","tags":["VxRail","HCI","dk","dokyung"],"title":"The Documentation VxRail","uri":"/vxrail/"},{"categories":["Documentation"],"content":"Tanzu Application Platform","date":"2022-03-09","objectID":"/tanzu-application-platform/","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"1. VMware TAP? VMware Tanzu 애플리케이션 플랫폼은 개발자와 운영자가 Kubernetes 플랫폼에서 앱을 보다 쉽게 구축, 배포 및 관리할 수 있도록 도와주는 패키지된 구성 요소 집합입니다. Tanzu 애플리케이션 플랫폼은 Kubernetes 기반 앱 개발의 내부 루프와 외부 루프 모두에서 워크플로를 단순화합니다. 내부 루프: 내부 루프는 개발자가 앱을 코딩하고 테스트하는 로컬 개발 환경을 설명합니다. 내부 루프에서 발생하는 활동에는 코드 작성, 버전 제어 시스템에 커밋, 개발 또는 스테이징 환경에 배포, 테스트 및 추가 코드 변경이 포함됩니다. 외부 루프: 외부 루프는 앱을 프로덕션에 배포하고 시간이 지남에 따라 유지 관리하는 단계를 설명합니다. 예를 들어, 클라우드 네이티브 플랫폼에서 외부 루프에는 컨테이너 이미지 빌드, 컨테이너 보안 추가, 지속적 통합(CI) 및 지속적 전달(CD) 파이프라인 구성과 같은 활동이 포함됩니다. VMware Tanzu 애플리케이션 플랫폼은 보안 및 확장을 지원하는 모든 Kubernetes에서 코드를 실행할 수 있도록 사전 포장된 프로덕션 경로를 개발 팀에 제공합니다. 팀이 조직의 기본 설정에 따라 사용자 지정할 수 있도록 모듈화된 애플리케이션 인식 플랫폼입니다. ","date":"2022-03-09","objectID":"/tanzu-application-platform/:1:0","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"주의 사항 현재 버그가 있는것으로 보임 Private Harbor 구성시 사설 인증서 문제가 발생 하기 때문에 외부 Registry 활용 필요, Github 연동시 Integration으로 설정 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:1:1","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"사전 설치 DOCKER GCR (Google Container Registry) GitHub DNS Records ","date":"2022-03-09","objectID":"/tanzu-application-platform/:1:2","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"Resource requirements To deploy all Tanzu Application Platform packages, your cluster must have at least: 8 CPUs for i9 (or equivalent) available to Tanzu Application Platform components 12 CPUs for i7 (or equivalent) available to Tanzu Application Platform components 8 GB of RAM across all nodes available to Tanzu Application Platform 12 GB of RAM is available to build and deploy applications, including Minikube. VMware recommends 16 GB of RAM for an optimal experience. 70 GB of disk space available per node For the full profile, or use of Security Chain Security Tools - Store, your cluster must have a configured default StorageClass. ","date":"2022-03-09","objectID":"/tanzu-application-platform/:1:3","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2. TAP 1.0.1 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:0","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"Tools and CLI requirements Installation requires: The Kubernetes CLI, kubectl, v1.20, v1.21 or v1.22, installed and authenticated with administrator rights for your target cluster. See Install Tools in the Kubernetes documentation. ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:1","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2.1. TAP 2.1.1 Tanzu Network 등록 Tanzu Network ID/PW export INSTALL_REGISTRY_USERNAME= #### Tanzu Network ID export INSTALL_REGISTRY_PASSWORD= #### Tanzu Network PW export INSTALL_REGISTRY_HOSTNAME=registry.tanzu.vmware.com export TAP_VERSION=1.0.1 namespace 생성 kubectl create ns tap-install tanzu registry 추가 tanzu secret registry add tap-registry \\ --username ${INSTALL_REGISTRY_USERNAME} --password ${INSTALL_REGISTRY_PASSWORD} \\ --server ${INSTALL_REGISTRY_HOSTNAME} \\ --export-to-all-namespaces --yes --namespace tap-install tanzu repository 추가 tanzu package repository add tanzu-tap-repository \\ --url registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:$TAP_VERSION \\ --namespace tap-install EULA 허용 EULA 허용 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:2","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2.2. Tanzu TAP 설치 리스트 확인 Registry / Repository 확인 tanzu secret registry list -n tap-install tanzu package repository list -n tap-install tanzu package repository get tanzu-tap-repository --namespace tap-install tanzu package available list --namespace tap-install tanzu package available list tap.tanzu.vmware.com --namespace tap-install Registry 리스트 확인 Repository 리스트 확인 Package 리스트 확인 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:3","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2.3. Tanzu TAP 설치 Private Harbor의 경우 사설 인증서가 문제가 있으므로 외부에서 제공하는 Registry 사용하는 필요. TAP 설치 GCR에서 키값을 json으로 다운로드 받은 후 service_account_key[변수] 저장 tanzu secret registry add registry-credentials --server gcr.io --username _json_key --password \"$(cat main-xxxx-xxx-xxxx.json)\" --namespace tap-install service_account_key=\"$(cat main-xxxx-xxx-xxxx.json)\" 실행 파일 설정 cat \u003c\u003cEOF \u003e gcr-tap-values.yaml profile: full ceip_policy_disclosed: true # The value must be true for installation to succeed buildservice: kp_default_repository: \"gcr.io/{Registry ID}/build-service\" kp_default_repository_username: _json_key kp_default_repository_password: '$(echo $service_account_key)' tanzunet_username: \"\" ## Tanzu Network ID tanzunet_password: \"\" ## Tanzu Network Password descriptor_name: \"tap-1.0.0-full\" enable_automatic_dependency_updates: true supply_chain: basic cnrs: domain_name: tkg.io accelerator: server: service_type: \"ClusterIP\" ootb_supply_chain_basic: registry: server: \"gcr.io\" repository: \"{Registry ID}/supply_chain\" gitops: #repository_prefix: git@github.com:vmware-tanzu/ #branch: main #user_name: supplychain #user_email: supplychain #commit_message: supplychain@cluster.local #ssh_secret: git-ssh ssh_secret: \"\" cluster_builder: default service_account: default learningcenter: ingressDomain: \"tkg.io\" ingressClass: contour ingressSecret: secretName: workshops.example.com-tls contour: envoy: service: type: LoadBalancer tap_gui: service_type: ClusterIP ingressEnabled: \"true\" ingressDomain: \"tkg.io\" app_config: app: baseUrl: http://tap-gui.tkg.io support: url: https://tanzu.vmware.com/support items: - title: Contact Support icon: email links: - url: https://tanzu.vmware.com/support title: Tanzu Support Page - title: Documentation icon: docs links: - url: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/index.html title: Tanzu Application Platform Documentation integrations: github: # Other integrations available see NOTE below - host: github.com token: \"{GIT TOKEN}\" catalog: locations: - type: url target: https://github.com/huntedhappy/tanzu-java-web-app/catalog-info.yaml backend: baseUrl: http://tap-gui.tkg.io cors: origin: http://tap-gui.tkg.io # ##Existing values file above (OIDC) # auth: # allowGuestAccess: true # environment: development # loginPage: # github: # title: Github Login # message: Enter with your GitHub account # providers: # github: # development: # clientId: # clientSecret: # ## uncomment if using GitHub Enterprise # # enterpriseInstanceUrl: metadata_store: app_service_type: LoadBalancer # (optional) Defaults to LoadBalancer. Change to NodePort for distributions that don't support LoadBalancer grype: namespace: \"tap-install\" # (optional) Defaults to default namespace. EOF TAP 설치 tanzu package install tap -p tap.tanzu.vmware.com -v $TAP_VERSION --values-file gcr-tap-values.yml -n tap-install TAP 설치 완료 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:4","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2.4. Tanzu TAP RBAC 설정 RBAC 설정 dockerconfigjson=\"$(kubectl get secret tbs-builder-secret-gen-placeholder-secret -n tap-install -o jsonpath={.data.\\\\.dockerconfigjson})\" cat \u003c\u003cEOF | tee rbac.yaml apiVersion: v1 kind: Secret metadata: name: tap-registry annotations: secretgen.carvel.dev/image-pull-secret: \"\" type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: $(echo $dockerconfigjson) --- apiVersion: v1 kind: ServiceAccount metadata: name: default secrets: - name: registry-credentials imagePullSecrets: - name: registry-credentials - name: tap-registry --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default rules: - apiGroups: [source.toolkit.fluxcd.io] resources: [gitrepositories] verbs: ['*'] - apiGroups: [source.apps.tanzu.vmware.com] resources: [imagerepositories] verbs: ['*'] - apiGroups: [carto.run] resources: [deliverables, runnables] verbs: ['*'] - apiGroups: [kpack.io] resources: [images] verbs: ['*'] - apiGroups: [conventions.apps.tanzu.vmware.com] resources: [podintents] verbs: ['*'] - apiGroups: [\"\"] resources: ['configmaps'] verbs: ['*'] - apiGroups: [\"\"] resources: ['pods'] verbs: ['list'] - apiGroups: [tekton.dev] resources: [taskruns, pipelineruns] verbs: ['*'] - apiGroups: [tekton.dev] resources: [pipelines] verbs: ['list'] - apiGroups: [kappctrl.k14s.io] resources: [apps] verbs: ['*'] - apiGroups: [serving.knative.dev] resources: ['services'] verbs: ['*'] - apiGroups: [servicebinding.io] resources: ['servicebindings'] verbs: ['*'] - apiGroups: [services.apps.tanzu.vmware.com] resources: ['resourceclaims'] verbs: ['*'] - apiGroups: [scanning.apps.tanzu.vmware.com] resources: ['imagescans', 'sourcescans'] verbs: ['*'] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default subjects: - kind: ServiceAccount name: default EOF ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:5","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"2.5. workload 실행 INGRESS IP 확인 kubectl get svc -n tap-install kubectl get httpproxy -A ingress 및 DNS 확인 gui 접속 후 Tanzu Java Web App 실행 gui 접속 후 Tanzu Java Web App 실행 gui 접속 후 Tanzu Java Web App 실행 gui 접속 후 Tanzu Java Web App 다운로드 GIT PUSH 미리 GIT에 프로젝트 생성 후 다운로드 받은 ZIP파일 PUSH unzip tanzu-java-web-app.zip git init git remote add origin git@github.com:huntedhappy/tanzu-java-web-app git add . git commit -m 'first' git push origin main apps workload 실행 tanzu apps workload create tanzu-java-web-app \\ --git-repo https://github.com/huntedhappy/tanzu-java-web-app \\ --git-branch main \\ --type web \\ --label apps.tanzu.vmware.com/has-tests=true \\ --yes \\ -n tap-install 배포 상태 확인 tanzu apps cluster-supply-chain list tanzu apps workload tail tanzu-java-web-app --since 10m --timestamp -n tap-install kubectl get workload,gitrepository,pipelinerun,images.kpack,podintent,app,services.serving -n tap-install ","date":"2022-03-09","objectID":"/tanzu-application-platform/:2:6","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"3. TAP 1.1.0 Repository를 설정한다. 여기서는 GCR을 사용하기 때문에 GCR의 정보를 입력 export INSTALL_REGISTRY_HOSTNAME=gcr.io export TAP_VERSION=1.1.0 TANZU NET 및 GCR docker login 후 GCR에 이미지들을 다운로드 docker login registry.tanzu.vmware.com docker login -u _json_key --password-stdin https://gcr.io \u003c {gcr key} imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:${TAP_VERSION} --to-repo ${INSTALL_REGISTRY_HOSTNAME}/{gcr project}/tap-packages namespace 및 secret 생성 후 tanzu package Repository 생성 tanzu secret registry add tap-registry --server gcr.io --username _json_key --password \"$(cat {gcr key})\" --export-to-all-namespaces --yes -n tap-install tanzu secret registry add registry-credentials --server gcr.io --username _json_key --password \"$(cat {gcr key})\" --export-to-all-namespaces --yes -n tap-install tanzu package repository add tanzu-tap-repository \\ --url ${INSTALL_REGISTRY_HOSTNAME}/main-tokenizer-343509/tap-packages:$TAP_VERSION \\ --namespace tap-install ## Repository가 생성이 되었으면 설치 가능한 packages를 확인 tanzu package available list tap.tanzu.vmware.com --namespace tap-install 권한 설정 kubectl annotate secret tap-registry -n tap-install secretgen.carvel.dev/image-pull-secret=\"\" kubectl patch sa default -n tap-install --type 'json' -p '[{\"op\":\"add\",\"path\":\"/secrets\",\"value\":[\"name\":\"registry-credentials\",\"name\":\"tap-registry\"]}]' kubectl patch sa default -n tap-install --type 'json' -p '[{\"op\":\"add\",\"path\":\"/imagePullSecrets\",\"value\":[\"name\":\"registry-credentials\",\"name\":\"tap-registry\"]}]' cat \u003c\u003cEOF | kubectl -n tap-install apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-permit-deliverable roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: deliverable subjects: - kind: ServiceAccount name: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-permit-workload roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: workload subjects: - kind: ServiceAccount name: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: dev-permit-app-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: app-viewer subjects: - kind: Group name: \"namespace-developers\" apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: namespace-dev-permit-app-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: app-viewer-cluster-access subjects: - kind: Group name: \"namespace-developers\" apiGroup: rbac.authorization.k8s.io EOF TAP 1.1.0 설치 GCR에서 키값을 json으로 다운로드 받은 후 service_account_key[변수] 저장 tanzu secret registry add registry-credentials --server gcr.io --username _json_key --password \"$(cat main-xxxx-xxx-xxxx.json)\" --namespace tap-install service_account_key=\"$(cat main-xxxx-xxx-xxxx.json)\" 실행 파일 설정 cat \u003c\u003cEOF \u003e gcr-tap-values.yaml profile: full ceip_policy_disclosed: true # The value must be true for installation to succeed buildservice: kp_default_repository: \"gcr.io/{Registry ID}/build-service\" kp_default_repository_username: _json_key kp_default_repository_password: '$(echo $service_account_key)' tanzunet_username: \"\" ## Tanzu Network ID tanzunet_password: \"\" ## Tanzu Network Password descriptor_name: \"full\" enable_automatic_dependency_updates: true supply_chain: basic cnrs: domain_name: tkg.io accelerator: server: service_type: \"ClusterIP\" ootb_supply_chain_basic: registry: server: \"gcr.io\" repository: \"{Registry ID}/supply_chain\" gitops: #repository_prefix: git@github.com:vmware-tanzu/ #branch: main #user_name: supplychain #user_email: supplychain #commit_message: supplychain@cluster.local #ssh_secret: git-ssh ssh_secret: \"\" cluster_builder: default service_account: default learningcenter: ingressDomain: \"tkg.io\" ingressClass: contour ingressSecret: secretName: workshops.example.com-tls contour: envoy: service: type: LoadBalancer tap_gui: service_type: ClusterIP ingressEnabled: \"true\" ingressDomain: \"tk","date":"2022-03-09","objectID":"/tanzu-application-platform/:3:0","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"3.1. MultiCluster MultiCluster 3개의 클러스터를 생성한다. 여기서는 아래와 같이 구성 하였다. tkgm01-tkc-dev02 = build tkgm02-tkc-dev03 = run tkgm03-tkc-dev04 = view 이미지를 Repository에 저장한다. export INSTALL_REGISTRY_HOSTNAME=gcr.io export TAP_VERSION=1.1.0 ## Tanzu Network에 로그인을 한다. docker login registry.tanzu.vmware.com -u {ID} --password-stdin \u003c ./password.txt ## GCR에 로그인을 한다. docker login -u _json_key --password-stdin https://gcr.io \u003c {togken}}.json ## 이미지를 GCR에 복사 한다. imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:${TAP_VERSION} --to-repo ${INSTALL_REGISTRY_HOSTNAME}/main-tokenizer-343509/tap-packages 모든 이미지가 GCR에 저장이 완료 되면 Repository를 각각의 클러스터에 등록 해준다. ## Cluster 변경 kubectl use-context {cluster context} ## NameSpace 생성 kubectl create ns tap-install ## GCR 접속 Secret 생성 tanzu secret registry add tap-registry --server gcr.io --username _json_key --password \"$(cat {gcr-key}.json)\" --export-to-all-namespaces --yes -n tap-install tanzu secret registry add registry-credentials --server gcr.io --username _json_key --password \"$(cat {gcr-key}.json)\" --export-to-all-namespaces --yes -n tap-install ## Tanzu Package Repository 추가 tanzu package repository add tanzu-tap-repository \\ --url ${INSTALL_REGISTRY_HOSTNAME}/main-tokenizer-343509/tap-packages:$TAP_VERSION \\ --namespace tap-install ## Tanzu Package Repository 확인 tanzu package repository list -n tap-install 각각의 클러스터의 맞는 tap을 설치 해준다. build-tap-values.yaml cat \u003c\u003c EOF | tee build-tap-values.yaml profile: build ceip_policy_disclosed: true buildservice: kp_default_repository: \"gcr.io/{gcr project}/tap-packages\" kp_default_repository_username: _json_key kp_default_repository_password: {gcr key} tanzunet_username: \"\" ## Tanzu Network ID tanzunet_password: \"\" ## Tanzu Network Password descriptor_name: \"full\" enable_automatic_dependency_updates: true supply_chain: basic ootb_supply_chain_basic: registry: server: \"gcr.io\" repository: \"{gcr project}/supply_chain\" gitops: ssh_secret: \"\" cluster_builder: default service_account: default grype: namespace: \"tap-install\" # (optional) Defaults to default namespace. targetImagePullSecret: tap-registry EOF run-tap-values.yaml cat \u003c\u003c EOF | tee run-tap-values.yaml profile: run ceip_policy_disclosed: true # Installation fails if this is not set to true. Not a string. supply_chain: basic cnrs: domain_name: tkg.io contour: envoy: service: type: LoadBalancer #NodePort can be used if your Kubernetes cluster doesn't support LoadBalancing appliveview_connector: backend: sslDisabled: \"true\" host: appliveview.tkg.io EOF 클러스터를 변경 하면서 TAP을 설치 해준다. kubectl use-context {TAP 클러스터} tanzu package install tap -p tap.tanzu.vmware.com -v 1.1.0 -f build-tap-values.yaml -n tap-install tanzu package install tap -p tap.tanzu.vmware.com -v 1.1.0 -f run-tap-values.yaml -n tap-install build 와 run 클러스터에 TAP 설치가 완료 되었다면 클러스터의 URL 과 TOKEN을 확인한다. CLUSTER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}') CLUSTER_TOKEN=$(kubectl -n tap-gui get secret $(kubectl -n tap-gui get sa tap-gui-viewer -o=json \\ | jq -r '.secrets[0].name') -o=json \\ | jq -r '.data[\"token\"]' \\ | base64 --decode) echo CLUSTER_URL: $CLUSTER_URL echo CLUSTER_TOKEN: $CLUSTER_TOKEN view-tap-values.yaml cat \u003c\u003c EOF | tee view-tap-values.yaml profile: view ceip_policy_disclosed: true # Installation fails if this is not set to true. Not a string. contour: envoy: service: type: LoadBalancer #NodePort can be used if your Kubernetes cluster doesn't support LoadBalancing learningcenter: ingressDomain: \"tkg.io\" tap_gui: service_type: ClusterIP ingressEnabled: \"true\" ingressDomain: \"tkg.io\" app_config: app: baseUrl: http://tap-gui.tkg.io catalog: locations: - type: url target: https://GIT-CATALOG-URL/catalog-info.yaml backend: baseUrl: http://tap-gui.tkg.io cors: origin: http://tap-gui.tkg.io kubernetes: serviceLocatorMethod: type: 'multiTenant' clusterLocatorMethods: - type: 'config' clusters: - url: https://10.253.125.252:6443 name: tkgm01-tkc-dev02 authProvider: serviceAccount skipTLSVe","date":"2022-03-09","objectID":"/tanzu-application-platform/:3:1","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"4. Visual Studio IDE를 Visual Studio를 사용하여 동작, 현재는 Visual studio만 지원 하고 있음 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:4:0","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"4.1. Extenstion 설정 apt search openjdk apt install openjdk-11-jdk -y java --version echo \"allow_k8s_contexts('$(kubectl config current-context)')\" \u003e\u003e /var/tmp/tap/tanzu-java-web-app/Tiltfile ## 맨아래 해당 context가 들어가 있는 것을 확인 할 수 있다. cat /var/tmp/tap/tanzu-java-web-app/Tiltfile ctrl + shift + p VS 설정 VS 설정 VS 설정 VS 설정 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:4:1","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"4.2. Live Update Start 해당 부분을 수정 하면 자동으로 GIT에 업데이트가 되면서 바뀌는것을 볼수 있다. 수정#1 수정#2 수정#3 수정#4 ","date":"2022-03-09","objectID":"/tanzu-application-platform/:4:2","tags":["tanzu","k8s","TAP","tanzu application platform","devops","dk","dokyung","tap 1.0.1","tap 1.1.0","vmware tap 1.0.1","vmware tap 1.1.0","tap multicluster"],"title":"The Documentation TAP","uri":"/tanzu-application-platform/"},{"categories":["Documentation"],"content":"PRACTICE","date":"2022-01-20","objectID":"/pratics/","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"본 문서는 코딩 따라하기 테스트 페이지, 제가 다른 문서를 참조해서 따라해보는 페이지 입니다. ","date":"2022-01-20","objectID":"/pratics/:0:0","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"1. NGINX + PHP + MYSQL 연동 환경 UBUNTU: VERSION=“20.04.3 LTS (Focal Fossa)” MYSQL: mysql Ver 8.0.28 for Linux on x86_64 (MySQL Community Server - GPL) PHP: PHP 7.4.3 (cli) (built: Nov 25 2021 23:16:22) ( NTS ) NGINX: nginx version: nginx/1.20.2 사전 지식 NGINX MYSQL PHP PDO PDO(PHP Data Objects)란 여러가지 데이터베이스를 제어하는 방법을 표준화시킨 것이다. 데이터베이스는 다양한 종류가 있다. 그리고 종류에 따라서 서로 다른 드라이브를 사용해 왔는데 드라이브의 종류에 따라서 데이터베이스를 제어하기 위한 API가 달랐다. PDO를 사용하면 동일한 방법으로 데이터베이스를 제어할 수 있다. PDO를 사용하기 위해서는 ## 해당 파일에서 vi /etc/php/7.4/fpm/php.ini ## 아래 pdo_mysql 주석을 제거 한다. ;extension=pdo_firebird extension=pdo_mysql ;extension=pdo_oci ;extension=pdo_odbc ;extension=pdo_pgsql ;extension=pdo_sqlite mysql 사전 작업 ## 데이터베이스(스키마) 생성 CREATE DATABASE opentutorials CHARACTER SET utf8 COLLATE utf8_general_ci; ## 설정 할 스키마 선택 use opentutorials; ## TABLE 생성 CREATE TABLE topic ( id int(11) NOT NULL AUTO_INCREMENT, title varchar(255) NOT NULL , description text NULL , created datetime NOT NULL , PRIMARY KEY (id) ); PHP 구성 input.php \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"/\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"./process.php?mode=insert\" method=\"POST\"\u003e \u003cp\u003e제목 : \u003cinput type=\"text\" name=\"title\"\u003e\u003c/p\u003e \u003cp\u003e본문 : \u003ctextarea name=\"description\" id=\"\" cols=\"30\" rows=\"10\"\u003e\u003c/textarea\u003e\u003c/p\u003e \u003cp\u003e\u003cinput type=\"submit\" /\u003e\u003c/p\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e processphp \u003c?php $dbh = new PDO('mysql:host=localhost;dbname=opentutorials', 'root', '111111', array(PDO::MYSQL_ATTR_INIT_COMMAND =\u003e \"SET NAMES utf8\")); switch($_GET['mode']){ case 'insert': $stmt = $dbh-\u003eprepare(\"INSERT INTO topic (title, description, created) VALUES (:title, :description, now())\"); $stmt-\u003ebindParam(':title',$title); $stmt-\u003ebindParam(':description',$description); $title = $_POST['title']; $description = $_POST['description']; $stmt-\u003eexecute(); header(\"Location: list.php\"); break; case 'delete': $stmt = $dbh-\u003eprepare('DELETE FROM topic WHERE id = :id'); $stmt-\u003ebindParam(':id', $id); $id = $_POST['id']; $stmt-\u003eexecute(); header(\"Location: list.php\"); break; case 'modify': $stmt = $dbh-\u003eprepare('UPDATE topic SET title = :title, description = :description WHERE id = :id'); $stmt-\u003ebindParam(':title', $title); $stmt-\u003ebindParam(':description', $description); $stmt-\u003ebindParam(':id', $id); $title = $_POST['title']; $description = $_POST['description']; $id = $_POST['id']; $stmt-\u003eexecute(); header(\"Location: list.php?id={$_POST['id']}\"); break; } ?\u003e list.php \u003c?php $dbh = new PDO('mysql:host=localhost;dbname=opentutorials', 'root', '111111'); $stmt = $dbh-\u003eprepare('SELECT * FROM topic'); $stmt-\u003eexecute(); $list = $stmt-\u003efetchAll(); if(!empty($_GET['id'])) { $stmt = $dbh-\u003eprepare('SELECT * FROM topic WHERE id = :id'); $stmt-\u003ebindParam(':id', $id, PDO::PARAM_INT); $id = $_GET['id']; $stmt-\u003eexecute(); $topic = $stmt-\u003efetch(); } ?\u003e\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\" /\u003e \u003cstyle type=\"text/css\"\u003e body { font-size: 0.8em; font-family: dotum; line-height: 1.6em; } header { border-bottom: 1px solid #ccc; padding: 20px 0; } nav { float: left; margin-right: 20px; min-height: 1000px; min-width:150px; border-right: 1px solid #ccc; } nav ul { list-style: none; padding-left: 0; padding-right: 20px; } article { float: left; } .description{ width:500px; } \u003c/style\u003e \u003c/head\u003e \u003cbody id=\"body\"\u003e \u003cdiv\u003e \u003cnav\u003e \u003cul\u003e \u003c?php foreach($list as $row) { echo \"\u003cli\u003e\u003ca href=\\\"?id={$row['id']}\\\"\u003e\".htmlspecialchars($row['title']).\"\u003c/a\u003e\u003c/li\u003e\"; } ?\u003e \u003c/ul\u003e \u003cul\u003e \u003cli\u003e\u003ca href=\"input.php\"\u003e추가\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e \u003c/nav\u003e \u003carticle\u003e \u003c?php if(!empty($topic)){ ?\u003e \u003ch2\u003e\u003c?=htmlspecialchars($topic['title'])?\u003e\u003c/h2\u003e \u003cdiv class=\"description\"\u003e \u003c?=htmlspecialchars($topic['description'])?\u003e \u003c/div\u003e \u003cdiv\u003e \u003ca href=\"modify.php?id=\u003c?=$topic['id']?\u003e\"\u003e수정\u003c/a\u003e \u003cform method=\"POST\" action=\"process.php?mode=delete\"\u003e \u003cinput type=\"hidden\" name=\"id\" value=\"\u003c?=$topic['id']?\u003e\" /\u003e \u003cinput type=\"submit\" value=\"삭제\" /\u003e \u003c/form\u003e \u003c/div\u003e \u003c?php } ?\u003e \u003c/article\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e modify.php \u003c?php $dbh = new PDO('mysql:host=localhost;dbname=opentutorials', 'root', '111111'); $stmt = $dbh-\u003eprepare('SELECT * FROM topic WHERE id = :id');","date":"2022-01-20","objectID":"/pratics/:1:0","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2. GOLANG fmt link ","date":"2022-01-20","objectID":"/pratics/:2:0","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.1. Hello World package main import \"fmt\" func main() { fmt.println(\"Hello World\") } Hello World ","date":"2022-01-20","objectID":"/pratics/:2:1","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.2. Const package main import \"fmt\" func main() { var conferenceName = \"Go Conference\" const conferenceTickets = 50 fmt.Println(\"Welcome to\", conferenceName, \"booking application\") fmt.Println(\"Get your tickets here to attend\") conferenceTickets = 30 fmt.Println(conferenceTickets) } const로 정의를 하면 아래처럼 값을 변경 할 수 없다. const var로 변경을 하면 아래에서 30으로 변경이 가능하다. var 변경 ","date":"2022-01-20","objectID":"/pratics/:2:2","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.3. printf package main import \"fmt\" func main() { var conferenceName = \"Go Conference\" const conferenceTickets = 50 var remainingTickets = 50 fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } printf#1 printf#2 ","date":"2022-01-20","objectID":"/pratics/:2:3","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.4. Data Type Golang DataType 설정한 DataType을 확인 할 수 있다. fmt.Printf(“Welcome to %T booking application\\n”, conferenceName) var conferenceName = “Go Confernece” -\u003e conferenceName := “Go Conference” 로 하면 알아서 타입을 변경 해준다. 만약 숫자를 집어 넣으면 알아서 int로 변경을 해줌 package main import \"fmt\" func main() { conferenceName := \"Go Conference\" const conferenceTickets = 50 var remainingTickets = 50 fmt.Printf(\"conferenceTickets is %T, remainingTickets is %T, conferencename is %T\\n\", conferenceTickets, remainingTickets, conferenceName) fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") var userName string var userTickets int userName = \"TOM\" userTickets = 2 fmt.Printf(\"User %v booked %v tickets.\\n\", userName, userTickets) } datatype#1 datatype#2 %T로 했을 경우 DataType을 확인 할 수 있다. datatype#3 datatype#4 ","date":"2022-01-20","objectID":"/pratics/:2:4","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.4. INPUT, 포인터를 사용해야한다. \u0026 \u003c- 포인터를 사용 fmt.Scan(\u0026useraName) 을하면 사용자가 입력을 할 수 있게 해준다. 포인터를 사용하지 않을 경우 저장된 메모리 값을 확인 할 수 있다. package main import \"fmt\" func main() { conferenceName := \"Go Conference\" const conferenceTickets = 50 var remainingTickets uint = 50 fmt.Printf(\"conferenceTickets is %T, remainingTickets is %T, conferencename is %T\\n\", conferenceTickets, remainingTickets, conferenceName) fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) // 이지점에서 사용자에게 물어봄 fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) // 이지점에서 사용자에게 물어봄 fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) // 이지점에서 사용자에게 물어봄 fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) // 이지점에서 사용자에게 물어봄 remainingTickets = remainingTickets - userTickets fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } input ","date":"2022-01-20","objectID":"/pratics/:2:5","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.5. Array 배열의 경우 공간중간 값이 없어도 빈공간으로 인식을 하기 때문에 메모리를 그만큼 할당 하게 된다. 배열 2가지가 추가됨 var bookings [50]string bookings[0] = firstName + \" \" + lastName package main import \"fmt\" func main() { conferenceName := \"Go Conference\" const conferenceTickets = 50 var remainingTickets uint = 50 var bookings [50]string fmt.Printf(\"conferenceTickets is %T, remainingTickets is %T, conferencename is %T\\n\", conferenceTickets, remainingTickets, conferenceName) fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) remainingTickets = remainingTickets - userTickets bookings[0] = firstName + \" \" + lastName fmt.Printf(\"The whole array: %v\\n\", bookings) fmt.Printf(\"The first value: %v\\n\", bookings[0]) fmt.Printf(\"The first value: %T\\n\", bookings) fmt.Printf(\"The first value: %v\\n\", len(bookings)) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } 공간이 이미 할당이 되어 있는 것을 확인 할 수 있다. array ","date":"2022-01-20","objectID":"/pratics/:2:6","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.6. Slice Slice구성 아래 2가지가 변경되고 하나 추가됨. var bookings [50]string -\u003e bookings := []string{} bookings[0] = firstName + \" \" + lastName -\u003e bookings = append(bookings, firstName + \" \" + lastName) fmt.Printf(“These are all our booking: %v\\n”, bookings) package main import \"fmt\" func main() { conferenceName := \"Go Conference\" const conferenceTickets = 50 var remainingTickets uint = 50 bookings := []string{} fmt.Printf(\"conferenceTickets is %T, remainingTickets is %T, conferencename is %T\\n\", conferenceTickets, remainingTickets, conferenceName) fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"The whole slice: %v\\n\", bookings) fmt.Printf(\"The first value: %v\\n\", bookings[0]) fmt.Printf(\"The first type: %T\\n\", bookings) fmt.Printf(\"The first lgenth: %v\\n\", len(bookings)) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) fmt.Printf(\"These are all our booking: %v\\n\", bookings) } 작성한 값만 들어가 있는 것을 확인 할 수 있다. slice ","date":"2022-01-20","objectID":"/pratics/:2:7","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.7 Loop \u0026 If package main import ( \"fmt\" \"strings\" ) func main() { conferenceName := \"Go Conference\" const conferenceTickets int = 50 var remainingTickets uint = 50 bookings := []string{} fmt.Printf(\"conferenceTickets is %T, remainingTickets is %T, conferencename is %T\\n\", conferenceTickets, remainingTickets, conferenceName) fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") for { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) isValidName := len(firstName) \u003e= 2 \u0026\u0026 len(lastName) \u003e= 2 isValidEmail := strings.Contains(email, \"@\") isValidTicketNumber := userTickets \u003e 0 \u0026\u0026 userTickets \u003c= remainingTickets if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) firstNames := []string{} for _, booking := range bookings { var names = strings.Fields(booking) firstNames = append(firstNames, names[0]) } fmt.Printf(\"These first names of bookings are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } ","date":"2022-01-20","objectID":"/pratics/:2:8","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.8. function package main import ( \"fmt\" \"strings\" ) func main() { conferenceName := \"Go Conference\" const conferenceTickets int = 50 var remainingTickets uint = 50 bookings := []string{} greetUsers(conferenceName, conferenceTickets, remainingTickets) for { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) isValidName := len(firstName) \u003e= 2 \u0026\u0026 len(lastName) \u003e= 2 isValidEmail := strings.Contains(email, \"@\") isValidTicketNumber := userTickets \u003e 0 \u0026\u0026 userTickets \u003c= remainingTickets if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) printFirstNames(bookings) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers(confName string, confTickets int, remainingTickets uint) { fmt.Printf(\"Welcome to %v booking application\\n\", confName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", confTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func printFirstNames(bookings []string) { firstNames := []string{} for _, booking := range bookings { var names = strings.Fields(booking) firstNames = append(firstNames, names[0]) } fmt.Printf(\"These first names of bookings are: %v\\n\", firstNames) } ","date":"2022-01-20","objectID":"/pratics/:2:9","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.9. return func package main import ( \"fmt\" \"strings\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = []string{} func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { var names = strings.Fields(booking) firstNames = append(firstNames, names[0]) } return firstNames } func validateUserInput(firstName string, lastName string, email string, userTickets uint) (bool, bool, bool) { isValidName := len(firstName) \u003e= 2 \u0026\u0026 len(lastName) \u003e= 2 isValidEmail := strings.Contains(email, \"@\") isValidTicketNumber := userTickets \u003e 0 \u0026\u0026 userTickets \u003c= remainingTickets return isValidName, isValidEmail, isValidTicketNumber } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } ","date":"2022-01-20","objectID":"/pratics/:2:10","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.10. package helper.go 파일 생성 package main import \"strings\" func validateUserInput(firstName string, lastName string, email string, userTickets uint) (bool, bool, bool) { isValidName := len(firstName) \u003e= 2 \u0026\u0026 len(lastName) \u003e= 2 isValidEmail := strings.Contains(email, \"@\") isValidTicketNumber := userTickets \u003e 0 \u0026\u0026 userTickets \u003c= remainingTickets return isValidName, isValidEmail, isValidTicketNumber } main.go package main import ( \"fmt\" \"strings\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = []string{} func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { var names = strings.Fields(booking) firstNames = append(firstNames, names[0]) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } ","date":"2022-01-20","objectID":"/pratics/:2:11","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.11. multi package helper.go package helper import \"strings\" func ValidateUserInput(firstName string, lastName string, email string, userTickets uint, remainingTickets uint) (bool, bool, bool) { isValidName := len(firstName) \u003e= 2 \u0026\u0026 len(lastName) \u003e= 2 isValidEmail := strings.Contains(email, \"@\") isValidTicketNumber := userTickets \u003e 0 \u0026\u0026 userTickets \u003c= remainingTickets return isValidName, isValidEmail, isValidTicketNumber } main.go package main import ( \"GOLANG/helper\" \"fmt\" \"strings\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = []string{} func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := helper.ValidateUserInput(firstName, lastName, email, userTickets, remainingTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { var names = strings.Fields(booking) firstNames = append(firstNames, names[0]) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets bookings = append(bookings, firstName+\" \"+lastName) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } multi pacakge ","date":"2022-01-20","objectID":"/pratics/:2:12","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.12. MAPS package main import ( \"fmt\" \"strconv\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = make([]map[string]string, 0) func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { firstNames = append(firstNames, booking[\"firtName\"]) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets var userData = make(map[string]string) userData[\"firstName\"] = firstName userData[\"lastName\"] = lastName userData[\"email\"] = email userData[\"numberOfTickets\"] = strconv.FormatUint(uint64(userTickets), 10) bookings = append(bookings, userData) fmt.Printf(\"List of bookings is %v\\n\", bookings) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } maps ","date":"2022-01-20","objectID":"/pratics/:2:13","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.13. struct package main import ( \"fmt\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = make([]UserData, 0) type UserData struct { firstName string lastName string email string numberOfTickets uint } func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { firstNames = append(firstNames, booking.firstName) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets var userData = UserData{ firstName: firstName, lastName: lastName, email: email, numberOfTickets: userTickets, } bookings = append(bookings, userData) fmt.Printf(\"List of bookings is %v\\n\", bookings) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } struct ","date":"2022-01-20","objectID":"/pratics/:2:14","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.14. long processing task 메일을 보내는 10초간 기다리게 구성 package main import ( \"fmt\" \"time\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = make([]UserData, 0) type UserData struct { firstName string lastName string email string numberOfTickets uint } func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) sendTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { firstNames = append(firstNames, booking.firstName) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets var userData = UserData{ firstName: firstName, lastName: lastName, email: email, numberOfTickets: userTickets, } bookings = append(bookings, userData) fmt.Printf(\"List of bookings is %v\\n\", bookings) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } func sendTicket(userTickets uint, firstName string, lastName string, email string) { time.Sleep(10 * time.Second) var ticket = fmt.Sprintf(\"%v tickets for %v %v\", userTickets, firstName, lastName) fmt.Println(\"#########################\") fmt.Printf(\"Sending ticket: \\n %v to email address %v\\n\", ticket, email) fmt.Println(\"#########################\") } long processing ","date":"2022-01-20","objectID":"/pratics/:2:15","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"2.15. 비동기 메일을 기다리지 않고 보내면서 메일은 별도로 보내지게 구성 한다. package main import ( \"fmt\" \"time\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = make([]UserData, 0) type UserData struct { firstName string lastName string email string numberOfTickets uint } func main() { greetUsers() for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) // go라는 것을 넣어주면 된다. go sendTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } } } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { firstNames = append(firstNames, booking.firstName) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets var userData = UserData{ firstName: firstName, lastName: lastName, email: email, numberOfTickets: userTickets, } bookings = append(bookings, userData) fmt.Printf(\"List of bookings is %v\\n\", bookings) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } func sendTicket(userTickets uint, firstName string, lastName string, email string) { time.Sleep(10 * time.Second) var ticket = fmt.Sprintf(\"%v tickets for %v %v\", userTickets, firstName, lastName) fmt.Println(\"#########################\") fmt.Printf(\"Sending ticket: \\n %v to email address %v\\n\", ticket, email) fmt.Println(\"#########################\") } ","date":"2022-01-20","objectID":"/pratics/:2:16","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"1.16. 동기 sync 함수를 사용한다. package main import ( \"fmt\" \"sync\" \"time\" ) const conferenceTickets int = 50 var conferenceName string = \"Go Conference\" var remainingTickets uint = 50 var bookings = make([]UserData, 0) type UserData struct { firstName string lastName string email string numberOfTickets uint } var wg = sync.WaitGroup{} func main() { greetUsers() //for { firstName, lastName, email, userTickets := getUserInput() isValidName, isValidEmail, isValidTicketNumber := validateUserInput(firstName, lastName, email, userTickets) if isValidName \u0026\u0026 isValidEmail \u0026\u0026 isValidTicketNumber { bookTicket(userTickets, firstName, lastName, email) wg.Add(1) go sendTicket(userTickets, firstName, lastName, email) firstNames := getFirstNames() fmt.Printf(\"The first names of bookins are: %v\\n\", firstNames) if remainingTickets == 0 { fmt.Println(\"Our conference is booked out. Come back next year.\") //break } } else { if !isValidName { fmt.Println(\"first name or last name you entered is too short\") } if !isValidEmail { fmt.Println(\"email address you entered doesn't contain @ sign\") } if !isValidTicketNumber { fmt.Println(\"number of tickets you entered is invalid\") } } wg.Wait() //} } func greetUsers() { fmt.Printf(\"Welcome to %v booking application\\n\", conferenceName) fmt.Printf(\"We have total of %v tickets and %v are still available\\n\", conferenceTickets, remainingTickets) fmt.Println(\"Get your tickets here to attend\") } func getFirstNames() []string { firstNames := []string{} for _, booking := range bookings { firstNames = append(firstNames, booking.firstName) } return firstNames } func getUserInput() (string, string, string, uint) { var firstName string var lastName string var email string var userTickets uint fmt.Println(\"Enter Your firstName: \") fmt.Scan(\u0026firstName) fmt.Println(\"Enter Your lastName: \") fmt.Scan(\u0026lastName) fmt.Println(\"Enter Your email: \") fmt.Scan(\u0026email) fmt.Println(\"Enter number of tickets: \") fmt.Scan(\u0026userTickets) return firstName, lastName, email, userTickets } func bookTicket(userTickets uint, firstName string, lastName string, email string) { remainingTickets = remainingTickets - userTickets var userData = UserData{ firstName: firstName, lastName: lastName, email: email, numberOfTickets: userTickets, } bookings = append(bookings, userData) fmt.Printf(\"List of bookings is %v\\n\", bookings) fmt.Printf(\"Thank you %v %v for booking %v tickets. You will receive a confirmation email at %v\\n\", firstName, lastName, userTickets, email) fmt.Printf(\"%v tickets remaining for %v\\n\", remainingTickets, conferenceName) } func sendTicket(userTickets uint, firstName string, lastName string, email string) { time.Sleep(10 * time.Second) var ticket = fmt.Sprintf(\"%v tickets for %v %v\", userTickets, firstName, lastName) fmt.Println(\"#########################\") fmt.Printf(\"Sending ticket: \\n %v to email address %v\\n\", ticket, email) fmt.Println(\"#########################\") wg.Done() } 참고동영상: TechWorld with Nana ","date":"2022-01-20","objectID":"/pratics/:2:17","tags":["practice","dk","dokyung"],"title":"The Documentation Practics","uri":"/pratics/"},{"categories":["Documentation"],"content":"owasp top 10","date":"2022-01-20","objectID":"/owsap/","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"OWASP TOP10을 정리해보자. owasp top10 출처:OWASP Name 설명 A01 Broken Access Control Moving up from the fifth position, 94% of applications were tested for some form of broken access control with the average incidence rate of 3.81%, and has the most occurrences in the contributed dataset with over 318k. Notable Common Weakness Enumerations (CWEs) included are CWE-200: Exposure of Sensitive Information to an Unauthorized Actor, CWE-201: Exposure of Sensitive Information Through Sent Data, and CWE-352: Cross-Site Request Forgery. A02 Cryptographic Failures Shifting up one position to #2, previously known as Sensitive Data Exposure, which is more of a broad symptom rather than a root cause, the focus is on failures related to cryptography (or lack thereof). Which often lead to exposure of sensitive data. Notable Common Weakness Enumerations (CWEs) included are CWE-259: Use of Hard-coded Password, CWE-327: Broken or Risky Crypto Algorithm, and CWE-331 Insufficient Entropy . A03 Injection Injection slides down to the third position. 94% of the applications were tested for some form of injection with a max incidence rate of 19%, an average incidence rate of 3%, and 274k occurances. Notable Common Weakness Enumerations (CWEs) included are CWE-79: Cross-site Scripting, CWE-89: SQL Injection, and CWE-73: External Control of File Name or Path. A04 Insecure Design A new category for 2021 focuses on risks related to design and architectural flaws, with a call for more use of threat modeling, secure design patterns, and reference architectures. As a community we need to move beyond “shift-left” in the coding space to pre-code activities that are critical for the principles of Secure by Design. Notable Common Weakness Enumerations (CWEs) include CWE-209: Generation of Error Message Containing Sensitive Information, CWE-256: Unprotected Storage of Credentials, CWE-501: Trust Boundary Violation, and CWE-522: Insufficiently Protected Credentials. A05 Security Misconfiguration Moving up from #6 in the previous edition, 90% of applications were tested for some form of misconfiguration, with an average incidence rate of 4.%, and over 208k occurences of a Common Weakness Enumeration (CWE) in this risk category. With more shifts into highly configurable software, it’s not surprising to see this category move up. Notable CWEs included are CWE-16 Configuration and CWE-611 Improper Restriction of XML External Entity Reference. A06 Vulnerable and Outdated Components It was #2 from the Top 10 community survey but also had enough data to make the Top 10 via data. Vulnerable Components are a known issue that we struggle to test and assess risk and is the only category to not have any Common Weakness Enumerations (CWEs) mapped to the included CWEs, so a default exploits/impact weight of 5.0 is used. Notable CWEs included are CWE-1104: Use of Unmaintained Third-Party Components and the two CWEs from Top 10 2013 and 2017. A07 Identification and Authentication Failures Previously known as Broken Authentication, this category slid down from the second position and now includes Common Weakness Enumerations (CWEs) related to identification failures. Notable CWEs included are CWE-297: Improper Validation of Certificate with Host Mismatch, CWE-287: Improper Authentication, and CWE-384: Session Fixation A08 Software and Data Integrity Failures A new category for 2021 focuses on making assumptions related to software updates, critical data, and CI/CD pipelines without verifying integrity. One of the highest weighted impacts from Common Vulnerability and Exposures/Common Vulnerability Scoring System (CVE/CVSS) data. Notable Common Weakness Enumerations (CWEs) include CWE-829: Inclusion of Functionality from Untrusted Control Sphere, CWE-494: Download of Code Without Integrity Check, and CWE-502: Deserialization of Untrusted Data. A09 Security Logging and Monitoring Failures Security logging and monitoring came from the Top 10 community survey (#3), up slightly from the tenth p","date":"2022-01-20","objectID":"/owsap/:0:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A01 Broken Access Control CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 34 55.97% 3.81% 6.92 5.93 94.55% 47.72% 318,487 19,013 설명 액세스 제어는 사용자가 의도한 권한을 벗어나 행동할 수 없도록 정책을 시행합니다. 실패는 일반적으로 모든 데이터의 무단 정보 공개, 수정 또는 파괴로 이어지거나 사용자의 한계를 벗어난 비즈니스 기능을 수행하게 됩니다. 일반적인 액세스 제어 취약점은 다음과 같습니다. 특정 기능, 역할 또는 사용자에게만 액세스 권한을 부여해야 하지만 누구나 사용할 수 있는 최소 권한 또는 기본 거부 원칙 위반입니다. URL(매개변수 변조 또는 강제 탐색), 내부 애플리케이션 상태 또는 HTML 페이지를 수정하거나 API 요청을 수정하는 공격 도구를 사용하여 액세스 제어 검사를 우회합니다. 고유 식별자를 제공하여 다른 사람의 계정 보기 또는 편집 허용(안전하지 않은 직접 개체 참조) POST, PUT 및 DELETE에 대한 액세스 제어가 누락된 API 액세스. 특권 상승. 로그인하지 않고 사용자로 활동하거나 사용자로 로그인하면 관리자로 활동합니다. JSON 웹 토큰(JWT) 액세스 제어 토큰 재생 또는 변조, 권한 상승을 위해 조작된 쿠키 또는 숨겨진 필드, JWT 무효화 남용과 같은 메타데이터 조작. CORS 구성이 잘못되면 승인되지 않은/신뢰할 수 없는 출처의 API 액세스가 허용됩니다. 인증되지 않은 사용자로 인증된 페이지를 강제로 탐색하거나 표준 사용자로 권한 있는 페이지를 탐색합니다. ","date":"2022-01-20","objectID":"/owsap/:1:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A02 Cryptographic Failures CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 29 46.44% 4.49% 7.29 6.81 79.33% 34.85% 233,788 3,075 설명 첫 번째는 전송 중인 데이터와 저장되지 않은 데이터의 보호 요구 사항을 결정하는 것입니다. 예를 들어 암호, 신용 카드 번호, 건강 기록, 개인 정보 및 비즈니스 비밀은 주로 해당 데이터가 개인 정보 보호법(예: EU의 일반 데이터 보호 규정(GDPR) 또는 규정(예: 금융 데이터 보호)에 해당하는 경우 추가 보호가 필요합니다. PCI 데이터 보안 표준(PCI DSS)과 같은. 이러한 모든 데이터의 경우: 데이터가 일반 텍스트로 전송됩니까? 이는 STARTTLS와 같은 TLS 업그레이드도 사용하는 HTTP, SMTP, FTP와 같은 프로토콜과 관련이 있습니다. 외부 인터넷 트래픽은 위험합니다. 로드 밸런서, 웹 서버 또는 백엔드 시스템 간의 모든 내부 트래픽을 확인합니다. 기본적으로 또는 이전 코드에서 사용되는 오래되거나 약한 암호화 알고리즘이나 프로토콜이 있습니까? 기본 암호화 키가 사용 중이거나 약한 암호화 키가 생성 또는 재사용되거나 적절한 키 관리 또는 순환이 누락되었습니까? 암호화 키는 소스 코드 저장소에 체크인됩니까? 암호화가 시행되지 않습니까? 예를 들어 HTTP 헤더(브라우저) 보안 지시문이나 헤더가 누락되었습니까? 수신한 서버 인증서와 신뢰 체인이 제대로 검증되었습니까? 초기화 벡터가 무시, 재사용 또는 생성된 암호화 작동 모드에 대해 충분히 안전하지 않습니까? ECB와 같은 안전하지 않은 작동 모드가 사용 중입니까? 인증된 암호화가 더 적절할 때 암호화가 사용됩니까? 암호 기반 키 파생 기능이 없는 경우 암호가 암호 키로 사용됩니까? 암호화 요구 사항을 충족하도록 설계되지 않은 암호화 목적에 무작위성이 사용됩니까? 올바른 기능을 선택하더라도 개발자가 시드해야 합니까? 그렇지 않은 경우 개발자가 엔트로피/예측 불가능성이 부족한 시드로 내장된 강력한 시딩 기능을 덮어썼습니까? MD5 또는 SHA1과 같은 더 이상 사용되지 않는 해시 함수가 사용 중이거나 암호화 해시 함수가 필요할 때 비암호화 해시 함수가 사용됩니까? PKCS 번호 1 v1.5와 같은 더 이상 사용되지 않는 암호화 패딩 방법이 사용 중입니까? 예를 들어 패딩 오라클 공격의 형태로 암호화 오류 메시지 또는 부채널 정보가 악용될 수 있습니까? ","date":"2022-01-20","objectID":"/owsap/:2:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A03 Injection CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 33 19.09% 3.37% 7.25 7.15 94.04% 47.90% 274,228 32,078 설명 애플리케이션은 다음과 같은 경우 공격에 취약합니다. 사용자 제공 데이터는 애플리케이션에서 검증, 필터링 또는 삭제되지 않습니다. 컨텍스트 인식 이스케이프가 없는 동적 쿼리 또는 매개변수화되지 않은 호출은 인터프리터에서 직접 사용됩니다. 적대적인 데이터는 ORM(객체 관계형 매핑) 검색 매개변수 내에서 사용되어 중요한 추가 레코드를 추출합니다. 적대적인 데이터를 직접 사용하거나 연결합니다. SQL 또는 명령은 동적 쿼리, 명령 또는 저장 프로시저의 구조 및 악성 데이터를 포함합니다. 더 일반적인 주입에는 SQL, NoSQL, OS 명령, ORM(Object Relational Mapping), LDAP 및 EL(Expression Language) 또는 OGNL(Object Graph Navigation Library) 주입이 있습니다. 개념은 모든 통역사 간에 동일합니다. 소스 코드 검토는 애플리케이션이 주입에 취약한지 감지하는 가장 좋은 방법입니다. 모든 매개변수, 헤더, URL, 쿠키, JSON, SOAP 및 XML 데이터 입력에 대한 자동 테스트를 적극 권장합니다. 조직은 CI/CD 파이프라인에 정적(SAST), 동적(DAST) 및 대화형(IAST) 애플리케이션 보안 테스트 도구를 포함하여 프로덕션 배포 전에 도입된 주입 결함을 식별할 수 있습니다. ","date":"2022-01-20","objectID":"/owsap/:3:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A04 Insecure Design CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 40 24.19% 3.00% 6.46 6.78 77.25% 42.51% 262,407 2,691 설명 안전하지 않은 설계는 “누락되거나 비효율적인 제어 설계\"로 표현되는 다양한 약점을 나타내는 광범위한 범주입니다. 안전하지 않은 디자인은 다른 모든 상위 10개 위험 범주의 원인이 아닙니다. 안전하지 않은 설계와 안전하지 않은 구현에는 차이가 있습니다. 우리는 근본 원인과 해결 방법이 다르기 때문에 설계 결함과 구현 결함을 구별합니다. 보안 설계에는 여전히 악용될 수 있는 취약점으로 이어지는 구현 결함이 있을 수 있습니다. 안전하지 않은 설계는 정의상 특정 공격을 방어하기 위해 필요한 보안 제어가 생성되지 않았기 때문에 완벽한 구현으로 수정할 수 없습니다. 안전하지 않은 설계에 기여하는 요인 중 하나는 개발 중인 소프트웨어 또는 시스템에 내재된 비즈니스 위험 프로파일링의 부족입니다. ","date":"2022-01-20","objectID":"/owsap/:4:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A05 Security Misconfiguration CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 20 19.84% 4.51% 8.12 6.56 89.58% 44.84% 208,387 789 설명 애플리케이션이 다음과 같은 경우 취약할 수 있습니다. 애플리케이션 스택의 모든 부분에서 적절한 보안 강화가 누락되었거나 클라우드 서비스에 대한 권한이 부적절하게 구성되었습니다. 불필요한 기능이 활성화되거나 설치됩니다(예: 불필요한 포트, 서비스, 페이지, 계정 또는 권한). 기본 계정과 해당 암호는 여전히 활성화되어 있으며 변경되지 않습니다. 오류 처리는 스택 추적 또는 기타 지나치게 유익한 오류 메시지를 사용자에게 보여줍니다. 업그레이드된 시스템의 경우 최신 보안 기능이 비활성화되거나 안전하게 구성되지 않습니다. 애플리케이션 서버, 애플리케이션 프레임워크(예: Struts, Spring, ASP.NET), 라이브러리, 데이터베이스 등의 보안 설정이 보안 값으로 설정되어 있지 않습니다. 서버가 보안 헤더 또는 지시문을 보내지 않거나 보안 값으로 설정되지 않았습니다. 소프트웨어가 오래되었거나 취약합니다( A06:2021-Vulnerable and Outdated Components 참조). 일관되고 반복 가능한 애플리케이션 보안 구성 프로세스가 없으면 시스템이 더 큰 위험에 노출됩니다. ","date":"2022-01-20","objectID":"/owsap/:5:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A06 Vulnerable and Outdated Components CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 3 27.96% 8.77% 51.78% 22.47% 5.00 5.00 30,457 0 설명 당신은 취약할 가능성이 있습니다: 사용하는 모든 구성 요소(클라이언트 측 및 서버 측 모두)의 버전을 모르는 경우. 여기에는 직접 사용하는 구성 요소와 중첩된 종속성이 포함됩니다. 소프트웨어가 취약하거나 지원되지 않거나 오래된 경우. 여기에는 OS, 웹/애플리케이션 서버, 데이터베이스 관리 시스템(DBMS), 애플리케이션, API 및 모든 구성 요소, 런타임 환경 및 라이브러리가 포함됩니다. 취약점을 정기적으로 스캔하지 않고 사용하는 구성 요소와 관련된 보안 게시판을 구독하지 않는 경우. 위험 기반의 적시에 기본 플랫폼, 프레임워크 및 종속성을 수정하거나 업그레이드하지 않는 경우. 이는 패치가 변경 통제 하에 있는 월별 또는 분기별 작업인 환경에서 일반적으로 발생하여 조직이 수정된 취약점에 며칠 또는 몇 달 동안 불필요하게 노출될 수 있습니다. 소프트웨어 개발자가 업데이트, 업그레이드 또는 패치된 라이브러리의 호환성을 테스트하지 않는 경우. 구성 요소의 구성을 보호하지 않는 경우(A05:2021-Security Misconfiguration 참조). ","date":"2022-01-20","objectID":"/owsap/:6:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A07 Identification and Authentication Failures CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 22 14.84% 2.55% 7.40 6.50 79.51% 45.72% 132,195 3,897 설명 사용자의 신원 확인, 인증 및 세션 관리는 인증 관련 공격으로부터 보호하는 데 중요합니다. 애플리케이션이 다음과 같은 경우 인증 취약점이 있을 수 있습니다. 공격자가 유효한 사용자 이름 및 암호 목록을 가지고 있는 경우 자격 증명 스터핑과 같은 자동화된 공격을 허용합니다. 무차별 대입 또는 기타 자동화된 공격을 허용합니다. “Password1” 또는 “admin/admin\"과 같은 기본 암호, 취약하거나 잘 알려진 암호를 허용합니다. 안전할 수 없는 “지식 기반 답변\"과 같은 취약하거나 비효율적인 자격 증명 복구 및 비밀번호 찾기 프로세스를 사용합니다. 일반 텍스트, 암호화되거나 약하게 해시된 암호 데이터 저장소를 사용합니다( A02:2021-Cryptographic Failures 참조). 다단계 인증이 없거나 비효율적입니다. URL의 세션 식별자를 노출합니다. 로그인 성공 후 세션 식별자를 재사용합니다. 세션 ID를 올바르게 무효화하지 않습니다. 사용자 세션 또는 인증 토큰(주로 SSO(Single Sign-On) 토큰)은 로그아웃 또는 비활성 기간 동안 제대로 무효화되지 않습니다. ","date":"2022-01-20","objectID":"/owsap/:7:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A08 Software and Data Integrity Failures CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 10 16.67% 2.05% 6.94 7.94 75.04% 45.35% 47,972 1,152 설명 소프트웨어 및 데이터 무결성 오류는 무결성 위반으로부터 보호하지 않는 코드 및 인프라와 관련됩니다. 이에 대한 예는 애플리케이션이 신뢰할 수 없는 소스, 저장소 및 CDN(콘텐츠 전달 네트워크)의 플러그인, 라이브러리 또는 모듈에 의존하는 경우입니다. 안전하지 않은 CI/CD 파이프라인은 무단 액세스, 악성 코드 또는 시스템 손상의 가능성을 유발할 수 있습니다. 마지막으로, 많은 응용 프로그램에는 이제 충분한 무결성 확인 없이 업데이트가 다운로드되고 이전에 신뢰했던 응용 프로그램에 적용되는 자동 업데이트 기능이 포함됩니다. 공격자는 잠재적으로 자신의 업데이트를 업로드하여 배포하고 모든 설치에서 실행할 수 있습니다. 또 다른 예는 공격자가 보고 수정할 수 있는 구조로 개체 또는 데이터가 인코딩되거나 직렬화되어 안전하지 않은 역직렬화에 취약한 경우입니다. ","date":"2022-01-20","objectID":"/owsap/:8:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A09 Security Logging and Monitoring Failures CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 4 19.23% 6.51% 6.87 4.99 53.67% 39.97% 53,615 242 설명 OWASP Top 10 2021로 돌아가서 이 카테고리는 활성 침해를 감지, 에스컬레이션 및 대응하는 데 도움이 됩니다. 로깅 및 모니터링 없이는 위반을 감지할 수 없습니다. 불충분한 로깅, 탐지, 모니터링 및 활성 응답은 다음과 같은 경우에 발생합니다. 로그인, 로그인 실패 및 가치가 높은 트랜잭션과 같은 감사 가능한 이벤트는 기록되지 않습니다. 경고 및 오류는 아니오, 부적절하거나 불명확한 로그 메시지를 생성합니다. 애플리케이션 및 API 로그는 의심스러운 활동에 대해 모니터링되지 않습니다. 로그는 로컬에만 저장됩니다. 적절한 경고 임계값 및 응답 에스컬레이션 프로세스가 적절하지 않거나 효과적이지 않습니다. DAST(동적 애플리케이션 보안 테스트) 도구(예: OWASP ZAP)에 의한 침투 테스트 및 스캔은 경고를 트리거하지 않습니다. 애플리케이션은 실시간 또는 거의 실시간으로 활성 공격을 감지, 확대 또는 경고할 수 없습니다. 사용자나 공격자가 볼 수 있는 로깅 및 경고 이벤트를 통해 정보 유출에 취약합니다( A01:2021-Broken Access Control 참조). ","date":"2022-01-20","objectID":"/owsap/:9:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"A10 Server-Side Request Forgery (SSRF) CWEs Mapped Max Incidence Rate Avg Incidence Rate Avg Weighted Exploit Avg Weighted Impact Max Coverage Avg Coverage Total Occurrences Total CVEs 1 2.72% 2.72% 8.28 6.72 67.72% 67.72% 9,503 385 설명 SSRF 결함은 웹 애플리케이션이 사용자가 제공한 URL의 유효성을 검사하지 않고 원격 리소스를 가져올 때마다 발생합니다. 이를 통해 공격자는 방화벽, VPN 또는 다른 유형의 네트워크 ACL(액세스 제어 목록)에 의해 보호되는 경우에도 응용 프로그램이 조작된 요청을 예기치 않은 대상으로 보내도록 강제할 수 있습니다. 최신 웹 애플리케이션이 최종 사용자에게 편리한 기능을 제공함에 따라 URL 가져오기가 일반적인 시나리오가 되었습니다. 그 결과 SSRF의 발병률이 증가하고 있습니다. 또한 클라우드 서비스와 아키텍처의 복잡성으로 인해 SSRF의 심각도가 높아지고 있습니다. 자료출처: OWASP ","date":"2022-01-20","objectID":"/owsap/:10:0","tags":["OWASP","dk","dokyung"],"title":"The Documentation OWASP","uri":"/owsap/"},{"categories":["Documentation"],"content":"VASN RAID-5, RAID-6","date":"2022-01-20","objectID":"/vsan/","tags":["vsan","Erasure Coding RAID-5/RAID-6","dk","dokyung"],"title":"The Documentation vSAN","uri":"/vsan/"},{"categories":["Documentation"],"content":"1. VSAN 개념 VSAN SDS(Software Defined Storage), ESXi 호스트의 로컬 스토리지 리소스를 가상화하고, 서비스 품질 요구 사항에 따라 분할하여 가상 시스템 및 애플리케이션에 할당 할 수 있는 스토리지 풀로 변환. 참고 문헌 ","date":"2022-01-20","objectID":"/vsan/:1:0","tags":["vsan","Erasure Coding RAID-5/RAID-6","dk","dokyung"],"title":"The Documentation vSAN","uri":"/vsan/"},{"categories":["Documentation"],"content":"2. VSAN Erasure Coding RAID-5 및 RAID-6 Erasure Coding이란 일부 조각이 누락된 경우에도 원본데이터를 복구할 수 있는 방식으로 데이터를 조각으로 인코딩하고 분할하는 모든 체계를 나타내는 일반적인 용어. 여기서 더 자세한 내용을 확인 할 수 있을 거 같다. 참고 문헌 Reed-Solomon 알고리즘에 기반 한다. (알고리즘의 따라 계산 하는 방법이 달라진다.) 참고로 MINIO에서도 동일한 Erasure Coding이라는 용어가 나오지만, 위에 설명 했듯이 그냥 일반적인 용어이며, VSAN과 MiniO에서 사용하는 부분의 차이도 명확하다 우선 VSAN은 Host가 두개 실패 했을 경우까지만 제공을 한다. 하지만 MiniO에서 사용하는 Erasure Coding은 두개의 실패뿐만 아니라 더 많이 실패해도 복구를 할 수 있다는 점이다. VSAN 정책 RAID-5 물리 장애 실패 = 1 내결함성 = 용량 RAID-1은 물리 용량을 x2 이지만 RAID-5의 용량은 x1.33만 필요 VSAN구성시 클러스터는 최소 4개의 노드가 필요 스트라이프당 3개의 데이터 + 1개의 패리티 조각이 있는 RAID-5 스트라이핑 RAID-6 물리 장애 실패 = 2 내결함성 = 용량 RAID-1은 물리 용량을 x3 이지만 RAID-6의 용량은 x1.5만 필요 VSAN구성시 클러스터는 최소 6개의 노드가 필요 스트라이프당 4개의 데이터 + 1개의 패리티(P) + 1개의 RS(Q) 신드롬 조각이 있는 RAID-6 스트라이핑 내결함성? 시스템을 구성하는 부품의 일부에서 결함(fault) 또는 고장(failure)이 발생하여도 정상적 혹은 부분적으로 기능을 수행할 수 있는 시스템이다. 참고 문헌 vSAN 데이터 보호 공간 허용된 실패 RAID-1 RAID-5/6 Erasure Coding save vs Mirroring 필요한 최소 호스트 총 용량 요구 사항 필요한 최소 호스트 총 용량 요구 사항 FTT=0 3 x1 N/A N/A N/A FTT=1 3 x2 4 x1.33 33% 감소 FTT=2 5 x3 6 x1.5 50% 감소 FTT=3 7 x4 N/A N/A N/A ","date":"2022-01-20","objectID":"/vsan/:2:0","tags":["vsan","Erasure Coding RAID-5/RAID-6","dk","dokyung"],"title":"The Documentation vSAN","uri":"/vsan/"},{"categories":["Documentation"],"content":"3. 성능 vs 용량 앞서 설명한 대로 Erasure Coding을 사용한다는 것은 성능보다 용량을 우선시 한다는 것이다. Erasure Coding은 Software 기반으로 처리를 하기 때문에 CPU를 많이 사용하게 되어 있으며 RAID1보다는 복잡한 계산식으로 인해서 상황에 맞게 선택을 하는 것이 중요하다. ","date":"2022-01-20","objectID":"/vsan/:3:0","tags":["vsan","Erasure Coding RAID-5/RAID-6","dk","dokyung"],"title":"The Documentation vSAN","uri":"/vsan/"},{"categories":["Documentation"],"content":"4. 고려사항 RAID-5/RAID-6의 경우 vSAN에서 Streched Storage에는 지원 하지 않음, 또한 All-Flash 구성으로 제공해야함. VSAN Hardware CALCULATE VSAN Hardware CALCULATE 참고 문헌 참고 문헌 참고 문헌 ","date":"2022-01-20","objectID":"/vsan/:4:0","tags":["vsan","Erasure Coding RAID-5/RAID-6","dk","dokyung"],"title":"The Documentation vSAN","uri":"/vsan/"},{"categories":["Documentation"],"content":"Ansible Configuration for NSXT","date":"2022-01-19","objectID":"/nsxt/","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"1. Ansible을 통한 NSXT 구성 NSXT를 Ansible로 구성. Ansible의 대한 보충 설명을 할 수 있으면 추후에 진행 하기로 하고 우선 설정의 대해서 설명을 먼저 하겠다. 먼저.. 이 부분을 블로그로 쓰는게 맞을까라는 고민을 좀 했다. 이유는 우선 Ansible로 구성이 되어 있기 때문에 코드가 들어가 있다. 그래서 NSXT Ansible Module을 다운로드 받고 나서 추가 된 부분을 Git Hub에 올려 두었다. ","date":"2022-01-19","objectID":"/nsxt/:1:0","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2. 설치 ","date":"2022-01-19","objectID":"/nsxt/:2:0","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2.1. 파이썬 설치 yum update -y rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm yum install epel-release yum-utils python3-pip -y pip3 install --upgrade pip setuptools ansible pyvmomi pyvim requests ruamel.yaml dnf install libnsl -y 파이썬을 설치 후 버전을 변경 하고 싶으면 아래 처럼 구성 sudo rm /usr/bin/python sudo update-alternatives --install /usr/bin/python python /usr/bin/python(TAB) ## 설치되어 있는 버전을 확인 할 수 있다. 파이썬 버전 확인 sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.8 2 sudo update-alternatives --config python 파이썬 버전 선택 ","date":"2022-01-19","objectID":"/nsxt/:2:1","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2.2. OVF Tool OVF Tool 다운로드 OVF Tool 다운로드 링크 원하는 버전으로 다운로드 받는다. ","date":"2022-01-19","objectID":"/nsxt/:2:2","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2.3. NSXT Ansible Download 제공한 버전은 3.1 기준으로 구성을 하였다. 원하는 버전으로 다운로드 받는다. ","date":"2022-01-19","objectID":"/nsxt/:2:3","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2.4. Ansible 실행 ovftool -v 에러가 나오면 dnf install libnsl 설치 ansible-playbook 01_deploy_first_node.yml -vvv 에러 발생시 dnf install libnsl 설치. ","date":"2022-01-19","objectID":"/nsxt/:2:4","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"2.5. Github 다운로드 NSXT Ansible Module을 압축을 해제 하면 되는대, 그 부분을 별도로 github에 올려두었다. 추가적으로 vars라는 폴더와, 00 ~ 10 번 , answerfile,yml이 추가 된 것을 확인 할 수 있다. 코드를 하나 하나 설명을 하기에는.. 좀 벅찬 느낌이 든다. git clone https://github.com/huntedhappy/nsxt3.1 ","date":"2022-01-19","objectID":"/nsxt/:2:5","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NSXT and Ansible","uri":"/nsxt/"},{"categories":["Documentation"],"content":"LDAPS(AD) 관련","date":"2022-01-16","objectID":"/ldaps/","tags":["Ldpas","dk","dokyung"],"title":"The Documentation LDAPS(AD)","uri":"/ldaps/"},{"categories":["Documentation"],"content":"1. LDAP Over SSL 대략 설명 보다. 이미지를 많이 첨부 LDAPS를 구성 할 경우 IP로 ldapsearch를 하면 실패하는 경우를 보게 될 것이다. FQDN인증서를 확인 하면 FQDN만 구성이 되어 있는 것을 알 수 있다. 그래서 SAN으로 구성하면서 IP도 포함되게 구성을 해보자. Ldaps 인증서 확인 ","date":"2022-01-16","objectID":"/ldaps/:1:0","tags":["Ldpas","dk","dokyung"],"title":"The Documentation LDAPS(AD)","uri":"/ldaps/"},{"categories":["Documentation"],"content":"1.1. certreq.req 설정 ## 아래 IP로 ldapsearch를 하게 되면 에러가 난다. 만약 IP로도 가능하게 하려면 인증서 변경이 필요하다. ldapsearch -x -H ldaps://10.253.241.2 -D 'cn=administrator,cn=users,dc=tkg,dc=io' -w 'Passw0rd' -b 'ou=tanzu,dc=tkg,dc=io' ## 에러 ldap_sasl_bind(SIMPLE): Can't contact LDAP server (-1) Ldaps Falied 윈도우에서 certlm.msc 실행 certlm.msc certlm.msc \u003e 개인용 \u003e 모든 작업 \u003e 고급 작업 \u003e 사용자 지정 요청 만들기 certlm #1 certlm #2 certlm #3 certlm #4 certlm #5 certlm #6 certlm #7 certlm #8 certlm #9 certlm #10 certlm #11 인증서 생성 certreq -submit -sttrib \"CertificateTemplate:webserver\" certreq.req certreq.cer certlm #12 ","date":"2022-01-16","objectID":"/ldaps/:1:1","tags":["Ldpas","dk","dokyung"],"title":"The Documentation LDAPS(AD)","uri":"/ldaps/"},{"categories":["Documentation"],"content":"1.2. 인증서 등록 certlm.msc \u003e 개인용 \u003e 인증서 \u003e 모든 작업 \u003e 가져오기 인증서 등록#1 인증서 등록#2 인증서 등록#3 인증서 등록#4 인증서 등록#5 인증서 등록#6 등록 확인 인증서 등록 확인#1 인증서 등록 확인#2 Ldapsearch를 IP로 했을 때 성공 하는 것을 확인 할 수 있다. 인증서 등록 확인#2 ","date":"2022-01-16","objectID":"/ldaps/:1:2","tags":["Ldpas","dk","dokyung"],"title":"The Documentation LDAPS(AD)","uri":"/ldaps/"},{"categories":["Documentation"],"content":"NAP Built","date":"2022-01-13","objectID":"/nap/","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"1. NAP? NAP(NGINX App Protect)은 WAF and DoS Protection을 제공 한다. NGINX Plus에서 제공을 하며 컨테이너 환경에서 App을 보호 하기 위한 솔루션이다. 대부분 컨테이너 환경에서 Ingress를 NGINX로 많이 사용 할 것이다. NGINX Plus 라이센스를 구매 하면 NAP을 사용 할 수 있다. ","date":"2022-01-13","objectID":"/nap/:1:0","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"2. 사전 구성 Docker v18.09+ GNU Make git Helm3 OpenSSL https://github.com/OpenVPN/easy-rsa.git apt install git \\ make \\ make-guile ","date":"2022-01-13","objectID":"/nap/:2:0","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"3. 설치 Namespace 생성 후 easy-rsa git clone kubectl create ns ingress-nginx git clone https://github.com/OpenVPN/easy-rsa.git easy rsa 인증서 생성 (인증서 생성은 반드시 이렇게 하지 않아도 된다.) cd easy-rsa/easyrsa3/ ./easyrsa init-pki ./easyrsa build-ca easy rsa 구성#1 ./easyrsa gen-req wildcard ./easyrsa sign-req server wildcard easy rsa 구성#2 openssl rsa -in /var/tmp/easy-rsa/easyrsa3/pki/private/wildcard.key -out /var/tmp/easy-rsa/easyrsa3/pki/private/wildcard-unencrypted.key ## 인증서 Secret 생성 kubectl create -n ingress-nginx secret tls wildcard-tls --key /var/tmp/easy-rsa/easyrsa3/pki/private/wildcard-unencrypted.key --cert /var/tmp/easy-rsa/easyrsa3/pki/issued/wildcard.crt ## 안지워도된다. rm /var/tmp/easy-rsa/easyrsa3/pki/private/wildcard-unencrypted.key ssl 인증서 secret 생성 secret 인증서 확인 kubectl get secret -n ingress-nginx secret 확인 cd /var/tmp ## nap을 위한 yaml파일 다운로드 git clone https://github.com/nginxinc/kubernetes-ingress/ ## nap을 위한 helm repo 주소 추가 helm repo add nginx-stable https://helm.nginx.com/stable git clone cd /var/tmp/kubernetes-ingress/deployments/helm-chart git checkout v1.11.3 git checkout 라이센스 발급 30일 trial을 받을 수 있다. 가입 필요  NAP 발급 링크 미리 Harbor에 ingress-nginx project 생성이 되어 있어야함 REGISTRY=\u003cregistry IP or FQDN\u003e NS=\u003cyour namespace\u003e REGISTRY=10.253.110.4 NS=ingress-nginx Harbor 확인 발급 받은 라이센스를 복사 mkdir -p /var/tmp/kubernetes-ingress cp nginx-repo.key nginx-repo.crt /var/tmp/kubernetes-ingress 라이센스 복사 cd /var/tmp/kubernetes-ingress make debian-image-nap-plus PREFIX=$REGISTRY/$NS/nginx-plus-ingress TARGET=container Docker Pull docker image 확인 docker images Docker Image 확인 Harbor에 Push docker image tag 18a49497920c $REGISTRY/$NS/nginx-plus-ingress:1.11.3 docker login \u003charbor IP\u003e make push PREFIX=$REGISTRY/$NS/nginx-plus-ingress Harbor push#1 Harbor push#2 Harbor push#3 Helm Value 복사 (원본을 건드리지 않게 하기 위해 별도로 복사를 한다.) cd deployments/helm-chart cp values-plus.yaml values-plus.yaml.orig Helm Value 복사 복사한 파일을 열어 보면 아래와 같이 되어 있는대 수정해줘야 하는 부분을 수정 해준다. ## vi 편집 vi values-plus.yaml ## 내용 (imagePullSecretName의 경우 Harbor에 Public으로 만들었으면 상관 없음) controller: replicaCount: 1 nginxplus: true image: repository: 10.253.106.46/ingress-nginx/nginx-plus-ingress tag: \"1.11.3\" service: externalTrafficPolicy: Cluster appprotect: ## Enable the App Protect module in the Ingress Controller. enable: true wildcardTLS: ## The base64-encoded TLS certificate for every Ingress host that has TLS enabled but no secret specified. ## If the parameter is not set, for such Ingress hosts NGINX will break any attempt to establish a TLS connection. cert: \"\" ## The base64-encoded TLS key for every Ingress host that has TLS enabled but no secret specified. ## If the parameter is not set, for such Ingress hosts NGINX will break any attempt to establish a TLS connection. key: \"\" ## The secret with a TLS certificate and key for every Ingress host that has TLS enabled but no secret specified. ## The value must follow the following format: `\u003cnamespace\u003e/\u003cname\u003e`. ## Used as an alternative to specifying a certificate and key using `controller.wildcardTLS.cert` and `controller.wildcardTLS.key` parameters. ## Format: \u003cnamespace\u003e/\u003csecret_name\u003e secret: ingress-nginx/wildcard-tls serviceAccount: ## The name of the service account of the Ingress controller pods. Used for RBAC. ## Autogenerated if not set or set to \"\". name: ingress-nginx ## The name of the secret containing docker registry credentials. ## Secret must exist in the same namespace as the helm release. imagePullSecretName: \"regcred\" PSP를 혀용하기 위해 해당 파일을 실행 해준다. kubectl apply -f https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/nginx-psp.yaml psp 허용 HELM 실행 kubectl create secret generic regcred --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson -n ingress-nginx helm install nap nginx-stable/nginx-ingress -f values-plus.yaml -n ingress-nginx helm으로 nap 설치 설치 확인 watch -n 1 kubectl -n ingress-nginx get all 설치 확인 L4 확인 ","date":"2022-01-13","objectID":"/nap/:3:0","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"4. WAF WAF TEST를 위해 웹 구성 ","date":"2022-01-13","objectID":"/nap/:4:0","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"4.1. TEST WEB 구성 TEST를 하기 위해 제공하는 Manifest를 다운로드 한다. wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/cafe-rbac.yaml wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/cafe.yaml wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/cafe-ingress.yaml kubectl apply -f cafe-rbac.yaml -n test kubectl apply -f cafe.yaml -n test vi cafe-ingress.yaml (인증서 만들었던 Domain으로 변경) Domain을 변경 한다. Domain 변경 후 실행 kubectl apply -f cafe-ingress.yaml -n test Ingress 확인. POSTMAN으로 접속이 되는지 확인 POSTMAN 확인. ","date":"2022-01-13","objectID":"/nap/:4:1","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"4.2. WAF 구성 ELK를 구성 하기 위해 syslog pod를 구성한다. wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/syslog-rbac.yaml wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/syslog.yaml kubectl apply -f syslog-rbac.yaml -n ingress-nginx kubectl apply -f syslog.yaml -n ingress-nginx wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/ap-apple-uds.yaml wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/ap-dataguard-alarm-policy.yaml wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/ap-logconf.yaml kubectl apply -f ap-apple-uds.yaml -n test kubeclt apply -f ap-dataguard-alarm-policy.yaml -n test kubectl apply -f ap-logconf.yaml -n test ingress에 annotation을 설정 wget https://raw.githubusercontent.com/f5devcentral/f5-bd-tanzu-tkg-nginxplus/main/cafe-ingress-ap.yaml kubectl get pod -n ingress-nginx -o wide ## vi 편집 실행 vi cafe-ingress-ap.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: cafe-ingress annotations: appprotect.f5.com/app-protect-policy: \"test/dataguard-alarm\" appprotect.f5.com/app-protect-enable: \"True\" appprotect.f5.com/app-protect-security-log-enable: \"True\" appprotect.f5.com/app-protect-security-log: \"test/logconf\" appprotect.f5.com/app-protect-security-log-destination: \"syslog:server=SYSLOG:514\" spec: ingressClassName: nginx # use only with k8s version \u003e= 1.18.0 tls: - hosts: - cafe.vcf.local ## 인증서와 동일한 도메인으로 변경 rules: - host: cafe.vcf.local ## 인증서와 동일한 도메인으로 변경 http: paths: - path: /tea backend: serviceName: tea-svc servicePort: 80 - path: /coffee backend: serviceName: coffee-svc servicePort: 80 ## syslog service ip SYSLOG_IP=10.101.182.155 vi cafe-ingress-ap.yaml sed -e \"s/SYSLOG/$SYSLOG_IP/\" cafe-ingress-ap.yaml \u003e cafe-ingress-ap-syslog.yaml kubectl apply -n test -f cafe-ingress-ap-syslog.yaml kubectl get ingress -n test ## 아래 명령어로 annotation을 확인 할 수 있다. kubrectl get ingress cafe-ingress -n test -o yaml Annotation 확인. kubectl get pod -n ingress-nginx LOG 확인 kubectl -n ingress-nginx exec -it syslog-65d847447d-ghbvq -- tail -f /var/log/messages syslog pod 확인. PostMan 요청 후 REJECT확인. Pod 로그 확인시 Attack 확인. ","date":"2022-01-13","objectID":"/nap/:4:2","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"5. ELK LOG를 좀 가시적이게 표현하기 위해 ELK를 구성 ","date":"2022-01-13","objectID":"/nap/:5:0","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"5.1. Elastic 연동 logstach를 5144로 구성한 이유는 Logstach 구성시 514가 well-known 포트라 5144로 변경 logstash yaml 파일,  logstash_test.yaml. logstach 확인. ingress 설정에서 syslog server 를 logstash cluster IP로 설정 annotation에서 logstach로 syslog IP 변경 . 해당 파일을 다운로드 받는다. Dashboard 링크 mkdir /var/tmp/kibana cd /var/tmp/kibana cp false-positives-dashboards.ndjson overview-dashboard.ndjson /var/tmp/kibana 파일 다운로드 대쉬보드 업로드 cd /var/tmp KIBANA_URL= {FQDN or IP} KIBANA_URL=http://kibana.vcf.local:5601 jq -s . kibana/overview-dashboard.ndjson | jq '{\"objects\": . }' | \\ curl -k --location --request POST \"$KIBANA_URL/api/kibana/dashboards/import\" \\ --header 'kbn-xsrf: true' \\ --header 'Content-Type: text/plain' -d @- \\ | jq jq -s . kibana/false-positives-dashboards.ndjson | jq '{\"objects\": . }' | \\ curl -k --location --request POST \"$KIBANA_URL/api/kibana/dashboards/import\" \\ --header 'kbn-xsrf: true' \\ --header 'Content-Type: text/plain' -d @- \\ | jq kibana접속 후 index patterns에 waf-logs-* 확인 kibana 확인 kibana 대시보드 확인 참고문헌 ","date":"2022-01-13","objectID":"/nap/:5:1","tags":["nap","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation NAP","uri":"/nap/"},{"categories":["Documentation"],"content":"Opshift Install Guide","date":"2022-01-13","objectID":"/openshift/","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"1. Openshift 오픈시프트 오리진(OpenShift Origin)은 오픈시프트 온라인, 오픈시프트 데디케이티드, 오픈시프트 컨테이너 플랫폼에 사용되는 업스트림 커뮤니티 프로젝트이다. 도커 컨테이너 패키징 코어와 쿠버네티스 컨테이너 클러스터 관리 기능을 기반에 두고 개발된 오리진은 애플리케이션 수명 관리 기능과 데브옵스 도구를 통해 증강된다. 오리진은 오픈 소스 애플리케이션 컨테이너 플랫폼을 제공한다. 오리진 프로젝트의 모든 소스 코드는 깃허브에서 아파치 라이선스 (버전 2.0)을 통해 이용이 가능하다.[4] 오픈시프트 온라인(OpenShift Online)은 레드햇의 퍼블릭 클라우드 애플리케이션 개발 및 호스팅 서비스이다. 온라인은 오리진 프로젝트 소스 코드의 버전 2를 제공하였으며, 아파치 라이선스 버전 2.0 하에서 이용이 가능하다.[5] 온라인은 리소스 할당 기어(gear) 하에서 구동되는 미리 빌드된 카트리지를 통해 다양한 언어, 프레임워크 데이터베이스를 지원한다. 개발자들은 오픈시프트 카트리지 API를 통해 다른 언어, 데이터베이스, 구성 요소를 추가할 수 있다.[6] 오픈시프트 3의 선호로 사용이 권장되지 않는다(deprecated). 오픈시프트 데디케이티드(OpenShift Dedicated)는 레드햇의 매니지드 프라이빗 클러스터 기능으로, 도커가 제공하는 애플리케이션 컨테이너의 코어를 기반으로 빌드되며 레드햇 엔터프라이즈 리눅스의 토대 위에 쿠버네티스가 제공하는 오케스트레이션 및 관리가 포함되어 있다. 아마존 웹 서비스(AWS)와 구글 클라우드 플랫폼(GCP) 마켓플레이스를 통해 이용이 가능하다. 오픈시프트 컨테이너 플랫폼(OpenShift Container Platform)은 레드햇의 사내(on-premises) 프라이빗 PaaS 제품으로, 도커가 제공하는 애플리케이션 컨테이너의 코어를 기반으로 빌드되며 레드햇 엔터프라이즈 리눅스의 토대 위에 쿠버네티스가 제공하는 오케스트레이션 및 관리가 포함되어 있다. 참고문헌 Openshift ","date":"2022-01-13","objectID":"/openshift/:1:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"2. 사전구성 ","date":"2022-01-13","objectID":"/openshift/:2:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"2.1. DNS 구성 Common Component Name Cluster Name BaseDomain A Record api openshift vcf.local 10.253.107.254 api-int openshift vcf.local 10.253.107.254 * apps openshift vcf.local 10.253.107.10 console-openshift-console apps openshift vcf.local 10.253.107.10 * apps openshift vcf.local 10.253.107.10 bootstrap openshift vcf.local 10.253.107.10 master0 openshift vcf.local 10.253.107.11 master1 openshift vcf.local 10.253.107.12 master2 openshift vcf.local 10.253.107.13 worker1 openshift vcf.local 10.253.107.14 worker2 openshift vcf.local 10.253.107.15 worker3 openshift vcf.local 10.253.107.16 DNS 구성 ","date":"2022-01-13","objectID":"/openshift/:2:1","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"2.2. DHCP 구성 DHCP 구성 - NSXT로 구성을 하였다. DHCP 구성#1 DHCP 구성#2 DHCP 구성#3 ","date":"2022-01-13","objectID":"/openshift/:2:2","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"3. 파일 다운로드 OC를 다운로드 하기 위해 redhat에 가입 하고 Login 필요 OC 다운로드 링크 OC 다운로드 링크 OC 다운로드#1 PullSecret을 저장해 둔다. OC 다운로드#2 압축을 해제 하고 환경변수를 별도로 구성하지 않게 /usr/local/bin 에다가 copy를 한다. tar -xzvf openshift-client-linux.tar.gz tar -xzvf openshift-install-linux.tar.gz mv oc kubectl openshift-install /usr/local/bin oc version openshift-install version sshkeygen을 생성 한다. ssh-keygen -t ed25519 -N '' -f ~/.ssh/id_rsa eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_rsa ","date":"2022-01-13","objectID":"/openshift/:3:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"4. vCenter SSH thumbprint 얻기 openssl s_client -servername vcsa01.vcf.local -connect vcsa01.vcf.local:443 | openssl x509 | tee ca.crt cp ca.crt /usr/local/share/ca-certificates/ update-ca-certificates ","date":"2022-01-13","objectID":"/openshift/:4:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"5. Temp Image  RHCOS Download Link wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/latest/rhcos-vmware.x86_64.ova 또는 아래와 같이 GUI에서 다운로드 받을 수 있다. RHCOS OVA GUI 다운로드#2 RHCOS OVA GUI 다운로드#2 RHCOS OVA GUI 다운로드#3 RHCOS OVA GUI 다운로드#4 ","date":"2022-01-13","objectID":"/openshift/:5:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"5.1. Temp 구성 vSphere Temp Upload#1 vSphere Temp Upload#2 vSphere Temp Upload#3 vSphere Temp Upload#4 vSphere Temp Upload#5 vSphere Temp Upload#6 vSphere Temp Upload#7 vSphere Temp Upload#8 ","date":"2022-01-13","objectID":"/openshift/:5:1","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"6. OC Install SSL 구성 install-config.yaml 참조 apiVersion: v1 baseDomain: vcf.local compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: {} replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: {} replicas: 3 metadata: creationTimestamp: null name: ocp networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 10.0.0.0/16 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: vsphere: apiVIP: 10.253.107.254 cluster: OBCLUSTER datacenter: OBDC defaultDatastore: vsanDatastore ingressVIP: 10.253.107.253 network: LS-MGMT-10.253.107.x password: Openbase!234 username: administrator@vsphere.local vCenter: vcsa01.vcf.local fips: false publish: External pullSecret: 'full secret 넣어줘야함' sshKey: | ssh-ed25519 AAAAC ocp 실행 mkdir ocp cp install-config.yaml ocp openshift-install create install-config --dir=ocp Manifest 변경 openshift-install create manifests --dir ocp cd ~/ocp/openshift rm -rf 99_openshift-cluster-api_master-* rm -rf 99_openshift-cluster-api_worker-machineset-0.yaml cd ~/ocp/manifests/ vi cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false ### true \u003e false change policy: name: \"\" status: {} ignition 실행 openshift-install create ignition-configs --dir ocp L4 VIP로 설정 필요 cat \u003c\u003c EOF | tee append-bootstrap.ign { \"ignition\": { \"config\": { \"merge\": [ { \"source\": \"http://10.253.107.254:8080/bootstrap.ign\" ## L4 VIP로 변경 } ] }, \"version\": \"3.1.0\" } } EOF BASE64 실행 base64 -w0 append-bootstrap.ign \u003e append-bootstrap.64 base64 -w0 master.ign \u003e master.64 base64 -w0 worker.ign \u003e worker.64 웹 구성에서 파일을 다운로드 할 수 있게 file 폴더 구성 후 ign을 복사 한다. mkdir -p /usr/share/nginx/html/files cp *.ign /usr/share/nginx/html/files/ chmod 644 /usr/share/nginx/html/files/*.ign ","date":"2022-01-13","objectID":"/openshift/:6:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"7. NGINX NGINX 설치 apt update \u0026\u0026 apt upgrade -y make로 설치 apt install gcc libpcre3 libpcre3-dev libssl-dev make -y mkdir -p /var/tmp/src \u0026\u0026 cd /var/tmp/src wget http://nginx.org/download/nginx-1.20.2.tar.gz tar -xzf nginx-1.20.2.tar.gz cd nginx-1.20.2 ./configure --prefix=/var/www/html --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --with-pcre --lock-path=/var/lock/nginx.lock --pid-path=/var/run/nginx.pid --with-http_ssl_module --with-http_image_filter_module=dynamic --modules-path=/etc/nginx/modules --with-http_v2_module --with-stream=dynamic --with-http_addition_module --with-http_mp4_module --with-stream make make install vi /lib/systemd/system/nginx.service [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/var/run/nginx.pid ExecStartPre=/usr/sbin/nginx -t ExecStart=/usr/sbin/nginx ExecReload=/usr/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target nginx.conf 설정 ## 아래 내용 추가 vi etc/nginx/nginx.conf worker_processes auto; ## 추가 error_log /var/log/nginx/error.log; ## 추가 pid /run/nginx.pid; ## 추가 include /etc/nginx/stream.conf.d/*.conf; ## 추가 http { include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server { listen 8081; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } include /etc/nginx/conf.d/*.conf; ## 추가 } 웹 구성 cat \u003c\u003c EOF | tee /etc/nginx/conf.d/openshift.conf server { listen 8080; server_name localhost; location / { root /usr/share/nginx/html/files; autoindex on; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } EOF nginx 테스트 및 실행 nginx -t systemctl restart nginx systemctl enable nginx NGINX 확인 ","date":"2022-01-13","objectID":"/openshift/:7:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"8. Temp 이미지로 VM 구성 Temp를 활용하여 bootstrap , master 3개 , worker 3개를 배포한다. Image 구성#1 Image 구성#2 Image 구성#3 base64로 변경한 값을 여기서 넣어 준다. bootstrap : cat append-bootstrap.64 , 마스터 : cat master.64 , Worker : cat worker.64 의 값을 넣어 주면 됨 Image 구성#4 ","date":"2022-01-13","objectID":"/openshift/:8:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"9. L4 구성 L4장비가 없을 경우 / L4장비가 있을 경우를 생각해서 NGINX도 포함 시킴 ","date":"2022-01-13","objectID":"/openshift/:9:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"9.1. NSXT L4 구성 ## 설명 ocp_8080 : jumphost (nginx에서 파일을 땡기기 위해 구성) ## 배포 완료 후 bootstrap은 삭제 해도 됨 ocp-master-and-boot-machine-22623 : bootstrap 및 master ocp_master-and-boot-api-6443: bootstrap 및 master ## Openshift는 Route를사용 하기 때문에 설정 ocp_443, ocp_80 : master 및 worker L4 구성#1 L4 구성#2 ","date":"2022-01-13","objectID":"/openshift/:9:1","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"9.2. NGINX L4 구성 ## LB 설정 cat \u003c\u003c EOF | tee /etc/nginx/stream.conf.d/lb.conf stream{ upstream ocp_k8s_api { #round-robin; server 10.253.107.10:6443; #bootstrap server 10.253.107.11:6443; #master1 server 10.253.107.12:6443; #master2 server 10.253.107.13:6443; #master3 } server { listen 6443; proxy_pass ocp_k8s_api; } upstream ocp_m_config { #round-robin; server 10.253.107.10:22623; #bootstrap server 10.253.107.11:22623; #master1 server 10.253.107.12:22623; #master2 server 10.253.107.13:22623; #master3 } server { listen 22623; proxy_pass ocp_m_config; } upstream ocp_http { #round-robin; server 10.253.107.11:80; #master1 server 10.253.107.12:80; #master2 server 10.253.107.13:80; #master3 server 10.253.107.14:80; #worker1 server 10.253.107.15:80; #worker2 server 10.253.107.16:80; #worker3 } server{ listen 80; proxy_pass ocp_http; } upstream ocp_https { #round-robin; server 10.253.107.11:443; #master1 server 10.253.107.12:443; #master2 server 10.253.107.13:443; #master3 server 10.253.107.14:443; #worker1 server 10.253.107.15:443; #worker2 server 10.253.107.16:443; #worker3 } server{ listen 443; proxy_pass ocp_https; } } EOF ## nginx restart nginx -t systemctl restart nginx ","date":"2022-01-13","objectID":"/openshift/:9:2","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"10. 완료 후 확인 export KUBECONFIG=\u003cinstallation_directory\u003e/auth/kubeconfig 예시 export KUBECONFIG=~/ocp/auth/kubeconfig oc whoami oc get clusterversion 완료#1 oc get clusteroperators oc describe clusterversion oc get clusterversion -o jsonpath='{.items[0].spec}{\"\\n\"}' 완료#2 완료#3 ","date":"2022-01-13","objectID":"/openshift/:10:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"11. New Worker Node Add ## PENDING 확인 kubectl get csr ## PENDING 확인 후 적용 oc adm certificate approve csr-bghmp csr-hd9x8 csr-hlngb oc adm certificate approve csr-gpgv9 csr-n6lqm csr-zfws6 worker Node 추가 ","date":"2022-01-13","objectID":"/openshift/:11:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"12. 계정 Local 또는 AD을 통해 계정을 관리 할 수 있다. ","date":"2022-01-13","objectID":"/openshift/:12:0","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"12.1. Local 계정 생성 아래는 htpasswd를 사용 하는 방법의 대해서 구성 한다. ## 우분투 apt install apache2-utils -y # 유저 정보 # htpasswd -Bbc htpasswd {username} '{password}' $ htpasswd -Bbc htpasswd my1208 'Passw0rd' cat htpasswd oc --user=admin create secret generic htpasswd \\ --from-file=htpasswd -n openshift-config oc get secret -n openshift-config 유저 추가 secret 확인 secret 추가 cat \u003c\u003c EOF | tee oauth-config.yaml # oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: Local Password mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpasswd EOF oc replace -f oauth-config.yaml ## shows current user oc whoami ## shows cluster web console URL oc whoami --show-console ## shows cluster API URL oc whoami --show-server ## shows current OAuth token oc whoami --show-token GUI 접속#1 GUI 접속#2 ","date":"2022-01-13","objectID":"/openshift/:12:1","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"12.2. AD 연동 LDPAS를 구성하기 위한 configmap 생성 oc create configmap ca-config-map --from-file=ca.crt=/path/to/ca -n openshift-config 만약 LDAPS로 구성을 하지 않았으면 insecure: true, ca 항목을 삭제, url을 ldap으로 변경을 해주면 된다. oc apply -n openshift-config -f - \u003c\u003c EOF apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldapidp mappingMethod: claim type: LDAP ldap: attributes: id: - dn email: - mail name: - sAMAccountName preferredUsername: - sAMAccountName bindDN: cn=administrator,cn=users,dc=tkg,dc=io bindPassword: name: ldap-secret ca: name: ca-config-map insecure: false url: \"ldaps://tanzu-dns.tkg.io/ou=tanzu,dc=tkg,dc=io?sAMAccountName\" EOF GUI 접속 ","date":"2022-01-13","objectID":"/openshift/:12:2","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"12.3. 수동 GROUP-SYNC vi ldapsync.yaml # LDAP is case insensitive, but OpenShift is not, so all LDAP parameters have been converted to lower case as per https://access.redhat.com/solutions/3232051 (under \"Case Sensitivity\") kind: LDAPSyncConfig apiVersion: v1 url: ldaps://tanzu-dns.tkg.io:636 insecure: false ca: \"/data/cert/ldapserver.pem\" ### ldaps 인증서의 실제 위치 / 파일 bindDN: cn=administrator,cn=users,dc=tkg,dc=io bindPassword: \"Password\" rfc2307: groupsQuery: baseDN: \"ou=tanzu,dc=tkg,dc=io\" scope: sub filter: (objectClass=group) derefAliases: never timeout: 0 pageSize: 0 groupUIDAttribute: dn groupNameAttributes: [ cn ] groupMembershipAttributes: [ member ] usersQuery: basedn: \"ou=tanzu,dc=tkg,dc=io\" scope: sub derefAliases: never pageSize: 0 userUIDAttribute: dn userNameAttributes: [ cn ] tolerateMemberNotFoundErrors: true tolerateMemberOutOfScopeErrors: true ## 적용전 제대로 받아오는지 확인을 한다. oc adm groups sync --sync-config=ldapsync.yaml ## 확인이 끝나면 적용한다. oc adm groups sync --sync-config=ldapsync.yaml --confirm ## 권한 설정 oc adm policy add-cluster-role-to-group cluster-admin tkg ## 권한 삭제 oc adm policy remove-cluster-role-from-group cluster-admin tkg GROUP 확인 및 적용 ","date":"2022-01-13","objectID":"/openshift/:12:3","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"12.4 CronJob Group-Sync 위에서 LDAP을 연동 하였다면 cm , secret 이 생성 된 것을 확인 할 수 있다. ## password 이름 확인 password=`oc get secret -n openshift-authentication | grep v4-0-config-user-idp-0 | awk '{print $1}'` oc get secret -n openshift-authentication $password -o jsonpath={.data} ## 인증서 이름 확인 ca=`oc get cm -n openshift-authentication | grep v4-0-config-user | awk '{print $1}'` oc get cm -n openshift-authentication $ca -o jsonpath={.items[0].data} | awk '{print $1}' 그리고 계정 및 권한 설정을 해준다. cat \u003c\u003c EOF | tee ldap-sync-sa-clusterrole.yaml kind: ServiceAccount apiVersion: v1 metadata: name: ldap-group-syncer namespace: openshift-authentication labels: app: cronjob-ldap-group-sync --- kind: ServiceAccount apiVersion: v1 metadata: name: ldap-group-syncer namespace: openshift-authentication labels: app: cronjob-ldap-group-sync root@ubuntu:/var/tmp/oc/ldaps# root@ubuntu:/var/tmp/oc/ldaps# cat clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ldap-group-syncer labels: app: cronjob-ldap-group-sync rules: - apiGroups: - '' - user.openshift.io resources: - groups verbs: - get - list - create - update --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ldap-group-syncer labels: app: cronjob-ldap-group-sync subjects: - kind: ServiceAccount name: ldap-group-syncer namespace: openshift-authentication roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ldap-group-syncer EOF LDAP의 자동 Sync를 구성하기 위해 config-map 및 job 설정 witelist / balcklist의 경우 ldapsearch에서 distinguishedName: CN=test test,OU=tanzu,DC=tkg,DC=io 이부분의 이름으로 넣어야함. , 만약 별도로 witelist / blacklist가 필요 없으면 제거 해도 된다. cat \u003c\u003c EOF | tee ldap-sync-cm-cron.yaml kind: ConfigMap apiVersion: v1 metadata: name: ldap-group-syncer namespace: openshift-authentication labels: app: cronjob-ldap-group-sync data: ldap-group-sync.yaml: | kind: LDAPSyncConfig apiVersion: v1 url: ldaps://tanzu-dns.tkg.io bindDN: cn=administrator,cn=users,dc=tkg,dc=io bindPassword: file: \"/etc/secrets/bindPassword\" ## 위에서 설명한 secret, cronjob에서 voluemount 후 적용 insecure: false ca: \"/ldap-sync/ca/ca.crt\" ## 위에서 설명한 configmap, cronjob에서 voluemount 후 적용 rfc2307: groupsQuery: baseDN: \"ou=tanzu,dc=tkg,dc=io\" scope: sub derefAliases: never filter: (objectclass=group) groupUIDAttribute: dn groupNameAttributes: [ cn ] groupMembershipAttributes: [ member ] usersQuery: baseDN: \"ou=tanzu,dc=tkg,dc=io\" scope: sub derefAliases: never pageSize: 0 userUIDAttribute: dn userNameAttributes: [ sAMAccountName ] tolerateMemberNotFoundErrors: true tolerateMemberOutOfScopeErrors: true --- kind: ConfigMap apiVersion: v1 metadata: name: ldap-group-syncer-whitelist namespace: openshift-authentication labels: app: cronjob-ldap-group-sync data: whitelist.txt: | CN=kim dokyung,OU=tanzu,DC=tkg,DC=io --- kind: ConfigMap apiVersion: v1 metadata: name: ldap-group-syncer-blacklist namespace: openshift-authentication labels: app: cronjob-ldap-group-sync data: blacklist.txt: | CN=tkg,OU=tanzu,DC=tkg,DC=io --- kind: CronJob apiVersion: batch/v1beta1 metadata: name: ldap-group-syncer namespace: openshift-authentication labels: app: cronjob-ldap-group-sync spec: schedule: \"*/1 * * * *\" concurrencyPolicy: Forbid successfulJobsHistoryLimit: 5 failedJobsHistoryLimit: 5 jobTemplate: metadata: labels: app: cronjob-ldap-group-sync spec: backoffLimit: 0 template: metadata: labels: app: cronjob-ldap-group-sync spec: containers: - name: ldap-group-sync image: \"registry.redhat.io/openshift4/ose-cli:v4.7\" command: - \"/bin/bash\" - \"-c\" - oc adm groups sync --whitelist=/etc/whitelist/whitelist.txt --blacklist=/etc/blacklist/blacklist.txt --sync-config=/etc/config/ldap-group-sync.yaml --confirm volumeMounts: - mountPath: \"/etc/blacklist\" name: \"ldap-sync-volume-blacklist\" - mountPath: \"/etc/whitelist\" name: \"ldap-sync-volume-whitelist\" - mountPath: \"/etc/config\" name: \"ldap-sync-volume\" - mountPath: \"/etc/secrets\" name: \"ldap-bind-password\" - mountPath: \"/ldap-sync/ca\" name: \"ldap-sync-ca\" volumes: - name: \"","date":"2022-01-13","objectID":"/openshift/:12:4","tags":["openshift","k8s","devops","dk","dokyung","ldap연동","openshift ldap"],"title":"The Documentation Openshift","uri":"/openshift/"},{"categories":["Documentation"],"content":"EKS set up","date":"2022-01-13","objectID":"/eks/","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"1. EKS(Amazon Elastic Kubernetes Service)? Kubernetes를 실행하는 데 사용할 수 있는 관리형 서비스이다. 마스터노드, 워커노드를 설치, 작동 및 유지관리를 할 필요 없는 솔루션이다. 우선 문서가 너무 잘되어 있지만 그래도 나름 예전에 EKS를 테스트 했었던 지라 내가 테스트 했던 방법을 공유하고자 한다. 참고 문헌 AWS ","date":"2022-01-13","objectID":"/eks/:1:0","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"1.1. 사전설치 우선 OS의 따라 설치해야 되는 것들이 조금 있다. (chocolatey를 설치한 이유는 chocolatey를 통해 eksctl을 배포 하기 위함) 설치 Tool Chocolatey 설치 링크 chocolatey Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) eksctl 설치 링크 AWS eksctl chocolatey install -y eksctl aws-iam-authenticator kubectl 설치 링크 AWS kubectl 링크를 보게 되면 버전별로 다운로드 받을 수 있게 되어 있다. curl -o kubectl.exe https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/windows/amd64/kubectl.exe awscli 설치링크 AWS awscli msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi aws --version ","date":"2022-01-13","objectID":"/eks/:1:1","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"1.2. VPC 생성 CloudFormation을 통해 vpc를 자동으로 생성 해준다. 아래 링크는 현재 최신상태인지 확인이 가능. eks vcp cloudformation 최신 상태 확인 링크 AWS eks vcp eks vcp cloudformation 최신 파일 링크 AWS eks vcp download 위에 받은 파일로 cloudformation을 설정 해주면 된다. cloud formation 설정#1 cloud formation 설정#2 원하는 N/W으로 수정 후 그 이후에는 그냥 NEXT 후 CREATE만 하면 VPC가 구성이 된다. cloud formation 설정#3 cloud formation 설정#4 cloud formation 설정#5 완료가 되면 보는 바와 같이 VPC가 생성이 된 것을 확인 할 수 있다. VPC 확인 Route Table 확인 IGW 확인 NAT GW 확인 ","date":"2022-01-13","objectID":"/eks/:2:0","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"1.3. private key등록 aws에서 ec2에 접속하기 위한 security 키를 sshkeygen으로 publickey 생성 public key 생성#1 public key 생성#2 Notepad로 복사 ","date":"2022-01-13","objectID":"/eks/:2:1","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"1.4. EKS 실행 EKS 실행 그리고 아래와 같이 명령어를 넣어 준다. eksctl create cluster \\ --name aws-eks \\ --region us-east-2 \\ --nodegroup-name aws-node \\ --node-type t3.medium \\ --nodes 3 \\ --nodes-min 1 \\ --nodes-max 4 \\ --ssh-access \\ --ssh-public-key huntedhappy.pub \\ --with-oidc \\ --managed 구성 EKS 연동 및 확인 aws eks update-kubeconfig –name aws-eks Cloud Formation이 자동으로 돌아간다 Cloud Formation IAM에서 OIDC가 생성 되었는지 확인 한다. OIDC는 CSI를 EFS로 구성 할 떄 필요 하다. OIDC 생성 확인 ","date":"2022-01-13","objectID":"/eks/:2:2","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"2. CSI EFS구성 (작성예정) ","date":"2022-01-13","objectID":"/eks/:3:0","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"IAM 참고로 IAM은 아래와 같이 구성 하였다. IAM ","date":"2022-01-13","objectID":"/eks/:4:0","tags":["EKS","k8s","dk","dokyung"],"title":"The Documentation EKS","uri":"/eks/"},{"categories":["Documentation"],"content":"Service Type의 차이","date":"2022-01-08","objectID":"/k8s/","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"아래 내용을 작성 하는 이유 같이 일했던 동료가 다른 곳으로 회사를 이직 하고 나서 컨테이너를 해야 되는거 같았다. 같이 일을 했기 때문에 L4 / L7을 잘 했던 친구 였다. 그런대 뜬금없이 L4를 연동 하면 어떻게 컨테이너로 트래픽을 전달하냐라고 물어본적이 있었다. 그래서 혹시 모르는 사람을 위해서 간략하게 적어 내려 간다. ","date":"2022-01-08","objectID":"/k8s/:0:0","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"1. Service Type 컨테이너를 하기 위해서는 우선 Deploy , STS등으로 Pod를 생성한다. 그럼 일반적으로 테스트를 하기 위해서 아래와 같이 명령어를 칠 것이다. kubectl create deploy nginx –image=nginx -n nginx 그러면 deploy를 통해 pod가 생성 된 것을 확인 할 수 있다. 그리고 나서 서비스를 연동 할 것이다. 그럼 아래와 같은 명령어를 칠 것이다. kubectl expose deploy nginx –port=80 –target-port=80 –type=ClusterIP -n nginx ClusterIP는 그럼 아래와 같은 정보를 보게 될 것이다. svc ClusterIP상태 NodePort는 아래처럼 정보를 보게 된다. kubectl expose deploy nginx –port=80 –target-port=80 –type=NodePort -n nginx svc NodePort상태 그럼 보는바와 같이 차이가 좀 있는 것을 알 수 있다. NodePort를 하게 될 경우 아래와 같이 30000대의 Port를 확인 할 수 있을 것이다. ","date":"2022-01-08","objectID":"/k8s/:1:0","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"L4연동 후 NodePort 그럼 만약에 L4장비와 연결을 하게 되면 어떻게 보이게 될까? 아래 그림으로 한번 확인을 해보자 kubectl expose deploy nginx –port=80 –target-port=80 –type=LoadBalancer -n nginx svc LoadBalancer상태#1 svc LoadBalancer상태#2 위에 AVI에 설정된 서버가 실제적인 K8S의 Node인것을 확인 할 수 있다. svc LoadBalancer상태#3 보는 것과 같이 실제 노드IP에 32676번 (30000번대의 포트를 할당 받음) 포트가 연동 되어 있는것을 확인 할 수 있다. 그럼 실제적으로 클라이언트가 접속을 하게 되면 Node:Port(32767)의 연결된 노드로 트래픽이 가게 되고 노드는 트래픽이 들어오면 해당하는 EndPoint로 접속 하게 되는 것이다. Source \u003e L4 \u003e Node:Port \u003e Container 그럼 아래와 같이 SVC와 그의 대한 Endpoint가 어떻게 연결되어 있는지 알 수 있다. svc LoadBalancer상태#4 아래와 같이 접속이 되는 것을 확인 할 수 있다. svc LoadBalancer상태#5 ","date":"2022-01-08","objectID":"/k8s/:2:0","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"L4연동 후 ClusterIP 그런대 여기서 의문점이 들 것이다. 그럼 ClusterIP는 지원이 안되는건가? 그건 연동하는 L4에서 지원을 하면 가능 하다. 그럼 어떻게 나오는지 한번 확인 해보자. 특별하게 설정 할 것은 없고, AVI를 NodePort를 지원하는 것에서 ClusterIP로 변경 후 상태를 확인 해보면 실제 Pod의 IP로 맵핑이 된 것을 확인 할 수 있다. svc LoadBalancer상태#6 ","date":"2022-01-08","objectID":"/k8s/:3:0","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"결과 위에서 보듯이 NodePort와 ClusterIP의 차이점을 확인해 볼 수 있을 거 같다. ","date":"2022-01-08","objectID":"/k8s/:4:0","tags":["tanzu","k8s","devops","avi","ako","dk","dokyung"],"title":"The Documentation K8s","uri":"/k8s/"},{"categories":["Documentation"],"content":"Tanzu Explain","date":"2022-01-06","objectID":"/minio/","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"MiniO는 Minimal Object Storage를 의미 하며, 오픈소스 형태로 제공 하는 오브젝트 스토리지이다. Object Storage? 오브젝트 스토리지는 이미지, 오디오 파일, 스프레드시트 또는 바이너리 실행 코드등 문서 처럼 한줄 한문자 바꾸는 형식이 아니라 하나의 파일이 다 바뀌는 것으로 이해하면 쉬울 거 같다. MiniO는 3가지 형태로 도구를 제공 한다. MiniO Console / Server - UI / Cloud Storage Server를 구성 할 수 있다. MiniO Client(mc,admin) - Minio Server, AWS S3, GCS등등 연결하여 파일 업로드 및 관리등을 할 수 있다. MiniO gateway - Minio는 스토리지 Gateway도 지원한다. 예를들어 miniO게이트웨이를 구성 하면 가상머신등에서 Nas를 통해 파일 또는 파일공유 지점으로 miniO안 객체에 엑세스 할 수 있다. MiniO는 2가지의 배포 형식을 제공 한다. 독립형 배포: 단일 스토리지 볼륨 또는 폴더가 있는 단일 MiniO 서버 분산 배포: 모든 서버에 총 스토리지 볼륨이 4개 이상인 하나 이상의 MiniO서버 위의 내용은 Kasten 설치 후 백업 스토리지를 MiniO로 구성하기 위해 간단하게 MiniO가 무엇인지의 대한 설명 아래 내용은 Kasten으로 백업 스토리지를 MiniO로 구성시 Erasure Coding 및 Immutability가 되어야 하는대 이 부분의 대해서 설명 하고자 한다. ","date":"2022-01-06","objectID":"/minio/:0:0","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"1. Erasure Coding Erasure Coding은 클러스터의 여러개 디스크 드라이브 중 몇개가 손실이 발생 하더라도 자동으로 복구 를 할 수 있게 해주는 데이터 중복성 및 가용성 기능이다. Erasure Coding은 RAID 또는 복제와 같은 기술보다 적은 오버헤드로 복구를 제공한다. ","date":"2022-01-06","objectID":"/minio/:1:0","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"1.1. Erasure Coding 동작 Erasure Coding은 원본 데이터를 가져와서 데이터가 필요할 때 원본 정보를 재생성하기 위해 부분 집합만 필요로 하는 방식으로 인코딩을 한다. 예를들어 개체 또는 데이터의 원래 값이 95라고 가정하고 x=9 및 y=5가 되도록 나눈다. 인코딩 프로세스는 일련의 방정식을 생성 한다. 이 경우 다음과 같은 방적식을 생성한다고 가정 합니다. x + y = 14 x - y = 4 2x + y = 23 객체를 재생성 하려면 이 세 방정식 중 두가지가 필요 하므로 디코딩 할 수 있습니다. 따라서 방정식을 풀면 x와 y에 대한 값을 얻을 수 있습니다. 3개의 방정식이 있지만 그 중 2개에서 원래 정보를 얻을 수 있기 때문에 데이터를 조각으로 나누고 인코딩하여 여러위치에 저장하는 데이터 보호 체계 입니다. 요약하자면, Erasure Code를 활용하여 데이터를 인코딩 하고, 데이터 손실시 디코딩 과정을 거쳐 원본 데이터를 복구하는 데이터 복구 기법중 하나 Decode / Encode 자세한 설명은 링크를 걸어 두도록 하겠다. Minio Erasure Coding 참고링크#1 참고링크#1 ","date":"2022-01-06","objectID":"/minio/:1:1","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"1.2. Erasure Coding vs RAID? RAID로 구성시 데이터를 다른 위치에 저장할 수 있으며 드라이브 오류로부터 보호, Erasure Coding은 데이터가 부분적으로 분할된 다음 확장되고 인코딩이 된다. 그 후 세그먼트는 여러 위치에 보관하도록 구성이 된다. RAID는 무결성 위협으로 부터 데이터 보호를 용이 하게 할 수 있으며, Erasure Coding은 스토리지 소모를 덜 할 수 있게 해준다. 상황에 따라 RAID 및 Erasure Coding 모두 적합 할 수 있다. Erasure Coding의 현재 사용 사례 중 하나는 객체 기반 클라우드 스토리지입니다. Erasure Coding은 높은 CPU 사용률을 요구하고 대기 시간이 발생하므로 애플리케이션 보관에 적합합니다. 또한 Erasure Coding은 데이터 무결성 위협으로부터 보호할 수 없기 때문에 기본 워크로드에 적합하지 않습니다. ","date":"2022-01-06","objectID":"/minio/:1:2","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"1.3. Erasure Coding의 이점 Erasure Coding은 고급 데이터 보호 및 재해 복구 방법을 제공합니다 . 저장 공간 활용도: Erasure Coding은 소비되는 공간을 줄이고 동일한 수준의 중복성을 제공하여 더 나은 저장 활용률을 제공(복사본 3개). Erasure Coding을 활용하면 최대 50% 더 많은 공간을 절약할 수 있습니다. 신뢰성 향상: 데이터 조각은 독립적인 오류 더미 로 조각화됩니다 . 이렇게 하면 종속되거나 상관된 오류가 발생하지 않습니다. 적합성: Erasure Coding은 모든 파일 크기에 사용할 수 있습니다. KiloBytes의 작은 블록 크기에서 PetaBytes의 큰 블록 크기에 이르기까지 다양합니다. Suitability: 데이터를 복구하는 데 데이터의 Suitability만 필요합니다. 원본 데이터가 필요하지 않습니다. 유연성: 시스템을 오프라인으로 전환하지 않고도 편리할 때 고장난 구성 요소를 교체할 수 있습니다. Suitablility? Suitablility란 더 큰 집합에서의 부분적인 집합. ","date":"2022-01-06","objectID":"/minio/:1:3","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"1.4. MiniO Erasure Code 계산기\u003e 계산기 LINK ","date":"2022-01-06","objectID":"/minio/:1:4","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"2. Immutability MiniO 서버는 특정 개체에 대해 WORM을 허용하거나 모든 객체에 기본 보존 모드 및 보존기간을 적용하는 객체 잠금 구성으로 버킷을 구성하여 WORM을 허용합니다. 이렇게 하면 버킷의 객체를 변경 할 수 없습니다. 즉, 버킷의 객체 잠금 구성 또는 객체 보존에 지정된 만료일 까지 버전 삭제가 허용 되지 않습니다. 객체 잠금을 사용하려면 버킷 생성시 잠금을 활성화해야 하며, 객체 잠금도 버킷의 버전 관리를 자동으로 활성화 합니다. 또는 버킷에서 생성된 객체에 적용할 기본 보존 기간 및 보존 모드를 버킷에 구성 할 수 있습니다. WORM? Read Many(WORM) ","date":"2022-01-06","objectID":"/minio/:2:0","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"2.1. 개념 Immutability 개념 객체가 법적 보존 상태에 있는 경우 해당 버전ID에 대한 법적 보존이 명시적으로 제거되지 않는 한 삭제 할 수 없다. 그렇지 않으면 DeleteObjectVersio()이 실패 한다. Compliance모드 에서는 해당 버전 ID의 보존기간이 만료될 때때가지 누구도 객체를 삭제 할 수 없다. 사용자에게 필요한 거버넌스 우회 권한이 있는 경우 Compliance모드 에서 개체의 보존 날짜를 연장 할 수 있다. 객체 잠금 구성이 버킷으로 설정되면 새 객체는 버킷 객체 잠금 구성의 보존 설정을 자동으로 상속한다. 개체를 업로드할 때 보존 헤더를 선택적으로 설정 할 수 있다. 개체에서 명시적으로 PutObjectRetention API 호출을 할 수 있다. MINIO_NTP_SERVER환경 변수는 보존하는 날짜를 시스템시간으로 설정이 필요하지 않는 경우 원격 NTP 서버를 구성 할 수 있다. 객체잠금 기능은 삭제 코드 및 분산 삭제 코드 설정에서만 사용 할 수 있다. 자세한 설명은 링크를 걸어 두도록 하겠다. Minio Immutability ","date":"2022-01-06","objectID":"/minio/:2:1","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"카스텐 설정시 MiniO로 Backup Storage 구성 링크 참조. Kasten MiniO Install ","date":"2022-01-06","objectID":"/minio/:2:2","tags":["minio","object storage","tanzu","k8s","devops","dk","dokyung","notification"],"title":"The Documentation Minio","uri":"/minio/"},{"categories":["Documentation"],"content":"Tanzu Explain","date":"2022-01-01","objectID":"/tanzu/","tags":["tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation Tanzu","uri":"/tanzu/"},{"categories":["Documentation"],"content":"1. VMware TANZU? 2019년 8월 VMware에서 처음으로 TANZU 포트 폴리오를 발표 했다. VM웨어의 대표 제품군인 ‘브이스피어(vSphere)’를 쿠버네티스 네이티브 플랫폼으로 바꾸겠다고 선언했다. 이를 위해 VM웨어는 ‘프로젝트 퍼시픽(Project Pacific)’을 진행했다. Tanzu Portfolio 처음에는 VCF(VMware Cloud Foundation)라는 솔루션을 같이 설치 하면서 배포 해야 되었던 vsphere with tanzu가 현재는 VCF를 구성하지 않아도 설치가 가능하도록 변하게 되었다. VCF(VMware Cloud Foundation)? VMware 제품군을 자동으로 설치 해주는 솔루션이다. Excel, JSON파일을 읽어 드려 vSphere, vCenter, vSAN 그리고 NSX-T를 한번에 배포 해주는 솔루션 ","date":"2022-01-01","objectID":"/tanzu/:1:0","tags":["tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation Tanzu","uri":"/tanzu/"},{"categories":["Documentation"],"content":"1.1. vsphere with tanzu? vsphere with tanzu는 말그대로 vsphere 에서 컨테이너를 바로 올리는 컨셉으로 나왔다. 용어로 TKGS라고도 불린다. Tanzu를 서비스형태로 올릴 수 있다고 해서 TKGS라고 불리며, 말그대로 vCenter에서 서비스 형태로 설치를 할 수 있기 때문이다. 하지만 vCenter에 종속이 되버리기 때문에 vCenter가 업그레이드가 되어야 K8S의 버전을 올릴수 있다. 정확히 말하면 vsphere위에 올릴 경우는 이벤트 형식으로 올리고, 프로덕션의 경우 TKC위에다가 POD를 올리는 것이 낫지 않을까 싶다. 그래서 컨셉은 아래와 같다. TKGS TKC(Tanzu Kubernetes Cluster)? TKC는 별도로 클러스터를 배포 하는 것이다. vmware에서 배포되는 Supervisor가 관리를 하게 되며, 네임스페이스에 TKC를 배포 하여 사용 할 수 있다. 자세한 TKGS의 대한 설명은 링크를 걸어 두도록 하겠다. VMware TKGS ","date":"2022-01-01","objectID":"/tanzu/:1:1","tags":["tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation Tanzu","uri":"/tanzu/"},{"categories":["Documentation"],"content":"1.2. Tanzu Kubernetes Grid? TKGM이라고 하며, 예전 Pivotal에서 PKS가 이렇게 변한 것이 아닐까 싶다. 예전 Pivotal에서는 bosh라는 관리 솔루션을 통해 PKS를 배포 하여 클러스터를 구성 하였다. 그렇다고 TKG가 기존에 없었던 솔루션은 아니다 6.7에서 이미 TKG는 있었지만 많은 사람들이 사용하지는 않았다. 그리고 Pivotal이 VMware로 인수 되면서 Tanzu로 변화하고 있는 것으로 알고 있다. TKGM은 TKGS와는 다른게 별도로 관리 클러스터를 배포해야 한다. TKGS같은 경우에는 Supervisor가 그 역할을 하였다. 그리고 TKC를 배포하는 형식이다. 자세한 TKG의 대한 설명은 링크를 걸어 두도록 하겠다. VMware TKG ","date":"2022-01-01","objectID":"/tanzu/:1:2","tags":["tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation Tanzu","uri":"/tanzu/"},{"categories":["Documentation"],"content":"1.3. Tanzu Kubernetes Integrated? 기존 Pivotal에서 Bosh로 배포한 PKS라고 보면 될거 같다. 현재는 사용하지 않을 것 같으므로 설명은 패스 하겠다. 자세한 TKGI의 대한 설명은 링크를 걸어 두도록 하겠다. VMware TKGI ","date":"2022-01-01","objectID":"/tanzu/:1:3","tags":["tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation Tanzu","uri":"/tanzu/"},{"categories":["Documentation"],"content":"Jenkins Install Guide","date":"2021-12-31","objectID":"/jenkins/","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"Jenkins? Jenkins는 아무래도 많이 사용하는 CI/CD 일것이다. 우선 컨테이너 환경에서 CI를 구성하기 위해서 Jenkins를 구성 하였고, 클러스터가 많은 환경에서도 접근을 할 수 있게 (물론 컨테이너로 구성을 해도 되나. 굳이 컨테이너로 구성할 필요성이 있나 싶어 별도의 VM으로 구성) VM형태로 설치를 하였다. CI/CD CI/CD는 애플리케이션 개발 단계를 자동화하여 애플리케이션을 보다 짧은 주기로 고객에게 제공한다. CI (Continuous Integration) CI를 통해 개발자들은 코드 변경사항을 공유 브랜치로 다시 병합하는 작업을 더욱 수월하게 자주 수행 할 수 있다. CD (Continuous Delivery || Continuous Deploy) 두용어는 상호 교환적으로 사용됨. Continuous Deliver의 경우 코드 변경 , 병합으로부터 Prodcution에 적합한 빌드를 제공하여 모든 단계에 테스트 및 릴리스를 자동화한다. Continuous Deploy는 어플리케이션을 프로덕션으로 릴리스 작업을 자동화 CICD 참고 문헌 Redhat ","date":"2021-12-31","objectID":"/jenkins/:1:0","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"1. 설치 Jenkins Install JAVA Install apt update \u0026\u0026 apt upgrade sudo apt search openjdk sudo apt install openjdk-11-jdk -y java --version Jenkins Install wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb https://pkg.jenkins.io/debian-stable binary/ \u003e /etc/apt/sources.list.d/jenkins.list' sudo apt update -y sudo apt install jenkins -y systemctl restart jenkins systemctl enable jenkins 패스워드 확인 cat /var/lib/jenkins/secrets/initialAdminPassword ","date":"2021-12-31","objectID":"/jenkins/:2:0","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"1.1. 설치 완료 접속 화면 #1 접속 화면 #2 접속 화면 #3 ","date":"2021-12-31","objectID":"/jenkins/:2:1","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"2. HTTPS 설정 내부에서 사용하기 때문에 HTTP로 구성을 해도 되나 보안상 HTTPS를 구성이 필요할 경우가 있을 수 있기 때문에 2가지 경우로 HTTPS 구성의 대해서 설명하고자 하자. 하나는 자체적으로 구성을 하는 것이고 다른 하나는 NGINX를 구성해요 HTTPS를 구성 하는 것이다. 물론 L4를 사용해서 구성 할 수 도 있으나, 굳이 L4 장비까지 사용할 이유가 없으니 간단하게 NGINX를 사용하기로 한다. ","date":"2021-12-31","objectID":"/jenkins/:3:0","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"2.1. HTTPS 구성 인증서 설치,  root.sh. Jenkins SSL 구성 인증서 생성 export domain=jenkins.tkg.io root.sh에 있는 파일 내용을 복사해서 shell 실행 . root.sh 인증서 권한 변경 chmod 700 /data/cert chmod 600 /data/cert/yourdomain.com.cert chmod 600 /data/cert/yourdomain.com.key Jenkins 파일 변경 vi /etc/default/jenkins HTTP_PORT=8080 ### ---\u003e 이부분을 찾아서 아래 부분을 채워 넣어주자. HTTP_PORT_DISABLE=-1 ### HTTP DISABLE HTTPS_CERT=/data/cert/yourdomain.com.cert ### 인증서 HTTPS_KEY=/data/cert/yourdomain.com.key ### KEY ### args 마지막 줄에 빨간 부분을 채워서 넣어준다. JENKINS_ARGS=\"--webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT--httpPort=$HTTP_PORT_DISABLE--httpsPort=$HTTP_PORT--httpsCertificate=$HTTPS_CERT--httpsPrivateKey=$HTTPS_KEY\" ### jenkins restart systemctl restart jenkins HTTPS 접속 화면 #1 ","date":"2021-12-31","objectID":"/jenkins/:3:1","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"2.2. NGINX로 HTTPS 구성 NGINX를 구성하기 위해 Jenkins 설정은 그냥 HTTP로 구성을 해도 무방하다. 여기서는 위에서 설정을 했기 때문에 별도로 설정을 변경하지 않고했기 때문에 뒷단의 Jenkins를 HTTPS로 그대로 놔둔것이라고 봐도 된다. 만약에 Jenkins Server는 HTTP로 구성을 하려면 그냥 두고 NGINX에서 Reverse Proxy 구성을 하면 된다. 용어가 나와서 헷갈릴수도 있지만. Proxy_pass 부분만 http:// 로 바꾸면 된다. NGINX PROXY 구성 NGINX 가상 서버 구성 vi /etc/nginx/sites-available/jenkins server { server_name jenkins.tkg.io; location / { proxy_redirect off; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass https://127.0.0.1:8080/; } listen 443 ssl; ssl_certificate /data/cert/yourdomain.com.crt; ssl_certificate_key /data/cert/yourdomain.com.key; ssl_client_certificate /data/cert/ca.crt; } server { if ($host = jenkins.tkg.io) { return 301 https://$host$request_uri; } # managed by Certbot listen 80; server_name jenkins.tkg.io; return 404; # managed by Certbot } symbolic link 연결 cd /etc/nginx/sites-enabled ln -s /etc/nginx/sites-available/jenkins . NGINX로 연결 후 HTTPS 접속 화면 #1 ","date":"2021-12-31","objectID":"/jenkins/:3:2","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"3. SLACK 연동 SLACK을 연동하여 메시지를 받을 수 있게 구성을 한다. ","date":"2021-12-31","objectID":"/jenkins/:4:0","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"3.1. SLCAK 설정 SLACK 접속 APP 등록 SLACK APP 추가#1 SLACK APP 추가#2 SLACK APP 추가#3 SLACK APP 추가#4 위에 내용까지 설정을 하면 Jenkins를 어떻게 설정하라고 나오는대 좀 오래 되었나보다. 요즘에 변경된 부분의 대해서 설정 하는 방법을 나열한다. ","date":"2021-12-31","objectID":"/jenkins/:4:1","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"3.2. JENKINS 설정 Jenkins Slack Plugin 설치#1 Jenkins Slack Plugin 설치#2 Jenkins Slack Plugin 설치#3 Jenkins Slack 설정#1 Jenkins Slack 설정#2 Jenkins Slack Credentials 설정 Jenkins Slack 설정 테스트 ","date":"2021-12-31","objectID":"/jenkins/:4:2","tags":["Jenkins","CI/CD","tanzu","k8s","devops","dk","dokyung","jenkins slack","slack","jenkins slack 연동","notification"],"title":"The Documentation Jenkins","uri":"/jenkins/"},{"categories":["Documentation"],"content":"Argo Install Guide","date":"2021-12-31","objectID":"/argo/","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"gitops explain ing….. ","date":"2021-12-31","objectID":"/argo/:1:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"ARGO? CI/CD에서 CD를 아르고로 선택한 이유는 인프라 변경 사항에 대한 추적이 좀 가능 하기도 하며 또한 구성 및 배포가 쉽다라고 생각 했다. 설치 환경은 Tanzu 1.4 버전으로 진행 CI/CD CI/CD는 애플리케이션 개발 단계를 자동화하여 애플리케이션을 보다 짧은 주기로 고객에게 제공한다. CI (Continuous Integration) CI를 통해 개발자들은 코드 변경사항을 공유 브랜치로 다시 병합하는 작업을 더욱 수월하게 자주 수행 할 수 있다. CD (Continuous Delivery || Continuous Deploy) 두용어는 상호 교환적으로 사용됨. Continuous Deliver의 경우 코드 변경 , 병합으로부터 Prodcution에 적합한 빌드를 제공하여 모든 단계에 테스트 및 릴리스를 자동화한다. Continuous Deploy는 어플리케이션을 프로덕션으로 릴리스 작업을 자동화 CICD 참고 문헌 Redhat ","date":"2021-12-31","objectID":"/argo/:2:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"1. Requirements helm 설치,  Helm. ","date":"2021-12-31","objectID":"/argo/:3:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"2. 환경 vSphere : 7.0 vSAN NSX : 3.2 AVI : 21.1.1 Tanzu 1.4 ","date":"2021-12-31","objectID":"/argo/:4:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"3. 설치 ARGO Install Namespace 생성 kubectl create ns argocd Helm Repo 등록 helm repo add argo https://argoproj.github.io/argo-helm helm repo update Optional : Helm 에서 value 값을 수정 하고 싶으면 별도로 다운로드 helm show values argo/argo-cd \u003e argocd.yaml Optional : HTTPS로 구성을 하려고 한다면 인증서 등록을 해준다. kubectl create secret tls argo-tls --cert=/data/cert/yourdomain.com.crt --key=/data/cert/yourdomain.com.key -n argocd 수정이 필요 없으면 바로 시작 하면 된다. helm install argocd argo/argo-cd -n argo 접속 하기 위해 Portfoward를 하자 kubectl port-forward service/argocd-server -n argo 8080:443 ID는 admin 이며, PW는 별도의 명령으로 알아 낼수 있다. kubectl -n argo get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d 접속 페이지접속 페이지 \" 접속 페이지 User InfoUser Info \" User Info ","date":"2021-12-31","objectID":"/argo/:5:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"4. Auth - LDAP Integration LDAP Integration LDAP을 연동 하기 위해선 values 값을 다운로드 하는 것이 좋다. helm show values argo/argo-cd \u003e argocd.yaml vi argocd.yaml 다운로드 받은 Yaml파일중에 dex부분을 수정한다, 없으면 추가 한다. dex.config: | connectors: - type: ldap name: Ldap id: ldap config: # Ldap server address host: tanzu-dns.tkg.io:389 insecureNoSSL: true insecureSkipVerify: true startTLS: false bindDN: \"$dex.ldap.bindDN\" bindPW: \"$dex.ldap.bindPW\" usernamePrompt: Username userSearch: baseDN: \"ou=tanzu,dc=tkg,dc=io\" filter: (objectClass=person) username: sAMAccountName idAttr: DN emailAttr: mail nameAttr: sAMAccountName groupSearch: baseDN: \"ou=tanzu,dc=tkg,dc=io\" filter: (objectClass=person) userAttr: DN groupAttr: member nameAttr: name 그리고 Secret을 생성 해준다. kubectl -n argo patch secrets argocd-secret --patch \"{\\\"data\\\":{\\\"dex.ldap.bindPW\\\":\\\"$(echo 'Passw0rd' | base64 -w 0)\\\"}}\" kubectl -n argo patch secrets argocd-secret --patch \"{\\\"data\\\":{\\\"dex.ldap.bindDN\\\":\\\"$(echo cn=administrator,cn=users,dc=tanzu,dc=io | base64 -w 0)\\\"}}\" HELM 실행 helm install argocd argo/argo-cd -n argo \\ --set server.extraArgs[0]=--insecure \\ -f argocd.yaml 접속 하기 위해 Portfoward를 하자 kubectl port-forward service/argocd-server -n argo 8080:443 ID는 admin 이며, PW는 별도의 명령으로 알아 낼수 있다. kubectl -n argo get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d ","date":"2021-12-31","objectID":"/argo/:6:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"4.1. RBAC 적용 RBAC을 적용 해주어 Admin 권한으로 접속이 되는지 확인 LDAP Integration confimap을 수정 해서 RBAC의 대한 설정을 해준다. kubectl edit cm argocd-rbac-cm -n argo configmap을 수정 하면 아래 내용이 있으면 수정 하고 없으면 추가 해준다. apiVersion: v1 data: policy.csv: | p, role:my1208, applications, *, my1208/*, allow p, role:my1208, projects, get, my1208, allow p, role:my1208, repositories, get, *, allow p, role:my1208, clusters, get, *, allow g, my1208, role:admin p, role:none, *, *, */*, deny g, tkg, role:readonly g, my1208@openbase.co.kr, role:admin policy.default: role:none scopes: '[groups,email]' Pod를 재 실행 해 준다. delete=`kubectl get pod -n argo | grep -v repo | egrep 'server|dex' | awk '{print $1}' | xargs echo` kubectl delete pod $delete -n argo 접속 화면 ","date":"2021-12-31","objectID":"/argo/:6:1","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"5. SLACK 연동 메시지를 SLACK으로 받기 위해 연동 ","date":"2021-12-31","objectID":"/argo/:7:0","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"5.1. SLACK 설정 SLACK APP 등록 APP 추가#1 APP 추가#2 APP을 추가하기 위해 APP의 Name 설정 및 workspace를 선택 한다. APP 추가#3 OAuth \u0026 Permmissions을 클릭 하면 아래처럼 화면이 나온다. APP 추가#4 Scopes를 찾아서 chat을 찾은후 적용 APP 추가#5 그러면 OAuth Tokens for Your Workspace가 활성화 되는 것을 확인 할 수 있다 그리고 Install to Workspaces를 클릭 APP 추가#6 APP 추가#7 TOKEN을 복사 한다. APP 추가#8 APP을 추가 해준다. APP 추가#9 APP 추가#10 새로 만든 APP이 나오는 것을 확인 할 수 있다. APP 추가#11 솔직히 여기 잘 나와 있다. SLACK APP 등록 ","date":"2021-12-31","objectID":"/argo/:7:1","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":["Documentation"],"content":"5.2. ARGO 설정 ARGO 설정은 별도로 UI에서 제공을 하지 않기 때문에 ConfigMap을 좀 수정 해야 한다. Helm에서 제공을 하긴 하는대 현재는 버그가 있는지 배포가 되지 않아 별도의 방법으로 구성한다. Slack 연동 해당 파일을 다운 로드 받은 후 실행 wget -O argo-noty-secret.yaml https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/v1.2.1/manifests/install.yaml wget -O argo-noty-config.yaml https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/v1.2.1/catalog/install.yaml kubectl apply -f argo-noty-secret.yaml -n argo kubectl apply -f argo-noty-config.yaml -n argo 그리고 secret 과 configmap을 수정한다. 사전에 변경해도 상관은 없다. kubectl edit secret argocd-notifications-secret -n argo apiVersion: v1 kind: Secret metadata: name: argocd-notifications-secret stringData: slack-token: xoxb-xxxxxxxxxx-xxxxxxxxxx-xxxxxxx kubectl edit cm argocd-notifications-cm -n argo apiVersion: v1 kind: ConfigMap metadata: name: argocd-notifications-cm data: service.slack: | ## apiURL: \u003curl\u003e # optional URL, e.g. https://example.com/api token: $slack-token # 위에 secret을 참고 함 ## username: \u003coverride-username\u003e # optional username ## icon: \u003coverride-icon\u003e # optional icon for the message (supports both emoij and url notation) Default Definition 참고. SLACK으로 보낼 내용 정리 Slack 연동 테스트를 위해 Application을 배포 한다. cat \u003c\u003c EOF | tee guestbook.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: guestbook annotations: notifications.argoproj.io/subscribe.on-sync-succeeded.slack: dk-devops notifications.argoproj.io/subscribe.on-sync-succeeded.slack: dk-devops notifications.argoproj.io/subscribe.on-sync-failed.slack: dk-devops notifications.argoproj.io/subscribe.on-sync-running.slack: dk-devops notifications.argoproj.io/subscribe.on-sync-status-unknown.slack: dev-ops notifications.argoproj.io/subscribe.on-deployed.slack: dk-devops notifications.argoproj.io/subscribe.on-health-degraded.slack: dk-devops spec: destination: namespace: default server: https://kubernetes.default.svc project: default source: path: kustomize-guestbook repoURL: https://github.com/argoproj/argocd-example-apps.git targetRevision: HEAD syncPolicy: automated: {} EOF 그럼 아래와 같이 Slack으로 메시지가 오는 것을 확인 할 수 있다. SLACK 확인 또는 UI에서 Application에 Annotation을 설정해서 확인 할 수 있다. GUI에서 Annotation 설정#1 GUI에서 Annotation 설정#2 GUI에서 Annotation 설정#3 ","date":"2021-12-31","objectID":"/argo/:7:2","tags":["argo","CD","cicd","tanzu","k8s","devops","dk","dokyung","argo slack","argo slack 연동","argocd","argocd slack","notification"],"title":"The Documentation Argo-CD","uri":"/argo/"},{"categories":null,"content":" My Projects About Ing…. ","date":"2021-12-31","objectID":"/about/:0:0","tags":null,"title":"About Dokyung","uri":"/about/"},{"categories":null,"content":"About Me…. 나는 개발자는 아니다. 하지만 개발까지 하고 싶어 하는 엔지니어이다. OSI 7레이어 중 L1 부터 시작하게 된 케이스이다. 처음에는 아파트에 라인을 설치 했고, 군대 사업으로 CSU / DSU를 납품 하였다. 그리고 신한은행에서 2년 9개월간 상주를 하면서 네트워크에 대해 본격적으로 시작 하게 된거 같았다. 물론 국비지원으로 네트워크를 공부 하면서 이 분야에 뛰어 들게 되었지만.. 그래도 내가 본격적으로 시작하게 된 계기가 된 실제적인 이유는 신한은행에서 상주를 하게 되면서다. 그래서 본격적으로 CISCO 장비 L2 / L3 / L4의 대한 운영을 하게 되었다. 사수는 아니지만 운영 PM분께서 많은 기회를 주셔서 L2 및 L3 / L4 의 대해서 실제적으로 설계 / 설정(데이터센터 이전 포함) 까지 하게 되어 신한은행이라는 인프라의 대해서 많은 것을 알 수 있었다. 그 이후에는 L4 / L7 에 좀더 집중적으로 근무를 할 수 있게 되는 환경으로 이직을 하게 되었다. 하지만 불행인지 다행인지 모르겠지만 우리은행으로 또 2년 5개월정도 상주를 하게 되었다. 지금 생각해보면 Radware / F5의 대해서 잘 모르던 시절이었지만, 담당자분께서 나를 잘 봐준 덕분으로 (물론 엄청 공부를 했지만) 무리 없이 상주를 잘 할 수 있었던거 같다. L2 / L3를 해봤기 때문에 많은 도움이 되었고, 그러므로 인해 데이터 센터 이전 등등의 프로젝트도 잘 수행 할 수 있었다. 그리고 우리은행 상주를 끝마치고 본사로 복귀 후 수 많은 프로젝트를 하게 되었다. 일일이 나열 할 수 없지만 (SC제일은행 데이터 센터 이전, 한수원 경주 데이터 센터 이전, 농협투자증권 데이터 센터, 오렌지 라이프(구 ING생명), 웹방화벽 오렌지 라이프(구 ING생명) 구성 이전 등등..) 그리고 2016년에 새로운 도전을 하기 위해 회사에서 클라우드팀을 신설 하게 되어 팀을 변경 하게 되었다. 하지만 그 당시 클라우드에 대해서 잘 몰랐기 때문에 퍼블릭 클라우드인 AZURE / AWS의 대해서 공부를 하고 AWS는 자격증을 취득 하게 되었다. 그러면서 OPENSTACK / K8S 등등의 대해서도 많이 접하고 공부도 많이 하게 되었던 거 같다. 하지만 궁극적으로는 VMware를 하게 되었다. 시작은 사내에 VMware로 구축을 하게 되면서 본격적으로 하게 되었고, 그러면서 팀이 아닌 본부가 생기게 되었다. 그리고 농협에 NSXT 컨설팅을 진행하면서 VMware의 자동화 솔루션인 VRA 시연발표를 하면서 VMware의 FullStack을 하게 되었다. VMware에 NSXT로 회사를 알리게 된 계기가 되면서, 농협과 삼성무선사에 NSXT를 설치 하게 되었고, 그 이후로 포스코에서 SDDC를 구축하게 된 계기가 되기도 했던거 같다. POSCO는 vSphere, vSAN, NSXT, HCX, VROPS, VRA, VRNI, VRLI를 모두 구축을 하면서 HCX라는 마이그레이션 툴을 통해 기존 환경에서 신규 환경으로 대량 마이그레이션을 진행 하였다. 또한 TANZU(K8S)를 꾸준히 공부하면서 CKA를 취득 하게 되었고, 그 이후에 삼성에 퍼블릭 환경에서 TANZU를 구축 하면서 HARBOR, FluentBit, Prometheus/Grafana, HELM, Kubeapps, 멀티 클러스터를 관리 및 모니터링을 하기 위한 TMC / TO를 구축 하였다. LICENSE Certified Kubernetes Administrator, 2021 VMware Certified Advanced Professional – Data Center Virtualization Deployment, 2020 VMware Certified Advanced Professional – Data Center Virtualization Design, 2020 VMware Certified Advanced Professional – Network Virtualization Deployment, 2019 Red Hat Certified System Administrator, 2018 AWS Certified Solution Architect – Associate, 2017 LTM Specialist: Architect, Setup \u0026 Deploy F5 301a, 2015 LTM Specialist: Maintain \u0026 Troubleshoot F5 301b, 2015 GTM Specialist F5 302, 2015 Certified Information System Security Professional, 2012 Engineer Information Processing, 2010 Cisco Certified Network Professional, 2007 ","date":"2021-12-31","objectID":"/about/:1:0","tags":null,"title":"About Dokyung","uri":"/about/"},{"categories":["Documentation"],"content":"K10 Install Guide","date":"2021-12-30","objectID":"/k10/","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"컨테이너 환경에서 백업을 하기 위한 방법으로 Veeam에서 제공하는 Kasten 설치 설치 환경은 Tanzu 1.4 버전으로 진행 ","date":"2021-12-30","objectID":"/k10/:0:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"1. Requirements helm 설치,  Helm. ","date":"2021-12-30","objectID":"/k10/:1:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"2. 환경 vSphere : 7.0 vSAN NSX : 3.2 AVI : 21.1.1 Tanzu: 1.4 Ingress: Contour ","date":"2021-12-30","objectID":"/k10/:2:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3. 설치 Kasten은 백업으로 유명한 Veeam에서 인수를 하여 컨테이너 환경에서 백업을 도와준다. ","date":"2021-12-30","objectID":"/k10/:3:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.1. Helm 설치 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh helm version ","date":"2021-12-30","objectID":"/k10/:3:1","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.2. Kasten Repo Update helm repo add kasten https://charts.kasten.io/ helm repo update helm search repo kasten Kasten Repo CheckKasten Repo Check \" Kasten Repo Check ","date":"2021-12-30","objectID":"/k10/:3:2","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.3. Kasten Install SSL 구성 Namespace를 생성 해준다. kubectl create ns kasten-io Kasten을 SSL을 구성 하려면 아래와 같이 인증서를 생성 후 Secret을 생성 해준다. 만약 SSL이 필요 없다면 아래는 패스해도 무관 하다. kubectl create secret tls kasten-tls --cert=/data/cert/yourodmain.com.crt --key=/data/cert/yourdomain.com.key -n kasten-io helm을 통해 Kasten을 설치 하기 위해 아래와 같이 실행을 한다. Ingress를 사용하는 방식중 Token으로 접속 할 수 있게 구성 한다. helm install k10 kasten/k10 \\ --set ingress.create=true \\ --set ingress.class=contour \\ --set auth.tokenAuth.enabled=true \\ --set ingress.tls.enabled=true \\ --set ingress.tls.secretName=kasten-tls \\ --set ingress.host=kasten.tkg.io \\ -n kasten-io 만약 HTTPS로 구성 하면 자동으로 SSL Redirect를 해주기 위해서 ingress를 확인 후 Annotation을 설정 해준다. kubectl get ing kubectl annotate ing k10-ingress -n kasten-io ingress.kubernetes.io/force-ssl-redirect=true ","date":"2021-12-30","objectID":"/k10/:3:3","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.4. 배포 완료 Kasten ComplateKasten Complate \" Kasten Complate ","date":"2021-12-30","objectID":"/k10/:3:4","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.5. Annotation 설정 완료 Kasten ingress AnnotationKasten ingress Annotation \" Kasten ingress Annotation ","date":"2021-12-30","objectID":"/k10/:3:5","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"3.6. 접속 방법 접속을 하기 위해 유저 생성 TOKEN으로 접속하기 위해 유저를 생성 한다. kubectl create serviceaccount my-kasten-sa --namespace kasten-io TOKEN 확인 방법 sa_secret=$(kubectl get serviceaccount my-kasten-sa -o jsonpath=\"{.secrets[0].name}\" --namespace kasten-io) kubectl get secret $sa_secret --namespace kasten-io -ojsonpath=\"{.data.token}{'\\n'}\" | base64 --decode 우선 Ingress로 구성을 했지만 여기선 포트 포워딩으로 설명 하겠다. kubectl --namespace kasten-io port-forward service/gateway 8080:8000 포트 포워딩을 위해 다른 SSH를 오픈 한다. ssh root@{포트포워딩 한 OS} -L 8080:localhost:8080 접속시 성공을 한 것을 확인 할 수 있다. kubectl get ing kubectl annotate ing k10-ingress -n kasten-io ingress.kubernetes.io/force-ssl-redirect=true ","date":"2021-12-30","objectID":"/k10/:3:6","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"4. K10 Auth LDAP 설정 K10 Auth 설정 values 파일을 다운로드 받는다. helm show values kasten/k10 \u003e k10.yaml Auth라는 부분을 찾은 후 아래 부분을 수정 해준다. ldap: enabled: true restartPod: false # Enable this value to force a restart of the authentication service pod dashboardURL: \"http://kasten.tkg.io/k10\" #The URL for accessing K10's dashboard host: \"tanzu-dns.tkg.io:389\" ##ldap 접속 정보 insecureNoSSL: true insecureSkipVerifySSL: true startTLS: false bindDN: \"cn=administrator,cn=users,dc=tkg,dc=io\" bindPW: \"Passw0rd\" bindPWSecretName: \"\" userSearch: baseDN: \"ou=tanzu,dc=tkg,dc=io\" filter: (objectClass=person) username: sAMAccountName idAttr: DN emailAttr: mail nameAttr: sAMAccountName preferredUsernameAttr: \"\" groupSearch: baseDN: \"ou=tanzu,dc=tkg,dc=io\" filter: (objectClass=group) userMatchers: - userAttr: DN groupAttr: member nameAttr: name secretName: \"\" # The Kubernetes Secret that contains OIDC settings usernameClaim: \"email\" usernamePrefix: \"\" groupnameClaim: \"groups\" groupnamePrefix: \"\" 아래와 같이 실행을 해준다. helm install k10 kasten/k10 \\ --set ingress.create=true \\ --set ingress.class=contour \\ --set ingress.tls.enabled=true \\ --set ingress.tls.secretName=kasten-tls \\ --set ingress.host=\"kasten.tkg.io\" \\ --set auth.k10AdminUsers[0]='my1208@openbase.co.kr' \\ --set auth.k10AdminGroups[0]=\"tkg\" \\ -f k10.yaml \\ -n kasten-io 또는 아래와 같이 업그레이드를 해준다. 만약 실패 하면 삭제 후 다시 Install 해주면 된다. helm upgrade k10 kasten/k10 \\ --set ingress.create=true \\ --set ingress.class=contour \\ --set ingress.tls.enabled=true \\ --set ingress.tls.secretName=kasten-tls \\ --set ingress.host=\"kasten.tkg.io\" \\ --set auth.k10AdminUsers[0]='my1208@openbase.co.kr' \\ --set auth.k10AdminGroups[0]=\"tkg\" \\ -f k10.yaml \\ -n kasten-io ","date":"2021-12-30","objectID":"/k10/:4:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"4.1. 완료 화면 Kasten Auth IntegrationKasten Auth Integration \" Kasten Auth Integration Kasten Auth IntegrationKasten Auth Integration \" Kasten Auth Integration ","date":"2021-12-30","objectID":"/k10/:4:1","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"5. NFS 연동 카스텐에서 스토리지를 저정하기 위해 여러가지 방법이 있지만 우선 NFS 연동을 하여 백업을 하는 방법에 대해서 기술 ","date":"2021-12-30","objectID":"/k10/:5:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"5.1. PVC 생성 kubectl apply -f kasten-io -f - \u003c\u003c EOF apiVersion: v1 kind: PersistentVolume metadata: name: test-pv spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs mountOptions: - hard - nfsvers=3.0 nfs: path: /volume1/Cloud-Home/08.VEEAM server: 10.253.1.254 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc namespace: kasten-io spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 10Gi EOF PVC 생성 완료 확인 ","date":"2021-12-30","objectID":"/k10/:5:1","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"5.2. 카스텐 설정 카스텐 Locations 설정 카스텐 NFS 설정 카스텐 프로파일 설정 완료 확인 ","date":"2021-12-30","objectID":"/k10/:5:2","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"6. MINIO 연동 MINIO를 구성 하려면 먼저 Erasure Code와 Immutability가 되어야 한다. 참고 링크 MINIO 참조 MINIO를 컨테이너 형태로 배포를 하게 되면 우선 Immutability를 지원 하지 않는 것으로 보인다. 그래서 별도로 VM을 생성 해서 진행 ","date":"2021-12-30","objectID":"/k10/:6:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"6.1. VM 설정 VM을 생성 할때 스토리지를 OS가 저장되는 HDD를 제외한 4개를 구성 후 배포를 완료 한다. VM 생성 ","date":"2021-12-30","objectID":"/k10/:6:1","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"6.2 FDISK 생성 VOLUME 생성 모든 HDD를 FDISK구성 해준다. 순서는 (n \u003e p \u003e t \u003e 8e \u003e w) 로 입력 해준다. fdisk /dev/sdb1 fdisk /dev/sdc1 fdisk /dev/sdd1 fdisk /dev/sde1 Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. Created a new DOS disklabel with disk identifier 0xab657906. Command (m for help): n Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions) Select (default p): p Partition number (1-4, default 1): First sector (2048-209715199, default 2048): Last sector, +sectors or +size{K,M,G,T,P} (2048-209715199, default 209715199): Created a new partition 1 of type 'Linux' and of size 100 GiB. Command (m for help): t Selected partition 1 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. VOLUME을 생성해 준다. pvcreate /dev/sdb1 vgcreate vg_xfs_minio_1 /dev/sdb1 lvcreate -L +99G -n xfs_minio_1 vg_xfs_minio_1 mkfs.xfs /dev/vg_xfs_minio_1/xfs_minio_1 mkdir /root/xfs_minio_1 mount /dev/vg_xfs_minio_1/xfs_minio_1 /root/xfs_minio_1/ df -hT /root/xfs_minio_1/ pvcreate /dev/sdc1 vgcreate vg_xfs_minio_2 /dev/sdc1 lvcreate -L +99G -n xfs_minio_2 vg_xfs_minio_2 mkfs.xfs /dev/vg_xfs_minio_2/xfs_minio_2 mkdir /root/xfs_minio_2 mount /dev/vg_xfs_minio_2/xfs_minio_2 /root/xfs_minio_2/ df -hT /root/xfs_minio_2/ pvcreate /dev/sdd1 vgcreate vg_xfs_minio_3 /dev/sdd1 lvcreate -L +99G -n xfs_minio_3 vg_xfs_minio_3 mkfs.xfs /dev/vg_xfs_minio_3/xfs_minio_3 mkdir /root/xfs_minio_3 mount /dev/vg_xfs_minio_3/xfs_minio_3 /root/xfs_minio_3/ df -hT /root/xfs_minio_3/ pvcreate /dev/sde1 vgcreate vg_xfs_minio_4 /dev/sde1 lvcreate -L +99G -n xfs_minio_4 vg_xfs_minio_4 mkfs.xfs /dev/vg_xfs_minio_4/xfs_minio_4 mkdir /root/xfs_minio_4 mount /dev/vg_xfs_minio_4/xfs_minio_4 /root/xfs_minio_4/ df -hT /root/xfs_minio_4/ 부팅 후에도 마운투가 될 수 있게 fstab에 저장해준다. blkid로 UUID를 확인 blkid echo 'UUID=b6d3f331-deaf-428b-bcb0-c9b48bab2253 /root/xfs_minio_1 xfs defaults 1 1' \u003e\u003e /etc/fstab echo 'UUID=213694c7-bbaf-45c4-96c8-4e912dc70f3f /root/xfs_minio_2 xfs defaults 1 1' \u003e\u003e /etc/fstab echo 'UUID=e7aa0e12-3c0c-4e12-a00d-9ebeaab76669 /root/xfs_minio_3 xfs defaults 1 1' \u003e\u003e /etc/fstab echo 'UUID=ac211fab-162e-4f8a-854b-1960aa43e252 /root/xfs_minio_4 xfs defaults 1 1' \u003e\u003e /etc/fstab UUID 확인 UUID 확인 MINIO \u0026 MC 설치 wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio mv minio /usr/local/bin/ wget https://dl.min.io/client/mc/release/linux-amd64/mc chmod +x mc cp mc /usr/local/bin/ 설치가 완료 되면 SSL을 생성한다. SSL 생성은 간단 하므로 여기서 표시 하지는 않겠다. 생성된 SSL 인증서와 Key를 minio 폴더로 카피 한다. cp yourdomain.com.crt /root/.minio/certs/public.crt cp yourdomain.com.key /root/.minio/certs/private.key MINIO를 생성 한다. mc config host add minio-veeam https://minio.tkg.io minioadmin minioadmin --api S3v4 minio server --address \":443\" /root/xfs_minio_1/ /root/xfs_minio_2/ /root/xfs_minio_3/ /root/xfs_minio_4/ mc mb --debug -l minio-veeam/veeam-immutable mc retention set --default compliance 30d minio-veeam/veeam-immutable 프로파일 설정 카스텐 프로파일 설정 카스텐 프로파일 등록 완료 1 카스텐 프로파일 등록 완료 2 ","date":"2021-12-30","objectID":"/k10/:6:2","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"},{"categories":["Documentation"],"content":"7. 백업 완료 카스텐에서 백업 실행후 완료 확인 MINIO에서 백업 확인 ","date":"2021-12-30","objectID":"/k10/:7:0","tags":["K10","kasten","tanzu","k8s","devops","dk","dokyung"],"title":"The Documentation K10 Install","uri":"/k10/"}]